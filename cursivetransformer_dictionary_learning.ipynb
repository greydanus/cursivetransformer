{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOW1NSWcQhWC2vEtbPXJCtT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f5d8312194343d3b41b3958e50adc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bab47c9eacf4ebabd8e6fa8e4efa4fb",
              "IPY_MODEL_336d3e607a854798a18a495f02cace8f",
              "IPY_MODEL_d0dc2a6c06b0416fad2deea3a27c0d97"
            ],
            "layout": "IPY_MODEL_b686576fddd2456b96a6d9e6b321bbdb"
          }
        },
        "6bab47c9eacf4ebabd8e6fa8e4efa4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b60c716e93f1474d8ddc14362af57351",
            "placeholder": "​",
            "style": "IPY_MODEL_9da667b0b2e84ba0a7551e70425104c9",
            "value": "100%"
          }
        },
        "336d3e607a854798a18a495f02cace8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1641ecde1eb4ac09d39ef6adaf8a25e",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64bbab7c61a54023ba3157998cb1fd9a",
            "value": 5000
          }
        },
        "d0dc2a6c06b0416fad2deea3a27c0d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c5399003fd4558a1cf7f067397396e",
            "placeholder": "​",
            "style": "IPY_MODEL_fad7d42f3db04b2b942b2047a1a0307c",
            "value": " 5000/5000 [19:49&lt;00:00,  4.24it/s]"
          }
        },
        "b686576fddd2456b96a6d9e6b321bbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60c716e93f1474d8ddc14362af57351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da667b0b2e84ba0a7551e70425104c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1641ecde1eb4ac09d39ef6adaf8a25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64bbab7c61a54023ba3157998cb1fd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40c5399003fd4558a1cf7f067397396e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad7d42f3db04b2b942b2047a1a0307c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "216d7b86d6fb4dfca7c345c93d2d2d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4696eaf32af4f25b27d7ccde636161e",
              "IPY_MODEL_965a22fd377549d3a6f5f94d0b777aa8",
              "IPY_MODEL_be76330f9d0243ba9f62c365cd387dee"
            ],
            "layout": "IPY_MODEL_e9ce623a28994334a9ce1a777def9d7b"
          }
        },
        "c4696eaf32af4f25b27d7ccde636161e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0cbb088a44e4c67ba6329e50219187f",
            "placeholder": "​",
            "style": "IPY_MODEL_fa074d3bfe844577af3fbfe451b411b9",
            "value": "100%"
          }
        },
        "965a22fd377549d3a6f5f94d0b777aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e372ed1741b141d88fb08e82d421ef27",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcb3ad2e1ab2482c9429b8b7a2acfe53",
            "value": 25
          }
        },
        "be76330f9d0243ba9f62c365cd387dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6bf4164b0154190978436661b261b99",
            "placeholder": "​",
            "style": "IPY_MODEL_d7f5f3f202154da2b7b104a6d40a6605",
            "value": " 25/25 [00:01&lt;00:00, 16.77it/s]"
          }
        },
        "e9ce623a28994334a9ce1a777def9d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0cbb088a44e4c67ba6329e50219187f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa074d3bfe844577af3fbfe451b411b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e372ed1741b141d88fb08e82d421ef27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcb3ad2e1ab2482c9429b8b7a2acfe53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6bf4164b0154190978436661b261b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f5f3f202154da2b7b104a6d40a6605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer_dictionary_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cursive Transformer Mechanistic Interpretability\n",
        "---\n",
        "## Introduction\n",
        "The purpose of this notebook is to apply the techniques of mechanistic interpretability, in particular dictionary learning, to our cursive transformer. The goal of this exercise is two-fold:\n",
        "1. Attempt to gain insight into the structure and orientation of the features being learned by the model, in particular in the cross-attention and feed-forward layers\n",
        "2. More generally become familiar with the latest ME techniques, as they can easily transfer to be applied beyond the scope of this project to essentially ANY transformer-based model\n",
        "\n",
        "To this end, we will reference the following resources throughout this document:\n",
        "- https://transformer-circuits.pub/2023/monosemantic-features\n",
        "- https://www.alignmentforum.org/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s\n",
        "- https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing\n",
        "- https://github.com/TransformerLensOrg/TransformerLens\n",
        "- https://arena3-chapter1-transformer-interp.streamlit.app/"
      ],
      "metadata": {
        "id": "GdwBlsu_am35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JuHwKIaP3Oyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -V"
      ],
      "metadata": {
        "id": "12UmRQ9N3Rav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5070f4-116d-4ca3-e055-74bf34ef1414"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "5BpggN2t3Qr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "VY-xjph_3QkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c5f2c7-f86e-4b71-a3c4-25468b284835"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.34.2)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (3.0.1)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.34)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.44.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.18.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.10)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2372, done.\u001b[K\n",
            "remote: Counting objects: 100% (513/513), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 2372 (delta 433), reused 454 (delta 390), pack-reused 1859 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2372/2372), 32.13 MiB | 36.68 MiB/s, done.\n",
            "Resolving deltas: 100% (1339/1339), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 1)) (2.4.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 2)) (0.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r cursivetransformer/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r cursivetransformer/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (5.0.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "cWhbLc-33V2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "xit1BQED3ZK_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Autoencoder"
      ],
      "metadata": {
        "id": "NkFwM1If3bbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1p4_vsXeZigM"
      },
      "outputs": [],
      "source": [
        "# Configuration for the autoencoder\n",
        "cfg = {\n",
        "    \"seed\": 49,\n",
        "    \"batch_size\": 4096,\n",
        "    \"buffer_mult\": 384,\n",
        "    \"lr\": 1e-4,\n",
        "    \"num_tokens\": int(2e9),\n",
        "    \"l1_coeff\": 3e-4,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.99,\n",
        "    \"dict_mult\": 4,\n",
        "    \"seq_len\": 256,\n",
        "    \"d_mlp\": None,  # To be set after loading the model\n",
        "    \"enc_dtype\": \"fp32\",\n",
        "    \"remove_rare_dir\": False,\n",
        "}\n",
        "\n",
        "cfg[\"model_batch_size\"] = 64\n",
        "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
        "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
        "\n",
        "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
        "        d_mlp = cfg[\"d_mlp\"]\n",
        "        l1_coeff = cfg[\"l1_coeff\"]\n",
        "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
        "        torch.manual_seed(cfg[\"seed\"])\n",
        "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
        "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
        "\n",
        "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.l1_coeff = l1_coeff\n",
        "\n",
        "        self.to(\"cuda\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_cent = x - self.b_dec\n",
        "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
        "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
        "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
        "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
        "        loss = l2_loss + l1_loss\n",
        "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def remove_parallel_component_of_grads(self):\n",
        "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
        "        self.W_dec.grad -= W_dec_grad_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n",
        "\n"
      ],
      "metadata": {
        "id": "TDksX_ZQ3lcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Reconstruction Loss"
      ],
      "metadata": {
        "id": "iDiByoQV3kvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recons_loss(num_batches=5, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    loss_list = []\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for i in range(num_batches):\n",
        "        idx = torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        context = all_contexts[idx].to(model_device)\n",
        "        targets = all_targets[idx].to(model_device)\n",
        "\n",
        "        # Capture MLP activations\n",
        "        mlp_activations = []\n",
        "\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook on the MLP output layer\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj  # Adjust layer index as needed\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass without hooks\n",
        "        logits, loss = model(tokens, context, targets=targets)\n",
        "        loss = loss.item()\n",
        "\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "\n",
        "        # Flatten activations\n",
        "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Reconstruct activations using encoder\n",
        "        loss_enc, x_reconstruct, hidden_acts, l2_loss, l1_loss = local_encoder(mlp_acts_flattened)\n",
        "\n",
        "        # Reconstructed activations reshaped back to original shape\n",
        "        reconstructed_acts = x_reconstruct.view_as(mlp_acts)\n",
        "\n",
        "        # Define the hook to replace activations\n",
        "        def reconstruction_hook(module, input, output):\n",
        "            return reconstructed_acts\n",
        "\n",
        "        # Register the reconstruction hook\n",
        "        hook_handle = mlp_layer.register_forward_hook(reconstruction_hook)\n",
        "        # Forward pass with reconstructed activations\n",
        "        logits_recons, loss_recons = model(tokens, context, targets=targets)\n",
        "        recons_loss = loss_recons.item()\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Zero ablation\n",
        "        def zero_ablation_hook(module, input, output):\n",
        "            return torch.zeros_like(output)\n",
        "\n",
        "        hook_handle = mlp_layer.register_forward_hook(zero_ablation_hook)\n",
        "        logits_zero_abl, loss_zero_abl = model(tokens, context, targets=targets)\n",
        "        zero_abl_loss = loss_zero_abl.item()\n",
        "        hook_handle.remove()\n",
        "\n",
        "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
        "    losses = torch.tensor(loss_list)\n",
        "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
        "\n",
        "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
        "    score = ((zero_abl_loss - recons_loss) / (zero_abl_loss - loss))\n",
        "    print(f\"Reconstruction Score: {score:.2%}\")\n",
        "    return score, loss, recons_loss, zero_abl_loss"
      ],
      "metadata": {
        "id": "pxQjA1Xh3pgP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Frequencies"
      ],
      "metadata": {
        "id": "j1dKdlV13rTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_freqs(num_batches=25, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
        "    total = 0\n",
        "    for i in tqdm.trange(num_batches):\n",
        "        idx = torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        context = all_contexts[idx].to(model_device)\n",
        "\n",
        "        mlp_activations = []\n",
        "\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook on the MLP output layer\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj  # Adjust layer index as needed\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(tokens, context)\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Pass through encoder\n",
        "        hidden = local_encoder(mlp_acts_flattened)[2]  # Get the activations\n",
        "\n",
        "        act_freq_scores += (hidden > 0).sum(0)\n",
        "        total += hidden.shape[0]\n",
        "    act_freq_scores /= total\n",
        "    num_dead = (act_freq_scores == 0).float().mean()\n",
        "    print(\"Num dead\", num_dead.item())\n",
        "    return act_freq_scores"
      ],
      "metadata": {
        "id": "kbDgvhpS3vnU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualise Feature Utils"
      ],
      "metadata": {
        "id": "kk1mA6FK3y0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from html import escape\n",
        "import colorsys\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "SPACE = \"·\"\n",
        "NEWLINE = \"↩\"\n",
        "TAB = \"→\"\n",
        "\n",
        "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
        "    # Escape strings to deal with tabs, newlines, etc.\n",
        "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
        "    processed_strings = [\n",
        "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
        "        for s in escaped_strings\n",
        "    ]\n",
        "\n",
        "    if isinstance(values, torch.Tensor) and len(values.shape) > 1:\n",
        "        values = values.flatten().tolist()\n",
        "\n",
        "    if not allow_different_length:\n",
        "        assert len(processed_strings) == len(values)\n",
        "\n",
        "    # Scale values\n",
        "    if max_value is None:\n",
        "        max_value = max(max(values), -min(values)) + 1e-3\n",
        "    scaled_values = [v / max_value * saturation for v in values]\n",
        "\n",
        "    # Create HTML\n",
        "    html = \"\"\n",
        "    for i, s in enumerate(processed_strings):\n",
        "        if i < len(scaled_values):\n",
        "            v = scaled_values[i]\n",
        "        else:\n",
        "            v = 0\n",
        "        if v < 0:\n",
        "            hue = 0  # Red in HSV\n",
        "        else:\n",
        "            hue = 0.66  # Blue in HSV\n",
        "        rgb_color = colorsys.hsv_to_rgb(\n",
        "            hue, v, 1\n",
        "        )\n",
        "        hex_color = \"#%02x%02x%02x\" % (\n",
        "            int(rgb_color[0] * 255),\n",
        "            int(rgb_color[1] * 255),\n",
        "            int(rgb_color[2] * 255),\n",
        "        )\n",
        "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
        "    if return_string:\n",
        "        return html\n",
        "    else:\n",
        "        display(HTML(html))\n",
        "\n",
        "# - [ ] TODO: <-DEBUG THIS!!!\n",
        "def stroke_to_svg(stroke_sequence):\n",
        "    # This function generates an SVG path for each stroke in the sequence\n",
        "    svg_output = \"\"\n",
        "    for stroke in stroke_sequence:\n",
        "        if stroke[2] == 0:\n",
        "          continue\n",
        "        svg_output += f'<path d=\"M{stroke[0]},{stroke[1]}'\n",
        "        # for point in stroke[1:]:\n",
        "        #     svg_output += f' L{point[0]},{point[1]}'\n",
        "    svg_output += '\" style=\"stroke:black;fill:none;\" />'\n",
        "    return svg_output\n",
        "\n",
        "def basic_feature_vis(feature_index, max_val=0):\n",
        "    feature_in = encoder.W_enc[:, feature_index]\n",
        "    feature_bias = encoder.b_enc[feature_index]\n",
        "\n",
        "    # Get a sample stroke sequence and context from the dataset\n",
        "    x, c, _ = test_dataset[0]  # You can choose any index or randomize\n",
        "    tokens = x.unsqueeze(0).to(model_device)  # Stroke tokens\n",
        "    context = c.unsqueeze(0).to(model_device)  # Context tokens\n",
        "\n",
        "    # Capture MLP activations\n",
        "    mlp_activations = []\n",
        "\n",
        "    def capture_mlp_activations(module, input, output):\n",
        "        mlp_activations.append(output.detach())\n",
        "\n",
        "    # Register the hook\n",
        "    mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "    hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, _ = model(tokens, context)\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Get the activations\n",
        "    mlp_acts = mlp_activations[0][0].cpu()  # Shape: [seq_len, d_mlp]\n",
        "    feature_acts = F.relu((mlp_acts - encoder.b_dec.cpu()) @ feature_in.cpu() + feature_bias.cpu())\n",
        "    if max_val == 0:\n",
        "        max_val = max(1e-7, feature_acts.max().item())\n",
        "\n",
        "    # Decode stroke tokens back to the original stroke format\n",
        "    stroke_sequence = [offsets_to_strokes(test_dataset.decode_stroke(t)) for t in tokens]\n",
        "\n",
        "    # Convert decoded strokes to SVG format for visualization\n",
        "    stroke_svgs = [stroke_to_svg(stroke) for stroke in stroke_sequence]\n",
        "\n",
        "    return basic_token_vis_make_str(stroke_svgs, feature_acts, max_val)\n",
        "\n",
        "\n",
        "def basic_token_vis_make_str(strings, values, max_val=None):\n",
        "    if not isinstance(strings, list):\n",
        "        strings = list(strings)\n",
        "    values = values.detach().numpy()\n",
        "    if max_val is None:\n",
        "        max_val = values.max()\n",
        "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
        "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
        "    body_string = create_html(strings, values, max_value=max_val, return_string=True, allow_different_length=True)\n",
        "    return header_string + body_string"
      ],
      "metadata": {
        "id": "I9jLMFym32Yw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Token DataFrame"
      ],
      "metadata": {
        "id": "NZmEcWIx34gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_token_df(tokens, len_prefix=5, len_suffix=1):\n",
        "    str_tokens = [str(t.item()) for t in tokens]\n",
        "    unique_token = [f\"{s}/{i}\" for i, s in enumerate(str_tokens)]\n",
        "\n",
        "    context = []\n",
        "    batch = []\n",
        "    pos = []\n",
        "    label = []\n",
        "    for p in range(len(tokens)):\n",
        "        prefix = \"\".join(str_tokens[max(0, p - len_prefix):p])\n",
        "        if p == len(tokens) - 1:\n",
        "            suffix = \"\"\n",
        "        else:\n",
        "            suffix = \"\".join(str_tokens[p + 1:min(len(tokens) - 1, p + 1 + len_suffix)])\n",
        "        current = str_tokens[p]\n",
        "        context.append(f\"{prefix}|{current}|{suffix}\")\n",
        "        batch.append(0)  # Since we have a single batch\n",
        "        pos.append(p)\n",
        "        label.append(f\"{0}/{p}\")\n",
        "    return pd.DataFrame(dict(\n",
        "        str_tokens=str_tokens,\n",
        "        unique_token=unique_token,\n",
        "        context=context,\n",
        "        batch=batch,\n",
        "        pos=pos,\n",
        "        label=label,\n",
        "    ))"
      ],
      "metadata": {
        "id": "j3py2Keq37VI"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "rpgE-1vu37QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)\n",
        "\n",
        "\n",
        "cfg[\"d_mlp\"] = args.n_embd  # Assuming n_embd is the MLP dimension\n",
        "args"
      ],
      "metadata": {
        "id": "Fb6qP_3i72fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbaf1fe-8eaa-4d73-a4b4-6fea2a5e72ff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n",
            "For a dataset of 95 examples we can generate 3183545 combinations of 4 examples.\n",
            "Generating 3000 4-word examples.\n",
            "Number of examples in the train dataset: 497000\n",
            "Number of examples in the test dataset: 3000\n",
            "Max token sequence length: 1000\n",
            "Number of unique characters in the ascii vocabulary: 71\n",
            "Ascii vocabulary:\n",
            "\t\" enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"'(BCQLMWYU,ZF!DXV?KPGJ\"\n",
            "Split up the dataset into 497000 training examples and 3000 test examples\n",
            "Number of Transformer parameters: 368064\n",
            "Model #params: 397184\n",
            "Loaded model from local path: best_checkpoint.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(max_steps=110000,\n",
              "          print_every=100,\n",
              "          log_every=2500,\n",
              "          lr_decay=0.333,\n",
              "          step_lr_every=33000,\n",
              "          device='cuda',\n",
              "          seed=42,\n",
              "          n_layer=4,\n",
              "          n_embd=64,\n",
              "          n_embd2=64,\n",
              "          n_ctx_head=4,\n",
              "          learning_rate=0.01,\n",
              "          weight_decay=0.0001,\n",
              "          batch_size=32,\n",
              "          train_size=497000,\n",
              "          test_size=3000,\n",
              "          num_words=4,\n",
              "          max_seq_length=1000,\n",
              "          augment=True,\n",
              "          ablate_cross_attention=False,\n",
              "          downsample_mean=0.65,\n",
              "          downsample_width=0.1,\n",
              "          add_digits=True,\n",
              "          alphabet=' enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"\\'(BCQLMWYU,ZF!DXV?KPGJ',\n",
              "          dataset_name='bigbank',\n",
              "          wandb_project='bigbank_experiments',\n",
              "          wandb_entity='sam-greydanus',\n",
              "          wandb_run_name='cursivetransformer_dictionary_learning',\n",
              "          wandb_api_key=None,\n",
              "          load_from_run_id='6le6tujz',\n",
              "          sample_only=True,\n",
              "          local_checkpoint_path='best_checkpoint.pt',\n",
              "          block_size=1000,\n",
              "          context_block_size=50,\n",
              "          vocab_size=455,\n",
              "          context_vocab_size=72)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_device = 'cuda'\n",
        "model = model.to(model_device)\n",
        "model = model.eval()  # Set model to evaluation mode"
      ],
      "metadata": {
        "id": "f650Kjl_O-P4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "hTPC8yxS4DNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = cfg[\"model_batch_size\"] # - [ ] TODO: Determine if this is needed?\n",
        "num_samples = len(test_dataset)\n",
        "stroke_seqs = []\n",
        "contexts = []\n",
        "targets = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    x, c, y = test_dataset[i]\n",
        "    stroke_seqs.append(x)\n",
        "    contexts.append(c)\n",
        "    targets.append(y)\n",
        "\n",
        "all_tokens = torch.stack(stroke_seqs)\n",
        "all_contexts = torch.stack(contexts)\n",
        "all_targets = torch.stack(targets)"
      ],
      "metadata": {
        "id": "nAEcf7h_4DFN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "q7_RwgFV4Mln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Autoencoder"
      ],
      "metadata": {
        "id": "JwmkLrrc4Cv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = AutoEncoder(cfg)"
      ],
      "metadata": {
        "id": "Yiea6x6o4Q69"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Autoencoder"
      ],
      "metadata": {
        "id": "u4f-J5kyOa9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sae(model, encoder, num_batches=5000, batch_size=256):\n",
        "    optimizer = torch.optim.Adam(encoder.parameters(), lr=cfg['lr'], betas=(cfg['beta1'], cfg['beta2']))\n",
        "\n",
        "    for batch in tqdm.tqdm(range(num_batches)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sample data\n",
        "        idx = torch.randperm(len(all_tokens))[:batch_size]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        contexts = all_contexts[idx].to(model_device)\n",
        "\n",
        "        # Capture MLP activations\n",
        "        mlp_activations = []\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(tokens, contexts)\n",
        "\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0].reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Pass through encoder\n",
        "        loss, _, _, l2_loss, l1_loss = encoder(mlp_acts)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f\"Batch {batch}, Loss: {loss.item():.4f}, L2 Loss: {l2_loss.item():.4f}, L1 Loss: {l1_loss.item():.4f}\")\n",
        "\n",
        "    print(\"SAE training completed.\")\n",
        "\n",
        "# Train the SAE\n",
        "train_sae(model, encoder)\n",
        "# Save the encoder weights\n",
        "# torch.save(encoder.state_dict(), 'encoder_weights.pth')"
      ],
      "metadata": {
        "id": "FAgTAimjOajl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f5d8312194343d3b41b3958e50adc60",
            "6bab47c9eacf4ebabd8e6fa8e4efa4fb",
            "336d3e607a854798a18a495f02cace8f",
            "d0dc2a6c06b0416fad2deea3a27c0d97",
            "b686576fddd2456b96a6d9e6b321bbdb",
            "b60c716e93f1474d8ddc14362af57351",
            "9da667b0b2e84ba0a7551e70425104c9",
            "b1641ecde1eb4ac09d39ef6adaf8a25e",
            "64bbab7c61a54023ba3157998cb1fd9a",
            "40c5399003fd4558a1cf7f067397396e",
            "fad7d42f3db04b2b942b2047a1a0307c"
          ]
        },
        "outputId": "834cdc64-ee56-462d-efbc-9b85259bf9f5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5d8312194343d3b41b3958e50adc60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 199989.3750, L2 Loss: 91766.6641, L1 Loss: 108222.7188\n",
            "Batch 10, Loss: 188419.1406, L2 Loss: 82667.8125, L1 Loss: 105751.3281\n",
            "Batch 20, Loss: 178233.5000, L2 Loss: 74779.5547, L1 Loss: 103453.9453\n",
            "Batch 30, Loss: 165637.0000, L2 Loss: 65982.7891, L1 Loss: 99654.2031\n",
            "Batch 40, Loss: 152669.3906, L2 Loss: 57536.2344, L1 Loss: 95133.1562\n",
            "Batch 50, Loss: 141768.7188, L2 Loss: 50784.6992, L1 Loss: 90984.0156\n",
            "Batch 60, Loss: 132865.9062, L2 Loss: 45608.8086, L1 Loss: 87257.0938\n",
            "Batch 70, Loss: 125506.7656, L2 Loss: 41574.0820, L1 Loss: 83932.6875\n",
            "Batch 80, Loss: 120790.1719, L2 Loss: 39131.4102, L1 Loss: 81658.7656\n",
            "Batch 90, Loss: 112216.2656, L2 Loss: 35059.9961, L1 Loss: 77156.2656\n",
            "Batch 100, Loss: 107298.8984, L2 Loss: 32934.7969, L1 Loss: 74364.1016\n",
            "Batch 110, Loss: 98221.0781, L2 Loss: 28913.7148, L1 Loss: 69307.3594\n",
            "Batch 120, Loss: 94424.1719, L2 Loss: 27562.0488, L1 Loss: 66862.1250\n",
            "Batch 130, Loss: 95699.8984, L2 Loss: 28525.4043, L1 Loss: 67174.4922\n",
            "Batch 140, Loss: 89670.5234, L2 Loss: 26275.2773, L1 Loss: 63395.2461\n",
            "Batch 150, Loss: 87145.9844, L2 Loss: 25581.4648, L1 Loss: 61564.5156\n",
            "Batch 160, Loss: 81861.1094, L2 Loss: 23719.4766, L1 Loss: 58141.6328\n",
            "Batch 170, Loss: 77268.8281, L2 Loss: 22162.5000, L1 Loss: 55106.3242\n",
            "Batch 180, Loss: 76806.3281, L2 Loss: 22449.9863, L1 Loss: 54356.3438\n",
            "Batch 190, Loss: 73945.6875, L2 Loss: 21667.4219, L1 Loss: 52278.2695\n",
            "Batch 200, Loss: 70232.3438, L2 Loss: 20513.5859, L1 Loss: 49718.7539\n",
            "Batch 210, Loss: 67681.3281, L2 Loss: 19852.9043, L1 Loss: 47828.4219\n",
            "Batch 220, Loss: 66585.7031, L2 Loss: 19725.2363, L1 Loss: 46860.4688\n",
            "Batch 230, Loss: 64578.7656, L2 Loss: 19242.0410, L1 Loss: 45336.7266\n",
            "Batch 240, Loss: 60866.4961, L2 Loss: 18073.6484, L1 Loss: 42792.8477\n",
            "Batch 250, Loss: 59530.8633, L2 Loss: 17915.6680, L1 Loss: 41615.1953\n",
            "Batch 260, Loss: 56758.2969, L2 Loss: 17171.3730, L1 Loss: 39586.9219\n",
            "Batch 270, Loss: 56002.3281, L2 Loss: 17147.4023, L1 Loss: 38854.9258\n",
            "Batch 280, Loss: 54391.8594, L2 Loss: 16844.8457, L1 Loss: 37547.0156\n",
            "Batch 290, Loss: 52957.2109, L2 Loss: 16446.6094, L1 Loss: 36510.6016\n",
            "Batch 300, Loss: 50710.8320, L2 Loss: 15857.4766, L1 Loss: 34853.3555\n",
            "Batch 310, Loss: 49279.9141, L2 Loss: 15439.6191, L1 Loss: 33840.2969\n",
            "Batch 320, Loss: 49235.3438, L2 Loss: 15680.6270, L1 Loss: 33554.7188\n",
            "Batch 330, Loss: 46932.0664, L2 Loss: 14973.0283, L1 Loss: 31959.0391\n",
            "Batch 340, Loss: 45763.6484, L2 Loss: 14646.0449, L1 Loss: 31117.6016\n",
            "Batch 350, Loss: 45162.3867, L2 Loss: 14559.8789, L1 Loss: 30602.5078\n",
            "Batch 360, Loss: 44508.8320, L2 Loss: 14470.5928, L1 Loss: 30038.2383\n",
            "Batch 370, Loss: 43745.7969, L2 Loss: 14305.5127, L1 Loss: 29440.2852\n",
            "Batch 380, Loss: 41629.5508, L2 Loss: 13702.5361, L1 Loss: 27927.0156\n",
            "Batch 390, Loss: 41897.8633, L2 Loss: 13876.6299, L1 Loss: 28021.2344\n",
            "Batch 400, Loss: 39669.3242, L2 Loss: 13289.0459, L1 Loss: 26380.2773\n",
            "Batch 410, Loss: 39190.9258, L2 Loss: 13208.5645, L1 Loss: 25982.3613\n",
            "Batch 420, Loss: 38900.3984, L2 Loss: 13133.5273, L1 Loss: 25766.8711\n",
            "Batch 430, Loss: 39574.9531, L2 Loss: 13328.0146, L1 Loss: 26246.9395\n",
            "Batch 440, Loss: 37785.7227, L2 Loss: 12910.4229, L1 Loss: 24875.3008\n",
            "Batch 450, Loss: 36528.3906, L2 Loss: 12592.4316, L1 Loss: 23935.9570\n",
            "Batch 460, Loss: 36148.7656, L2 Loss: 12553.2627, L1 Loss: 23595.5020\n",
            "Batch 470, Loss: 35111.8789, L2 Loss: 12338.8311, L1 Loss: 22773.0488\n",
            "Batch 480, Loss: 35680.2266, L2 Loss: 12530.6035, L1 Loss: 23149.6250\n",
            "Batch 490, Loss: 34387.0039, L2 Loss: 12248.4004, L1 Loss: 22138.6035\n",
            "Batch 500, Loss: 34155.0430, L2 Loss: 12205.6445, L1 Loss: 21949.3984\n",
            "Batch 510, Loss: 32441.1719, L2 Loss: 11863.1709, L1 Loss: 20578.0020\n",
            "Batch 520, Loss: 32697.0078, L2 Loss: 11966.6689, L1 Loss: 20730.3379\n",
            "Batch 530, Loss: 32510.3945, L2 Loss: 11912.6846, L1 Loss: 20597.7090\n",
            "Batch 540, Loss: 32418.8398, L2 Loss: 11910.6377, L1 Loss: 20508.2031\n",
            "Batch 550, Loss: 32378.7266, L2 Loss: 11904.5312, L1 Loss: 20474.1953\n",
            "Batch 560, Loss: 30582.9023, L2 Loss: 11629.7344, L1 Loss: 18953.1680\n",
            "Batch 570, Loss: 31833.8984, L2 Loss: 11842.6826, L1 Loss: 19991.2168\n",
            "Batch 580, Loss: 30809.1582, L2 Loss: 11619.8125, L1 Loss: 19189.3457\n",
            "Batch 590, Loss: 29514.6914, L2 Loss: 11457.5283, L1 Loss: 18057.1621\n",
            "Batch 600, Loss: 29842.8711, L2 Loss: 11448.3525, L1 Loss: 18394.5195\n",
            "Batch 610, Loss: 29162.1680, L2 Loss: 11339.9326, L1 Loss: 17822.2344\n",
            "Batch 620, Loss: 29446.3691, L2 Loss: 11440.1426, L1 Loss: 18006.2266\n",
            "Batch 630, Loss: 28568.1836, L2 Loss: 11278.4502, L1 Loss: 17289.7324\n",
            "Batch 640, Loss: 28279.0156, L2 Loss: 11269.4209, L1 Loss: 17009.5957\n",
            "Batch 650, Loss: 27501.3633, L2 Loss: 11156.5166, L1 Loss: 16344.8467\n",
            "Batch 660, Loss: 28348.0859, L2 Loss: 11239.0361, L1 Loss: 17109.0508\n",
            "Batch 670, Loss: 26199.0137, L2 Loss: 10980.8730, L1 Loss: 15218.1406\n",
            "Batch 680, Loss: 28358.9492, L2 Loss: 11212.3867, L1 Loss: 17146.5625\n",
            "Batch 690, Loss: 25835.5430, L2 Loss: 10970.4639, L1 Loss: 14865.0801\n",
            "Batch 700, Loss: 26716.2773, L2 Loss: 11079.0225, L1 Loss: 15637.2539\n",
            "Batch 710, Loss: 25338.5703, L2 Loss: 10888.9121, L1 Loss: 14449.6592\n",
            "Batch 720, Loss: 26765.2695, L2 Loss: 11036.7090, L1 Loss: 15728.5615\n",
            "Batch 730, Loss: 25015.5938, L2 Loss: 10884.7480, L1 Loss: 14130.8447\n",
            "Batch 740, Loss: 25285.0703, L2 Loss: 10901.0381, L1 Loss: 14384.0332\n",
            "Batch 750, Loss: 24875.8086, L2 Loss: 10870.8359, L1 Loss: 14004.9736\n",
            "Batch 760, Loss: 24840.7266, L2 Loss: 10847.2275, L1 Loss: 13993.4980\n",
            "Batch 770, Loss: 24681.2344, L2 Loss: 10802.3896, L1 Loss: 13878.8457\n",
            "Batch 780, Loss: 24768.8438, L2 Loss: 10775.8281, L1 Loss: 13993.0156\n",
            "Batch 790, Loss: 25040.5391, L2 Loss: 10807.1748, L1 Loss: 14233.3643\n",
            "Batch 800, Loss: 24461.5859, L2 Loss: 10743.3311, L1 Loss: 13718.2539\n",
            "Batch 810, Loss: 23488.5781, L2 Loss: 10708.1943, L1 Loss: 12780.3848\n",
            "Batch 820, Loss: 23818.0312, L2 Loss: 10721.9004, L1 Loss: 13096.1309\n",
            "Batch 830, Loss: 23313.0117, L2 Loss: 10691.8408, L1 Loss: 12621.1709\n",
            "Batch 840, Loss: 22496.1875, L2 Loss: 10616.3467, L1 Loss: 11879.8398\n",
            "Batch 850, Loss: 22510.2422, L2 Loss: 10578.2119, L1 Loss: 11932.0293\n",
            "Batch 860, Loss: 22212.6016, L2 Loss: 10553.5947, L1 Loss: 11659.0059\n",
            "Batch 870, Loss: 23643.1836, L2 Loss: 10603.1221, L1 Loss: 13040.0625\n",
            "Batch 880, Loss: 22820.2148, L2 Loss: 10567.8369, L1 Loss: 12252.3770\n",
            "Batch 890, Loss: 22776.7070, L2 Loss: 10530.6611, L1 Loss: 12246.0459\n",
            "Batch 900, Loss: 22518.4844, L2 Loss: 10511.4756, L1 Loss: 12007.0088\n",
            "Batch 910, Loss: 22690.7383, L2 Loss: 10503.5547, L1 Loss: 12187.1826\n",
            "Batch 920, Loss: 22159.3633, L2 Loss: 10484.7314, L1 Loss: 11674.6318\n",
            "Batch 930, Loss: 21758.1016, L2 Loss: 10425.9326, L1 Loss: 11332.1689\n",
            "Batch 940, Loss: 21104.9492, L2 Loss: 10453.6885, L1 Loss: 10651.2598\n",
            "Batch 950, Loss: 21821.4219, L2 Loss: 10396.5273, L1 Loss: 11424.8955\n",
            "Batch 960, Loss: 21502.9531, L2 Loss: 10411.3369, L1 Loss: 11091.6172\n",
            "Batch 970, Loss: 21921.4141, L2 Loss: 10334.5645, L1 Loss: 11586.8506\n",
            "Batch 980, Loss: 22138.3633, L2 Loss: 10325.6562, L1 Loss: 11812.7070\n",
            "Batch 990, Loss: 20645.1211, L2 Loss: 10332.3506, L1 Loss: 10312.7695\n",
            "Batch 1000, Loss: 22012.1406, L2 Loss: 10265.0918, L1 Loss: 11747.0479\n",
            "Batch 1010, Loss: 21502.3555, L2 Loss: 10263.9785, L1 Loss: 11238.3770\n",
            "Batch 1020, Loss: 21385.9004, L2 Loss: 10227.9736, L1 Loss: 11157.9268\n",
            "Batch 1030, Loss: 21187.8906, L2 Loss: 10179.3506, L1 Loss: 11008.5410\n",
            "Batch 1040, Loss: 20764.8750, L2 Loss: 10219.2852, L1 Loss: 10545.5908\n",
            "Batch 1050, Loss: 20581.7109, L2 Loss: 10135.7080, L1 Loss: 10446.0039\n",
            "Batch 1060, Loss: 20809.1016, L2 Loss: 10085.8604, L1 Loss: 10723.2412\n",
            "Batch 1070, Loss: 19667.2168, L2 Loss: 10082.5771, L1 Loss: 9584.6396\n",
            "Batch 1080, Loss: 20879.1406, L2 Loss: 10016.2402, L1 Loss: 10862.8994\n",
            "Batch 1090, Loss: 20816.5176, L2 Loss: 9965.5410, L1 Loss: 10850.9766\n",
            "Batch 1100, Loss: 19911.4883, L2 Loss: 9949.5889, L1 Loss: 9961.8994\n",
            "Batch 1110, Loss: 19961.3867, L2 Loss: 9917.5996, L1 Loss: 10043.7871\n",
            "Batch 1120, Loss: 20259.1641, L2 Loss: 9878.4902, L1 Loss: 10380.6748\n",
            "Batch 1130, Loss: 20015.5977, L2 Loss: 9813.8662, L1 Loss: 10201.7324\n",
            "Batch 1140, Loss: 19922.4922, L2 Loss: 9745.2568, L1 Loss: 10177.2344\n",
            "Batch 1150, Loss: 20339.2539, L2 Loss: 9715.7051, L1 Loss: 10623.5479\n",
            "Batch 1160, Loss: 19815.1523, L2 Loss: 9661.8506, L1 Loss: 10153.3018\n",
            "Batch 1170, Loss: 19620.7578, L2 Loss: 9588.1406, L1 Loss: 10032.6182\n",
            "Batch 1180, Loss: 19286.2363, L2 Loss: 9525.6064, L1 Loss: 9760.6299\n",
            "Batch 1190, Loss: 19779.8887, L2 Loss: 9450.0869, L1 Loss: 10329.8018\n",
            "Batch 1200, Loss: 19203.6699, L2 Loss: 9400.6260, L1 Loss: 9803.0439\n",
            "Batch 1210, Loss: 19589.7129, L2 Loss: 9337.3613, L1 Loss: 10252.3516\n",
            "Batch 1220, Loss: 19115.7168, L2 Loss: 9365.1543, L1 Loss: 9750.5625\n",
            "Batch 1230, Loss: 19567.1328, L2 Loss: 9236.9648, L1 Loss: 10330.1680\n",
            "Batch 1240, Loss: 19291.6914, L2 Loss: 9180.6865, L1 Loss: 10111.0059\n",
            "Batch 1250, Loss: 19125.5273, L2 Loss: 9104.3750, L1 Loss: 10021.1514\n",
            "Batch 1260, Loss: 18787.2266, L2 Loss: 9023.8164, L1 Loss: 9763.4102\n",
            "Batch 1270, Loss: 18624.1094, L2 Loss: 8970.2129, L1 Loss: 9653.8965\n",
            "Batch 1280, Loss: 19017.3164, L2 Loss: 8924.8525, L1 Loss: 10092.4639\n",
            "Batch 1290, Loss: 18311.6445, L2 Loss: 8820.0762, L1 Loss: 9491.5674\n",
            "Batch 1300, Loss: 19147.0703, L2 Loss: 8762.8682, L1 Loss: 10384.2031\n",
            "Batch 1310, Loss: 18289.4434, L2 Loss: 8690.4775, L1 Loss: 9598.9658\n",
            "Batch 1320, Loss: 18334.5762, L2 Loss: 8630.3428, L1 Loss: 9704.2334\n",
            "Batch 1330, Loss: 19005.2539, L2 Loss: 8564.5400, L1 Loss: 10440.7129\n",
            "Batch 1340, Loss: 18810.3008, L2 Loss: 8491.6787, L1 Loss: 10318.6230\n",
            "Batch 1350, Loss: 18284.5996, L2 Loss: 8431.5205, L1 Loss: 9853.0791\n",
            "Batch 1360, Loss: 19022.0508, L2 Loss: 8367.1230, L1 Loss: 10654.9277\n",
            "Batch 1370, Loss: 18459.6523, L2 Loss: 8291.7490, L1 Loss: 10167.9033\n",
            "Batch 1380, Loss: 18206.4219, L2 Loss: 8258.8867, L1 Loss: 9947.5352\n",
            "Batch 1390, Loss: 17897.7617, L2 Loss: 8175.6812, L1 Loss: 9722.0801\n",
            "Batch 1400, Loss: 17874.5137, L2 Loss: 8105.4282, L1 Loss: 9769.0850\n",
            "Batch 1410, Loss: 17946.2051, L2 Loss: 8064.8755, L1 Loss: 9881.3301\n",
            "Batch 1420, Loss: 17349.5566, L2 Loss: 8001.9624, L1 Loss: 9347.5947\n",
            "Batch 1430, Loss: 17768.5176, L2 Loss: 7914.8091, L1 Loss: 9853.7080\n",
            "Batch 1440, Loss: 18296.8750, L2 Loss: 7848.8164, L1 Loss: 10448.0576\n",
            "Batch 1450, Loss: 17615.2207, L2 Loss: 7809.4648, L1 Loss: 9805.7559\n",
            "Batch 1460, Loss: 17331.1836, L2 Loss: 7767.7847, L1 Loss: 9563.3984\n",
            "Batch 1470, Loss: 18041.3418, L2 Loss: 7680.9497, L1 Loss: 10360.3916\n",
            "Batch 1480, Loss: 17685.7910, L2 Loss: 7611.0107, L1 Loss: 10074.7803\n",
            "Batch 1490, Loss: 17532.3691, L2 Loss: 7591.9766, L1 Loss: 9940.3926\n",
            "Batch 1500, Loss: 17588.0801, L2 Loss: 7531.6211, L1 Loss: 10056.4590\n",
            "Batch 1510, Loss: 17608.7617, L2 Loss: 7458.2485, L1 Loss: 10150.5127\n",
            "Batch 1520, Loss: 18239.0117, L2 Loss: 7380.2432, L1 Loss: 10858.7686\n",
            "Batch 1530, Loss: 17514.7734, L2 Loss: 7366.6035, L1 Loss: 10148.1689\n",
            "Batch 1540, Loss: 17851.3047, L2 Loss: 7331.7915, L1 Loss: 10519.5137\n",
            "Batch 1550, Loss: 17123.4688, L2 Loss: 7271.2100, L1 Loss: 9852.2588\n",
            "Batch 1560, Loss: 17077.5312, L2 Loss: 7259.0176, L1 Loss: 9818.5137\n",
            "Batch 1570, Loss: 17418.7246, L2 Loss: 7196.2529, L1 Loss: 10222.4717\n",
            "Batch 1580, Loss: 16678.0742, L2 Loss: 7169.2935, L1 Loss: 9508.7812\n",
            "Batch 1590, Loss: 17235.2480, L2 Loss: 7090.9087, L1 Loss: 10144.3398\n",
            "Batch 1600, Loss: 16988.3535, L2 Loss: 7104.1128, L1 Loss: 9884.2412\n",
            "Batch 1610, Loss: 16935.9453, L2 Loss: 7048.4102, L1 Loss: 9887.5342\n",
            "Batch 1620, Loss: 17086.8223, L2 Loss: 6961.2573, L1 Loss: 10125.5645\n",
            "Batch 1630, Loss: 16799.8164, L2 Loss: 6993.9902, L1 Loss: 9805.8252\n",
            "Batch 1640, Loss: 17026.0469, L2 Loss: 6915.0444, L1 Loss: 10111.0029\n",
            "Batch 1650, Loss: 17212.1973, L2 Loss: 6865.4707, L1 Loss: 10346.7266\n",
            "Batch 1660, Loss: 16987.3828, L2 Loss: 6807.9092, L1 Loss: 10179.4736\n",
            "Batch 1670, Loss: 16200.6797, L2 Loss: 6775.6558, L1 Loss: 9425.0244\n",
            "Batch 1680, Loss: 15867.3965, L2 Loss: 6810.3081, L1 Loss: 9057.0889\n",
            "Batch 1690, Loss: 16903.1953, L2 Loss: 6723.3311, L1 Loss: 10179.8633\n",
            "Batch 1700, Loss: 16420.8398, L2 Loss: 6714.0371, L1 Loss: 9706.8037\n",
            "Batch 1710, Loss: 16159.6611, L2 Loss: 6662.9902, L1 Loss: 9496.6709\n",
            "Batch 1720, Loss: 15705.2285, L2 Loss: 6642.5454, L1 Loss: 9062.6836\n",
            "Batch 1730, Loss: 16036.8418, L2 Loss: 6604.1045, L1 Loss: 9432.7373\n",
            "Batch 1740, Loss: 16545.5156, L2 Loss: 6544.4092, L1 Loss: 10001.1064\n",
            "Batch 1750, Loss: 16250.4941, L2 Loss: 6490.8584, L1 Loss: 9759.6357\n",
            "Batch 1760, Loss: 16127.7793, L2 Loss: 6471.2524, L1 Loss: 9656.5273\n",
            "Batch 1770, Loss: 16117.2832, L2 Loss: 6434.3218, L1 Loss: 9682.9609\n",
            "Batch 1780, Loss: 15719.4170, L2 Loss: 6412.9492, L1 Loss: 9306.4678\n",
            "Batch 1790, Loss: 16690.4395, L2 Loss: 6335.3652, L1 Loss: 10355.0742\n",
            "Batch 1800, Loss: 16278.6758, L2 Loss: 6349.1421, L1 Loss: 9929.5342\n",
            "Batch 1810, Loss: 16110.7168, L2 Loss: 6297.9458, L1 Loss: 9812.7705\n",
            "Batch 1820, Loss: 15641.9902, L2 Loss: 6289.2192, L1 Loss: 9352.7705\n",
            "Batch 1830, Loss: 15782.1152, L2 Loss: 6266.0293, L1 Loss: 9516.0859\n",
            "Batch 1840, Loss: 16057.8135, L2 Loss: 6210.4189, L1 Loss: 9847.3945\n",
            "Batch 1850, Loss: 15730.9033, L2 Loss: 6184.5312, L1 Loss: 9546.3721\n",
            "Batch 1860, Loss: 15629.4326, L2 Loss: 6157.1523, L1 Loss: 9472.2803\n",
            "Batch 1870, Loss: 16170.9883, L2 Loss: 6092.1230, L1 Loss: 10078.8652\n",
            "Batch 1880, Loss: 15441.0410, L2 Loss: 6097.3481, L1 Loss: 9343.6934\n",
            "Batch 1890, Loss: 15388.1436, L2 Loss: 6093.1396, L1 Loss: 9295.0039\n",
            "Batch 1900, Loss: 15509.1641, L2 Loss: 6021.1743, L1 Loss: 9487.9893\n",
            "Batch 1910, Loss: 15580.4512, L2 Loss: 6004.4185, L1 Loss: 9576.0332\n",
            "Batch 1920, Loss: 15524.1553, L2 Loss: 5953.7559, L1 Loss: 9570.3994\n",
            "Batch 1930, Loss: 15393.3398, L2 Loss: 5935.9883, L1 Loss: 9457.3516\n",
            "Batch 1940, Loss: 15559.8340, L2 Loss: 5905.4282, L1 Loss: 9654.4062\n",
            "Batch 1950, Loss: 15433.0801, L2 Loss: 5913.6313, L1 Loss: 9519.4492\n",
            "Batch 1960, Loss: 15107.1035, L2 Loss: 5880.2485, L1 Loss: 9226.8555\n",
            "Batch 1970, Loss: 15495.5723, L2 Loss: 5855.6714, L1 Loss: 9639.9014\n",
            "Batch 1980, Loss: 15099.9434, L2 Loss: 5778.5112, L1 Loss: 9321.4316\n",
            "Batch 1990, Loss: 15115.3320, L2 Loss: 5800.8013, L1 Loss: 9314.5312\n",
            "Batch 2000, Loss: 15404.5801, L2 Loss: 5724.9673, L1 Loss: 9679.6123\n",
            "Batch 2010, Loss: 14907.1641, L2 Loss: 5717.4111, L1 Loss: 9189.7529\n",
            "Batch 2020, Loss: 15283.3379, L2 Loss: 5695.6001, L1 Loss: 9587.7373\n",
            "Batch 2030, Loss: 15628.8350, L2 Loss: 5671.8027, L1 Loss: 9957.0322\n",
            "Batch 2040, Loss: 14963.6367, L2 Loss: 5656.2573, L1 Loss: 9307.3789\n",
            "Batch 2050, Loss: 15319.3896, L2 Loss: 5615.2773, L1 Loss: 9704.1123\n",
            "Batch 2060, Loss: 14566.0898, L2 Loss: 5588.6353, L1 Loss: 8977.4551\n",
            "Batch 2070, Loss: 14534.6523, L2 Loss: 5562.0991, L1 Loss: 8972.5527\n",
            "Batch 2080, Loss: 15193.4229, L2 Loss: 5550.6641, L1 Loss: 9642.7588\n",
            "Batch 2090, Loss: 14874.1387, L2 Loss: 5520.0684, L1 Loss: 9354.0703\n",
            "Batch 2100, Loss: 14816.4102, L2 Loss: 5521.8472, L1 Loss: 9294.5635\n",
            "Batch 2110, Loss: 15268.2617, L2 Loss: 5487.2183, L1 Loss: 9781.0430\n",
            "Batch 2120, Loss: 14669.9219, L2 Loss: 5465.6899, L1 Loss: 9204.2324\n",
            "Batch 2130, Loss: 15198.7832, L2 Loss: 5449.7461, L1 Loss: 9749.0371\n",
            "Batch 2140, Loss: 14163.7227, L2 Loss: 5403.4048, L1 Loss: 8760.3174\n",
            "Batch 2150, Loss: 14550.6074, L2 Loss: 5382.5083, L1 Loss: 9168.0996\n",
            "Batch 2160, Loss: 14349.5488, L2 Loss: 5355.3296, L1 Loss: 8994.2197\n",
            "Batch 2170, Loss: 14900.8281, L2 Loss: 5319.9194, L1 Loss: 9580.9092\n",
            "Batch 2180, Loss: 14866.2773, L2 Loss: 5306.7505, L1 Loss: 9559.5264\n",
            "Batch 2190, Loss: 14218.2314, L2 Loss: 5325.1621, L1 Loss: 8893.0693\n",
            "Batch 2200, Loss: 14006.5830, L2 Loss: 5266.7402, L1 Loss: 8739.8428\n",
            "Batch 2210, Loss: 14551.8975, L2 Loss: 5263.6416, L1 Loss: 9288.2559\n",
            "Batch 2220, Loss: 14481.4824, L2 Loss: 5202.6562, L1 Loss: 9278.8262\n",
            "Batch 2230, Loss: 14206.4316, L2 Loss: 5177.4609, L1 Loss: 9028.9707\n",
            "Batch 2240, Loss: 14176.1602, L2 Loss: 5165.4873, L1 Loss: 9010.6729\n",
            "Batch 2250, Loss: 14141.2871, L2 Loss: 5132.6855, L1 Loss: 9008.6016\n",
            "Batch 2260, Loss: 14269.5840, L2 Loss: 5141.6255, L1 Loss: 9127.9580\n",
            "Batch 2270, Loss: 13892.3223, L2 Loss: 5092.9253, L1 Loss: 8799.3975\n",
            "Batch 2280, Loss: 13859.5332, L2 Loss: 5063.0933, L1 Loss: 8796.4404\n",
            "Batch 2290, Loss: 14142.4668, L2 Loss: 5070.8110, L1 Loss: 9071.6562\n",
            "Batch 2300, Loss: 13860.6582, L2 Loss: 5072.2046, L1 Loss: 8788.4531\n",
            "Batch 2310, Loss: 14076.6270, L2 Loss: 5036.8062, L1 Loss: 9039.8213\n",
            "Batch 2320, Loss: 14237.0088, L2 Loss: 5015.8750, L1 Loss: 9221.1338\n",
            "Batch 2330, Loss: 14516.4727, L2 Loss: 4996.2983, L1 Loss: 9520.1748\n",
            "Batch 2340, Loss: 13757.6787, L2 Loss: 4989.0791, L1 Loss: 8768.5996\n",
            "Batch 2350, Loss: 13638.8496, L2 Loss: 4973.5234, L1 Loss: 8665.3262\n",
            "Batch 2360, Loss: 13887.1641, L2 Loss: 4973.0981, L1 Loss: 8914.0664\n",
            "Batch 2370, Loss: 13771.0879, L2 Loss: 4906.6323, L1 Loss: 8864.4551\n",
            "Batch 2380, Loss: 13440.2246, L2 Loss: 4866.9751, L1 Loss: 8573.2500\n",
            "Batch 2390, Loss: 14080.2031, L2 Loss: 4863.5244, L1 Loss: 9216.6787\n",
            "Batch 2400, Loss: 14114.9453, L2 Loss: 4856.3184, L1 Loss: 9258.6270\n",
            "Batch 2410, Loss: 13728.6973, L2 Loss: 4824.0430, L1 Loss: 8904.6543\n",
            "Batch 2420, Loss: 14085.0410, L2 Loss: 4832.8062, L1 Loss: 9252.2344\n",
            "Batch 2430, Loss: 13516.5137, L2 Loss: 4781.6792, L1 Loss: 8734.8340\n",
            "Batch 2440, Loss: 13664.8164, L2 Loss: 4766.7173, L1 Loss: 8898.0986\n",
            "Batch 2450, Loss: 13351.2676, L2 Loss: 4750.8022, L1 Loss: 8600.4648\n",
            "Batch 2460, Loss: 14106.9727, L2 Loss: 4729.9482, L1 Loss: 9377.0244\n",
            "Batch 2470, Loss: 13758.1445, L2 Loss: 4728.3950, L1 Loss: 9029.7500\n",
            "Batch 2480, Loss: 13622.0332, L2 Loss: 4708.2544, L1 Loss: 8913.7783\n",
            "Batch 2490, Loss: 13001.0957, L2 Loss: 4674.8608, L1 Loss: 8326.2344\n",
            "Batch 2500, Loss: 13725.6387, L2 Loss: 4672.4053, L1 Loss: 9053.2334\n",
            "Batch 2510, Loss: 13581.7090, L2 Loss: 4650.3628, L1 Loss: 8931.3467\n",
            "Batch 2520, Loss: 12892.4258, L2 Loss: 4633.5391, L1 Loss: 8258.8867\n",
            "Batch 2530, Loss: 13419.3154, L2 Loss: 4596.6621, L1 Loss: 8822.6533\n",
            "Batch 2540, Loss: 13691.8916, L2 Loss: 4592.9678, L1 Loss: 9098.9238\n",
            "Batch 2550, Loss: 13086.0156, L2 Loss: 4581.7300, L1 Loss: 8504.2852\n",
            "Batch 2560, Loss: 13237.1885, L2 Loss: 4535.3496, L1 Loss: 8701.8389\n",
            "Batch 2570, Loss: 13570.8281, L2 Loss: 4542.8130, L1 Loss: 9028.0156\n",
            "Batch 2580, Loss: 13025.9102, L2 Loss: 4557.8833, L1 Loss: 8468.0273\n",
            "Batch 2590, Loss: 13307.6152, L2 Loss: 4500.0410, L1 Loss: 8807.5742\n",
            "Batch 2600, Loss: 12888.2344, L2 Loss: 4498.3003, L1 Loss: 8389.9336\n",
            "Batch 2610, Loss: 13213.8057, L2 Loss: 4468.0352, L1 Loss: 8745.7705\n",
            "Batch 2620, Loss: 13274.5098, L2 Loss: 4451.0933, L1 Loss: 8823.4170\n",
            "Batch 2630, Loss: 12956.2832, L2 Loss: 4429.3022, L1 Loss: 8526.9805\n",
            "Batch 2640, Loss: 13224.7695, L2 Loss: 4415.8862, L1 Loss: 8808.8828\n",
            "Batch 2650, Loss: 12858.8584, L2 Loss: 4399.3291, L1 Loss: 8459.5293\n",
            "Batch 2660, Loss: 13066.2109, L2 Loss: 4337.3374, L1 Loss: 8728.8730\n",
            "Batch 2670, Loss: 12769.4277, L2 Loss: 4352.3062, L1 Loss: 8417.1221\n",
            "Batch 2680, Loss: 12846.6445, L2 Loss: 4354.9780, L1 Loss: 8491.6670\n",
            "Batch 2690, Loss: 12801.5586, L2 Loss: 4340.6138, L1 Loss: 8460.9443\n",
            "Batch 2700, Loss: 12501.5176, L2 Loss: 4306.2114, L1 Loss: 8195.3066\n",
            "Batch 2710, Loss: 13004.6025, L2 Loss: 4342.7158, L1 Loss: 8661.8867\n",
            "Batch 2720, Loss: 13095.2344, L2 Loss: 4307.4092, L1 Loss: 8787.8252\n",
            "Batch 2730, Loss: 12907.2471, L2 Loss: 4285.5225, L1 Loss: 8621.7246\n",
            "Batch 2740, Loss: 12630.5801, L2 Loss: 4272.1870, L1 Loss: 8358.3926\n",
            "Batch 2750, Loss: 12803.6035, L2 Loss: 4265.9761, L1 Loss: 8537.6279\n",
            "Batch 2760, Loss: 12487.0254, L2 Loss: 4236.7744, L1 Loss: 8250.2510\n",
            "Batch 2770, Loss: 12551.5801, L2 Loss: 4234.5024, L1 Loss: 8317.0781\n",
            "Batch 2780, Loss: 12667.0713, L2 Loss: 4188.6699, L1 Loss: 8478.4014\n",
            "Batch 2790, Loss: 12559.2354, L2 Loss: 4184.3457, L1 Loss: 8374.8896\n",
            "Batch 2800, Loss: 12794.4160, L2 Loss: 4173.4282, L1 Loss: 8620.9873\n",
            "Batch 2810, Loss: 12842.7070, L2 Loss: 4172.8335, L1 Loss: 8669.8730\n",
            "Batch 2820, Loss: 12432.7217, L2 Loss: 4172.7549, L1 Loss: 8259.9668\n",
            "Batch 2830, Loss: 12879.8740, L2 Loss: 4140.4004, L1 Loss: 8739.4736\n",
            "Batch 2840, Loss: 12499.3867, L2 Loss: 4109.0396, L1 Loss: 8390.3477\n",
            "Batch 2850, Loss: 12568.4316, L2 Loss: 4105.3394, L1 Loss: 8463.0928\n",
            "Batch 2860, Loss: 12636.5361, L2 Loss: 4084.5349, L1 Loss: 8552.0010\n",
            "Batch 2870, Loss: 12079.7217, L2 Loss: 4090.5002, L1 Loss: 7989.2212\n",
            "Batch 2880, Loss: 12842.2715, L2 Loss: 4085.6157, L1 Loss: 8756.6553\n",
            "Batch 2890, Loss: 12763.9727, L2 Loss: 4073.6970, L1 Loss: 8690.2754\n",
            "Batch 2900, Loss: 12239.0742, L2 Loss: 4071.3108, L1 Loss: 8167.7632\n",
            "Batch 2910, Loss: 12145.4775, L2 Loss: 4039.0867, L1 Loss: 8106.3911\n",
            "Batch 2920, Loss: 12540.0918, L2 Loss: 4037.9988, L1 Loss: 8502.0928\n",
            "Batch 2930, Loss: 12353.5146, L2 Loss: 4013.1956, L1 Loss: 8340.3193\n",
            "Batch 2940, Loss: 12198.2031, L2 Loss: 4001.6416, L1 Loss: 8196.5615\n",
            "Batch 2950, Loss: 12178.4297, L2 Loss: 3977.9092, L1 Loss: 8200.5205\n",
            "Batch 2960, Loss: 12132.2432, L2 Loss: 3984.0349, L1 Loss: 8148.2080\n",
            "Batch 2970, Loss: 12215.2500, L2 Loss: 3933.2566, L1 Loss: 8281.9932\n",
            "Batch 2980, Loss: 12210.0312, L2 Loss: 3953.2102, L1 Loss: 8256.8213\n",
            "Batch 2990, Loss: 12160.8066, L2 Loss: 3929.7712, L1 Loss: 8231.0352\n",
            "Batch 3000, Loss: 12429.0684, L2 Loss: 3918.1567, L1 Loss: 8510.9121\n",
            "Batch 3010, Loss: 12419.4707, L2 Loss: 3881.5713, L1 Loss: 8537.8994\n",
            "Batch 3020, Loss: 12146.1445, L2 Loss: 3895.5115, L1 Loss: 8250.6328\n",
            "Batch 3030, Loss: 12397.2031, L2 Loss: 3895.1863, L1 Loss: 8502.0166\n",
            "Batch 3040, Loss: 12231.3350, L2 Loss: 3861.4192, L1 Loss: 8369.9160\n",
            "Batch 3050, Loss: 12018.8086, L2 Loss: 3858.2593, L1 Loss: 8160.5498\n",
            "Batch 3060, Loss: 12374.3525, L2 Loss: 3851.5227, L1 Loss: 8522.8301\n",
            "Batch 3070, Loss: 12119.9971, L2 Loss: 3859.2812, L1 Loss: 8260.7158\n",
            "Batch 3080, Loss: 12075.5049, L2 Loss: 3824.5847, L1 Loss: 8250.9199\n",
            "Batch 3090, Loss: 11445.6807, L2 Loss: 3819.6721, L1 Loss: 7626.0088\n",
            "Batch 3100, Loss: 11992.0293, L2 Loss: 3826.4521, L1 Loss: 8165.5767\n",
            "Batch 3110, Loss: 11933.1338, L2 Loss: 3789.7029, L1 Loss: 8143.4307\n",
            "Batch 3120, Loss: 11875.9980, L2 Loss: 3787.7007, L1 Loss: 8088.2979\n",
            "Batch 3130, Loss: 12013.4287, L2 Loss: 3794.4993, L1 Loss: 8218.9297\n",
            "Batch 3140, Loss: 12086.8623, L2 Loss: 3729.0828, L1 Loss: 8357.7793\n",
            "Batch 3150, Loss: 12015.2080, L2 Loss: 3763.2493, L1 Loss: 8251.9590\n",
            "Batch 3160, Loss: 11805.5586, L2 Loss: 3768.2327, L1 Loss: 8037.3257\n",
            "Batch 3170, Loss: 12063.2520, L2 Loss: 3731.5029, L1 Loss: 8331.7490\n",
            "Batch 3180, Loss: 11634.5664, L2 Loss: 3735.1150, L1 Loss: 7899.4512\n",
            "Batch 3190, Loss: 12252.3271, L2 Loss: 3741.6941, L1 Loss: 8510.6328\n",
            "Batch 3200, Loss: 11677.1240, L2 Loss: 3711.2307, L1 Loss: 7965.8936\n",
            "Batch 3210, Loss: 11615.0576, L2 Loss: 3685.3828, L1 Loss: 7929.6748\n",
            "Batch 3220, Loss: 11713.6172, L2 Loss: 3678.6118, L1 Loss: 8035.0054\n",
            "Batch 3230, Loss: 11620.6182, L2 Loss: 3668.5686, L1 Loss: 7952.0498\n",
            "Batch 3240, Loss: 11683.0059, L2 Loss: 3671.7881, L1 Loss: 8011.2173\n",
            "Batch 3250, Loss: 11694.5684, L2 Loss: 3643.8394, L1 Loss: 8050.7295\n",
            "Batch 3260, Loss: 11685.4424, L2 Loss: 3621.8896, L1 Loss: 8063.5527\n",
            "Batch 3270, Loss: 11546.5898, L2 Loss: 3623.1873, L1 Loss: 7923.4023\n",
            "Batch 3280, Loss: 11822.4883, L2 Loss: 3633.6033, L1 Loss: 8188.8848\n",
            "Batch 3290, Loss: 11876.2100, L2 Loss: 3611.1145, L1 Loss: 8265.0957\n",
            "Batch 3300, Loss: 11659.8623, L2 Loss: 3593.8582, L1 Loss: 8066.0039\n",
            "Batch 3310, Loss: 11774.4307, L2 Loss: 3597.2764, L1 Loss: 8177.1543\n",
            "Batch 3320, Loss: 11502.2275, L2 Loss: 3569.7312, L1 Loss: 7932.4961\n",
            "Batch 3330, Loss: 11173.1582, L2 Loss: 3574.2285, L1 Loss: 7598.9302\n",
            "Batch 3340, Loss: 11385.6797, L2 Loss: 3580.6133, L1 Loss: 7805.0664\n",
            "Batch 3350, Loss: 11810.1768, L2 Loss: 3595.8381, L1 Loss: 8214.3389\n",
            "Batch 3360, Loss: 11160.4922, L2 Loss: 3553.2061, L1 Loss: 7607.2856\n",
            "Batch 3370, Loss: 11275.5391, L2 Loss: 3541.6331, L1 Loss: 7733.9058\n",
            "Batch 3380, Loss: 11241.6211, L2 Loss: 3542.3042, L1 Loss: 7699.3164\n",
            "Batch 3390, Loss: 11490.2051, L2 Loss: 3545.5842, L1 Loss: 7944.6206\n",
            "Batch 3400, Loss: 11088.6562, L2 Loss: 3515.8662, L1 Loss: 7572.7900\n",
            "Batch 3410, Loss: 11250.0820, L2 Loss: 3520.0508, L1 Loss: 7730.0312\n",
            "Batch 3420, Loss: 11355.7139, L2 Loss: 3519.3010, L1 Loss: 7836.4131\n",
            "Batch 3430, Loss: 11354.0781, L2 Loss: 3498.0312, L1 Loss: 7856.0474\n",
            "Batch 3440, Loss: 11334.0586, L2 Loss: 3493.6982, L1 Loss: 7840.3599\n",
            "Batch 3450, Loss: 11511.3447, L2 Loss: 3483.3081, L1 Loss: 8028.0366\n",
            "Batch 3460, Loss: 11198.5176, L2 Loss: 3436.6580, L1 Loss: 7761.8594\n",
            "Batch 3470, Loss: 11075.1377, L2 Loss: 3446.0320, L1 Loss: 7629.1060\n",
            "Batch 3480, Loss: 11084.2793, L2 Loss: 3438.0239, L1 Loss: 7646.2554\n",
            "Batch 3490, Loss: 11269.1562, L2 Loss: 3459.7683, L1 Loss: 7809.3882\n",
            "Batch 3500, Loss: 10859.4219, L2 Loss: 3406.1611, L1 Loss: 7453.2603\n",
            "Batch 3510, Loss: 11464.1270, L2 Loss: 3434.9517, L1 Loss: 8029.1748\n",
            "Batch 3520, Loss: 11253.7637, L2 Loss: 3409.4026, L1 Loss: 7844.3608\n",
            "Batch 3530, Loss: 11241.9180, L2 Loss: 3417.5051, L1 Loss: 7824.4131\n",
            "Batch 3540, Loss: 10999.8193, L2 Loss: 3402.5618, L1 Loss: 7597.2578\n",
            "Batch 3550, Loss: 10938.1514, L2 Loss: 3390.0623, L1 Loss: 7548.0894\n",
            "Batch 3560, Loss: 10736.7207, L2 Loss: 3392.9517, L1 Loss: 7343.7686\n",
            "Batch 3570, Loss: 10766.0547, L2 Loss: 3364.1621, L1 Loss: 7401.8921\n",
            "Batch 3580, Loss: 11127.0820, L2 Loss: 3371.6265, L1 Loss: 7755.4551\n",
            "Batch 3590, Loss: 10708.9053, L2 Loss: 3368.8647, L1 Loss: 7340.0405\n",
            "Batch 3600, Loss: 11363.1494, L2 Loss: 3345.5867, L1 Loss: 8017.5630\n",
            "Batch 3610, Loss: 10927.0752, L2 Loss: 3349.5325, L1 Loss: 7577.5430\n",
            "Batch 3620, Loss: 10663.2871, L2 Loss: 3341.6077, L1 Loss: 7321.6797\n",
            "Batch 3630, Loss: 10636.1367, L2 Loss: 3327.8706, L1 Loss: 7308.2666\n",
            "Batch 3640, Loss: 11142.2852, L2 Loss: 3339.8145, L1 Loss: 7802.4712\n",
            "Batch 3650, Loss: 10839.8125, L2 Loss: 3298.3677, L1 Loss: 7541.4448\n",
            "Batch 3660, Loss: 10885.1406, L2 Loss: 3292.8711, L1 Loss: 7592.2695\n",
            "Batch 3670, Loss: 10798.2803, L2 Loss: 3316.2874, L1 Loss: 7481.9932\n",
            "Batch 3680, Loss: 10958.4531, L2 Loss: 3311.5276, L1 Loss: 7646.9258\n",
            "Batch 3690, Loss: 10976.7969, L2 Loss: 3269.4312, L1 Loss: 7707.3662\n",
            "Batch 3700, Loss: 10648.1699, L2 Loss: 3260.0447, L1 Loss: 7388.1255\n",
            "Batch 3710, Loss: 11377.8545, L2 Loss: 3295.8616, L1 Loss: 8081.9927\n",
            "Batch 3720, Loss: 10896.3174, L2 Loss: 3251.2966, L1 Loss: 7645.0210\n",
            "Batch 3730, Loss: 10914.4795, L2 Loss: 3258.0847, L1 Loss: 7656.3945\n",
            "Batch 3740, Loss: 10940.8193, L2 Loss: 3249.1873, L1 Loss: 7691.6318\n",
            "Batch 3750, Loss: 10577.5244, L2 Loss: 3230.0278, L1 Loss: 7347.4966\n",
            "Batch 3760, Loss: 10505.8389, L2 Loss: 3225.9041, L1 Loss: 7279.9346\n",
            "Batch 3770, Loss: 10512.6963, L2 Loss: 3214.0564, L1 Loss: 7298.6396\n",
            "Batch 3780, Loss: 10581.6455, L2 Loss: 3205.1982, L1 Loss: 7376.4473\n",
            "Batch 3790, Loss: 10480.7305, L2 Loss: 3207.6987, L1 Loss: 7273.0312\n",
            "Batch 3800, Loss: 10440.2510, L2 Loss: 3209.7551, L1 Loss: 7230.4961\n",
            "Batch 3810, Loss: 10625.2021, L2 Loss: 3200.3960, L1 Loss: 7424.8062\n",
            "Batch 3820, Loss: 10443.8340, L2 Loss: 3191.9087, L1 Loss: 7251.9258\n",
            "Batch 3830, Loss: 10581.7266, L2 Loss: 3200.6147, L1 Loss: 7381.1113\n",
            "Batch 3840, Loss: 10627.9502, L2 Loss: 3181.3884, L1 Loss: 7446.5620\n",
            "Batch 3850, Loss: 10603.0010, L2 Loss: 3174.0901, L1 Loss: 7428.9111\n",
            "Batch 3860, Loss: 10424.8379, L2 Loss: 3195.4272, L1 Loss: 7229.4111\n",
            "Batch 3870, Loss: 10723.0137, L2 Loss: 3179.5417, L1 Loss: 7543.4717\n",
            "Batch 3880, Loss: 10997.2588, L2 Loss: 3180.5381, L1 Loss: 7816.7207\n",
            "Batch 3890, Loss: 10348.3887, L2 Loss: 3137.4971, L1 Loss: 7210.8916\n",
            "Batch 3900, Loss: 10183.8672, L2 Loss: 3121.4194, L1 Loss: 7062.4478\n",
            "Batch 3910, Loss: 10397.7520, L2 Loss: 3153.2312, L1 Loss: 7244.5205\n",
            "Batch 3920, Loss: 10245.5410, L2 Loss: 3130.4746, L1 Loss: 7115.0669\n",
            "Batch 3930, Loss: 10957.6016, L2 Loss: 3116.8657, L1 Loss: 7840.7354\n",
            "Batch 3940, Loss: 10990.6934, L2 Loss: 3097.6880, L1 Loss: 7893.0054\n",
            "Batch 3950, Loss: 10744.9355, L2 Loss: 3117.5288, L1 Loss: 7627.4062\n",
            "Batch 3960, Loss: 10257.6357, L2 Loss: 3103.1567, L1 Loss: 7154.4790\n",
            "Batch 3970, Loss: 10283.3711, L2 Loss: 3099.2715, L1 Loss: 7184.0991\n",
            "Batch 3980, Loss: 9986.9395, L2 Loss: 3084.9702, L1 Loss: 6901.9697\n",
            "Batch 3990, Loss: 10354.8223, L2 Loss: 3094.0398, L1 Loss: 7260.7827\n",
            "Batch 4000, Loss: 10326.0684, L2 Loss: 3089.5029, L1 Loss: 7236.5659\n",
            "Batch 4010, Loss: 10434.8115, L2 Loss: 3046.3992, L1 Loss: 7388.4121\n",
            "Batch 4020, Loss: 10214.2646, L2 Loss: 3062.8987, L1 Loss: 7151.3657\n",
            "Batch 4030, Loss: 10299.4404, L2 Loss: 3076.7900, L1 Loss: 7222.6504\n",
            "Batch 4040, Loss: 10135.5918, L2 Loss: 3065.8743, L1 Loss: 7069.7178\n",
            "Batch 4050, Loss: 10493.0176, L2 Loss: 3053.6631, L1 Loss: 7439.3550\n",
            "Batch 4060, Loss: 10272.4570, L2 Loss: 3053.1318, L1 Loss: 7219.3257\n",
            "Batch 4070, Loss: 9967.9043, L2 Loss: 3020.2202, L1 Loss: 6947.6841\n",
            "Batch 4080, Loss: 10249.9131, L2 Loss: 3043.4006, L1 Loss: 7206.5127\n",
            "Batch 4090, Loss: 10342.4170, L2 Loss: 3014.7148, L1 Loss: 7327.7021\n",
            "Batch 4100, Loss: 10280.7539, L2 Loss: 3017.7131, L1 Loss: 7263.0405\n",
            "Batch 4110, Loss: 10117.1582, L2 Loss: 3022.9526, L1 Loss: 7094.2061\n",
            "Batch 4120, Loss: 10219.0889, L2 Loss: 2998.4106, L1 Loss: 7220.6782\n",
            "Batch 4130, Loss: 9695.2930, L2 Loss: 2981.7529, L1 Loss: 6713.5405\n",
            "Batch 4140, Loss: 10358.5000, L2 Loss: 3015.7695, L1 Loss: 7342.7310\n",
            "Batch 4150, Loss: 9923.1035, L2 Loss: 2967.4783, L1 Loss: 6955.6255\n",
            "Batch 4160, Loss: 10082.8096, L2 Loss: 2954.2903, L1 Loss: 7128.5195\n",
            "Batch 4170, Loss: 9971.0967, L2 Loss: 2987.5854, L1 Loss: 6983.5112\n",
            "Batch 4180, Loss: 9993.0830, L2 Loss: 2979.5891, L1 Loss: 7013.4941\n",
            "Batch 4190, Loss: 9971.7021, L2 Loss: 2971.0330, L1 Loss: 7000.6694\n",
            "Batch 4200, Loss: 9993.0078, L2 Loss: 2956.7815, L1 Loss: 7036.2266\n",
            "Batch 4210, Loss: 10490.1729, L2 Loss: 2960.6907, L1 Loss: 7529.4824\n",
            "Batch 4220, Loss: 9918.2070, L2 Loss: 2948.2559, L1 Loss: 6969.9507\n",
            "Batch 4230, Loss: 9965.9609, L2 Loss: 2949.5957, L1 Loss: 7016.3657\n",
            "Batch 4240, Loss: 9978.3066, L2 Loss: 2957.3853, L1 Loss: 7020.9219\n",
            "Batch 4250, Loss: 9972.6436, L2 Loss: 2906.0325, L1 Loss: 7066.6108\n",
            "Batch 4260, Loss: 10027.4238, L2 Loss: 2906.5562, L1 Loss: 7120.8677\n",
            "Batch 4270, Loss: 9838.2764, L2 Loss: 2928.7756, L1 Loss: 6909.5005\n",
            "Batch 4280, Loss: 9943.0293, L2 Loss: 2904.6147, L1 Loss: 7038.4146\n",
            "Batch 4290, Loss: 9881.5732, L2 Loss: 2888.5801, L1 Loss: 6992.9932\n",
            "Batch 4300, Loss: 10077.4844, L2 Loss: 2899.5264, L1 Loss: 7177.9575\n",
            "Batch 4310, Loss: 9886.3770, L2 Loss: 2891.9714, L1 Loss: 6994.4058\n",
            "Batch 4320, Loss: 9715.7559, L2 Loss: 2909.7202, L1 Loss: 6806.0352\n",
            "Batch 4330, Loss: 9722.9102, L2 Loss: 2887.7791, L1 Loss: 6835.1309\n",
            "Batch 4340, Loss: 10049.0986, L2 Loss: 2898.4526, L1 Loss: 7150.6460\n",
            "Batch 4350, Loss: 9972.3896, L2 Loss: 2886.1116, L1 Loss: 7086.2783\n",
            "Batch 4360, Loss: 9751.7734, L2 Loss: 2881.4604, L1 Loss: 6870.3135\n",
            "Batch 4370, Loss: 9815.5391, L2 Loss: 2882.7710, L1 Loss: 6932.7681\n",
            "Batch 4380, Loss: 9968.0205, L2 Loss: 2863.0510, L1 Loss: 7104.9697\n",
            "Batch 4390, Loss: 9767.2920, L2 Loss: 2864.5471, L1 Loss: 6902.7446\n",
            "Batch 4400, Loss: 9466.8799, L2 Loss: 2836.9880, L1 Loss: 6629.8921\n",
            "Batch 4410, Loss: 9808.4756, L2 Loss: 2844.3552, L1 Loss: 6964.1201\n",
            "Batch 4420, Loss: 9781.9277, L2 Loss: 2845.6660, L1 Loss: 6936.2617\n",
            "Batch 4430, Loss: 9674.2227, L2 Loss: 2849.4231, L1 Loss: 6824.7998\n",
            "Batch 4440, Loss: 9823.5264, L2 Loss: 2830.7803, L1 Loss: 6992.7461\n",
            "Batch 4450, Loss: 9870.6895, L2 Loss: 2830.8699, L1 Loss: 7039.8193\n",
            "Batch 4460, Loss: 9823.6748, L2 Loss: 2842.1199, L1 Loss: 6981.5547\n",
            "Batch 4470, Loss: 9870.9258, L2 Loss: 2813.4785, L1 Loss: 7057.4478\n",
            "Batch 4480, Loss: 9707.8135, L2 Loss: 2824.6125, L1 Loss: 6883.2007\n",
            "Batch 4490, Loss: 9584.4307, L2 Loss: 2797.6763, L1 Loss: 6786.7544\n",
            "Batch 4500, Loss: 9613.9326, L2 Loss: 2779.9888, L1 Loss: 6833.9438\n",
            "Batch 4510, Loss: 9774.1426, L2 Loss: 2782.3862, L1 Loss: 6991.7563\n",
            "Batch 4520, Loss: 9510.9336, L2 Loss: 2786.7263, L1 Loss: 6724.2075\n",
            "Batch 4530, Loss: 9497.9180, L2 Loss: 2779.3833, L1 Loss: 6718.5347\n",
            "Batch 4540, Loss: 9901.5117, L2 Loss: 2788.8267, L1 Loss: 7112.6855\n",
            "Batch 4550, Loss: 9509.7285, L2 Loss: 2782.5752, L1 Loss: 6727.1533\n",
            "Batch 4560, Loss: 9666.6904, L2 Loss: 2760.3406, L1 Loss: 6906.3496\n",
            "Batch 4570, Loss: 9645.3252, L2 Loss: 2772.9929, L1 Loss: 6872.3320\n",
            "Batch 4580, Loss: 9439.5293, L2 Loss: 2761.8191, L1 Loss: 6677.7104\n",
            "Batch 4590, Loss: 10064.3809, L2 Loss: 2756.8950, L1 Loss: 7307.4854\n",
            "Batch 4600, Loss: 9652.3799, L2 Loss: 2740.3665, L1 Loss: 6912.0137\n",
            "Batch 4610, Loss: 9783.2539, L2 Loss: 2768.9797, L1 Loss: 7014.2739\n",
            "Batch 4620, Loss: 9417.1367, L2 Loss: 2735.1392, L1 Loss: 6681.9976\n",
            "Batch 4630, Loss: 9619.9150, L2 Loss: 2741.8479, L1 Loss: 6878.0674\n",
            "Batch 4640, Loss: 9698.2510, L2 Loss: 2724.4558, L1 Loss: 6973.7949\n",
            "Batch 4650, Loss: 9437.2871, L2 Loss: 2707.2837, L1 Loss: 6730.0029\n",
            "Batch 4660, Loss: 9469.6602, L2 Loss: 2712.8291, L1 Loss: 6756.8306\n",
            "Batch 4670, Loss: 9248.3047, L2 Loss: 2704.2776, L1 Loss: 6544.0273\n",
            "Batch 4680, Loss: 9614.7461, L2 Loss: 2719.1333, L1 Loss: 6895.6133\n",
            "Batch 4690, Loss: 9168.0869, L2 Loss: 2707.0659, L1 Loss: 6461.0210\n",
            "Batch 4700, Loss: 9463.7119, L2 Loss: 2701.9937, L1 Loss: 6761.7183\n",
            "Batch 4710, Loss: 9306.6748, L2 Loss: 2678.1892, L1 Loss: 6628.4858\n",
            "Batch 4720, Loss: 9177.6230, L2 Loss: 2675.3613, L1 Loss: 6502.2617\n",
            "Batch 4730, Loss: 9623.7002, L2 Loss: 2690.7646, L1 Loss: 6932.9355\n",
            "Batch 4740, Loss: 9413.2188, L2 Loss: 2695.0276, L1 Loss: 6718.1909\n",
            "Batch 4750, Loss: 9340.2148, L2 Loss: 2693.8970, L1 Loss: 6646.3179\n",
            "Batch 4760, Loss: 9052.3262, L2 Loss: 2661.5776, L1 Loss: 6390.7480\n",
            "Batch 4770, Loss: 9177.4268, L2 Loss: 2661.4548, L1 Loss: 6515.9722\n",
            "Batch 4780, Loss: 9397.6494, L2 Loss: 2648.0825, L1 Loss: 6749.5669\n",
            "Batch 4790, Loss: 9164.9287, L2 Loss: 2633.7017, L1 Loss: 6531.2271\n",
            "Batch 4800, Loss: 9442.3643, L2 Loss: 2654.3967, L1 Loss: 6787.9673\n",
            "Batch 4810, Loss: 9336.1055, L2 Loss: 2655.0623, L1 Loss: 6681.0435\n",
            "Batch 4820, Loss: 9397.9199, L2 Loss: 2647.3989, L1 Loss: 6750.5210\n",
            "Batch 4830, Loss: 9376.0195, L2 Loss: 2650.3796, L1 Loss: 6725.6396\n",
            "Batch 4840, Loss: 9233.2715, L2 Loss: 2618.8481, L1 Loss: 6614.4233\n",
            "Batch 4850, Loss: 9107.8857, L2 Loss: 2632.5476, L1 Loss: 6475.3379\n",
            "Batch 4860, Loss: 9304.0586, L2 Loss: 2618.2725, L1 Loss: 6685.7861\n",
            "Batch 4870, Loss: 9012.6387, L2 Loss: 2616.1836, L1 Loss: 6396.4546\n",
            "Batch 4880, Loss: 9028.4229, L2 Loss: 2627.5564, L1 Loss: 6400.8667\n",
            "Batch 4890, Loss: 9130.5371, L2 Loss: 2625.1931, L1 Loss: 6505.3438\n",
            "Batch 4900, Loss: 9334.5703, L2 Loss: 2604.8201, L1 Loss: 6729.7505\n",
            "Batch 4910, Loss: 9185.6660, L2 Loss: 2587.3176, L1 Loss: 6598.3481\n",
            "Batch 4920, Loss: 9264.0508, L2 Loss: 2591.6257, L1 Loss: 6672.4253\n",
            "Batch 4930, Loss: 9431.1387, L2 Loss: 2582.6985, L1 Loss: 6848.4404\n",
            "Batch 4940, Loss: 9235.5146, L2 Loss: 2593.5259, L1 Loss: 6641.9888\n",
            "Batch 4950, Loss: 9072.8965, L2 Loss: 2587.8945, L1 Loss: 6485.0015\n",
            "Batch 4960, Loss: 9110.5918, L2 Loss: 2584.3477, L1 Loss: 6526.2446\n",
            "Batch 4970, Loss: 9195.5371, L2 Loss: 2573.0852, L1 Loss: 6622.4521\n",
            "Batch 4980, Loss: 8951.3115, L2 Loss: 2564.3577, L1 Loss: 6386.9536\n",
            "Batch 4990, Loss: 8865.4707, L2 Loss: 2562.4512, L1 Loss: 6303.0195\n",
            "SAE training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Autoencoder"
      ],
      "metadata": {
        "id": "4G3YdRYB4RZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = get_recons_loss(num_batches=5, local_encoder=encoder) # - [ ] TODO: Increase number of batches?"
      ],
      "metadata": {
        "id": "2BewccqL4Uj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9807e76b-3f2d-4a45-bc21-1dbbe25087cb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.1139, recons_loss: 2.5313, zero_abl_loss: 14.5073\n",
            "Reconstruction Score: 89.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Frequencies"
      ],
      "metadata": {
        "id": "9XdD3DmT4Vgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = get_freqs(num_batches=25, local_encoder=encoder)\n",
        "\n",
        "# Add 1e-6.5 so that dead features show up as log_freq -6.5\n",
        "log_freq = (freqs + 10 ** -6.5).log10().cpu().numpy()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(log_freq, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Log Frequency of Features\")\n",
        "plt.xlabel(\"Log Frequency\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmvsQC9y4a2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613,
          "referenced_widgets": [
            "216d7b86d6fb4dfca7c345c93d2d2d7d",
            "c4696eaf32af4f25b27d7ccde636161e",
            "965a22fd377549d3a6f5f94d0b777aa8",
            "be76330f9d0243ba9f62c365cd387dee",
            "e9ce623a28994334a9ce1a777def9d7b",
            "a0cbb088a44e4c67ba6329e50219187f",
            "fa074d3bfe844577af3fbfe451b411b9",
            "e372ed1741b141d88fb08e82d421ef27",
            "dcb3ad2e1ab2482c9429b8b7a2acfe53",
            "a6bf4164b0154190978436661b261b99",
            "d7f5f3f202154da2b7b104a6d40a6605"
          ]
        },
        "outputId": "4272f0c5-c7a5-4fcf-b262-5a8da2eb5fd9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "216d7b86d6fb4dfca7c345c93d2d2d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num dead 0.7265625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting A Feature"
      ],
      "metadata": {
        "id": "mdR_2rtj4cWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate the features"
      ],
      "metadata": {
        "id": "LfbqK50lBl-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - [ ] TODO: Enhance and extend this logic to be more modular\n",
        "import pandas as pd\n",
        "\n",
        "feature_id = 18 # Change as needed\n",
        "batch_size = 128  # Adjust as needed\n",
        "\n",
        "print(f\"Feature freq: {freqs[feature_id].item():.4f}\")\n",
        "\n",
        "# Get activations\n",
        "idx = torch.randperm(len(all_tokens))[:batch_size]\n",
        "tokens = all_tokens[idx].to(model_device)\n",
        "contexts = all_contexts[idx].to(model_device)\n",
        "targets = all_targets[idx].to(model_device)\n",
        "\n",
        "mlp_activations = []\n",
        "\n",
        "def capture_mlp_activations(module, input, output):\n",
        "    mlp_activations.append(output.detach())\n",
        "\n",
        "# Register the hook\n",
        "mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "# Forward pass\n",
        "logits, _ = model(tokens, contexts)\n",
        "# Remove the hook\n",
        "hook_handle.remove()\n",
        "\n",
        "# Get the activations\n",
        "mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "# Pass through encoder\n",
        "loss_enc, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
        "print(\"hidden_acts.shape\", hidden_acts.shape)\n",
        "\n",
        "# Create token dataframe\n",
        "token_df = make_token_df(tokens.cpu().flatten())\n",
        "token_df[\"feature\"] = hidden_acts[:, feature_id].detach().cpu().numpy()\n",
        "display(token_df.sort_values(\"feature\", ascending=False).head(20))"
      ],
      "metadata": {
        "id": "UYT7m6vp4cPn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "2a50e892-cf61-4da1-bc6b-febe97343dfb"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature freq: 0.0323\n",
            "hidden_acts.shape torch.Size([128000, 256])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       str_tokens unique_token               context  batch     pos     label  \\\n",
              "35018         431    431/35018   264511044727|431|34      0   35018   0/35018   \n",
              "82660         431    431/82660   353693340335|431|35      0   82660   0/82660   \n",
              "125626        431   431/125626   293774042417|431|48      0  125626  0/125626   \n",
              "31950         432    432/31950  1513569043639|432|39      0   31950   0/31950   \n",
              "41796         431    431/41796   264193042332|431|39      0   41796   0/41796   \n",
              "40780         432    432/40780   214513144144|432|45      0   40780   0/40780   \n",
              "40740         432    432/40740   354081542432|432|14      0   40740   0/40740   \n",
              "71784         431    431/71784   333463232735|431|34      0   71784   0/71784   \n",
              "71800         431    431/71800  4137715137262|431|27      0   71800   0/71800   \n",
              "122978        432   432/122978   174512043634|432|42      0  122978  0/122978   \n",
              "44988         431    431/44988   314253842221|431|13      0   44988   0/44988   \n",
              "59890         431    431/59890   383901030230|431|41      0   59890   0/59890   \n",
              "119882        431   431/119882   214133141721|431|27      0  119882  0/119882   \n",
              "35842         431    431/35842   364093241631|431|34      0   35842   0/35842   \n",
              "35820         431    431/35820   343533132022|431|34      0   35820   0/35820   \n",
              "126710        432   432/126710   213261245131|432|35      0  126710  0/126710   \n",
              "52784         431    431/52784   364083241631|431|34      0   52784   0/52784   \n",
              "81780         432    432/81780   343222630230|432|27      0   81780   0/81780   \n",
              "71740         431    431/71740   913023044942|431|34      0   71740   0/71740   \n",
              "78828         432    432/78828   333483330838|432|36      0   78828   0/78828   \n",
              "\n",
              "          feature  \n",
              "35018   21.098875  \n",
              "82660   19.616819  \n",
              "125626  19.597923  \n",
              "31950   19.525885  \n",
              "41796   19.261675  \n",
              "40780   19.033039  \n",
              "40740   19.026245  \n",
              "71784   18.958117  \n",
              "71800   18.864071  \n",
              "122978  18.855888  \n",
              "44988   18.845633  \n",
              "59890   18.834324  \n",
              "119882  18.789522  \n",
              "35842   18.765261  \n",
              "35820   18.731522  \n",
              "126710  18.678844  \n",
              "52784   18.582232  \n",
              "81780   18.546263  \n",
              "71740   18.482273  \n",
              "78828   18.450830  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5460cf01-44a6-4daf-9da7-fb40341946c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>str_tokens</th>\n",
              "      <th>unique_token</th>\n",
              "      <th>context</th>\n",
              "      <th>batch</th>\n",
              "      <th>pos</th>\n",
              "      <th>label</th>\n",
              "      <th>feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35018</th>\n",
              "      <td>431</td>\n",
              "      <td>431/35018</td>\n",
              "      <td>264511044727|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>35018</td>\n",
              "      <td>0/35018</td>\n",
              "      <td>21.098875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82660</th>\n",
              "      <td>431</td>\n",
              "      <td>431/82660</td>\n",
              "      <td>353693340335|431|35</td>\n",
              "      <td>0</td>\n",
              "      <td>82660</td>\n",
              "      <td>0/82660</td>\n",
              "      <td>19.616819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125626</th>\n",
              "      <td>431</td>\n",
              "      <td>431/125626</td>\n",
              "      <td>293774042417|431|48</td>\n",
              "      <td>0</td>\n",
              "      <td>125626</td>\n",
              "      <td>0/125626</td>\n",
              "      <td>19.597923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31950</th>\n",
              "      <td>432</td>\n",
              "      <td>432/31950</td>\n",
              "      <td>1513569043639|432|39</td>\n",
              "      <td>0</td>\n",
              "      <td>31950</td>\n",
              "      <td>0/31950</td>\n",
              "      <td>19.525885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41796</th>\n",
              "      <td>431</td>\n",
              "      <td>431/41796</td>\n",
              "      <td>264193042332|431|39</td>\n",
              "      <td>0</td>\n",
              "      <td>41796</td>\n",
              "      <td>0/41796</td>\n",
              "      <td>19.261675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40780</th>\n",
              "      <td>432</td>\n",
              "      <td>432/40780</td>\n",
              "      <td>214513144144|432|45</td>\n",
              "      <td>0</td>\n",
              "      <td>40780</td>\n",
              "      <td>0/40780</td>\n",
              "      <td>19.033039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40740</th>\n",
              "      <td>432</td>\n",
              "      <td>432/40740</td>\n",
              "      <td>354081542432|432|14</td>\n",
              "      <td>0</td>\n",
              "      <td>40740</td>\n",
              "      <td>0/40740</td>\n",
              "      <td>19.026245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71784</th>\n",
              "      <td>431</td>\n",
              "      <td>431/71784</td>\n",
              "      <td>333463232735|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>71784</td>\n",
              "      <td>0/71784</td>\n",
              "      <td>18.958117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71800</th>\n",
              "      <td>431</td>\n",
              "      <td>431/71800</td>\n",
              "      <td>4137715137262|431|27</td>\n",
              "      <td>0</td>\n",
              "      <td>71800</td>\n",
              "      <td>0/71800</td>\n",
              "      <td>18.864071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122978</th>\n",
              "      <td>432</td>\n",
              "      <td>432/122978</td>\n",
              "      <td>174512043634|432|42</td>\n",
              "      <td>0</td>\n",
              "      <td>122978</td>\n",
              "      <td>0/122978</td>\n",
              "      <td>18.855888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44988</th>\n",
              "      <td>431</td>\n",
              "      <td>431/44988</td>\n",
              "      <td>314253842221|431|13</td>\n",
              "      <td>0</td>\n",
              "      <td>44988</td>\n",
              "      <td>0/44988</td>\n",
              "      <td>18.845633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59890</th>\n",
              "      <td>431</td>\n",
              "      <td>431/59890</td>\n",
              "      <td>383901030230|431|41</td>\n",
              "      <td>0</td>\n",
              "      <td>59890</td>\n",
              "      <td>0/59890</td>\n",
              "      <td>18.834324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119882</th>\n",
              "      <td>431</td>\n",
              "      <td>431/119882</td>\n",
              "      <td>214133141721|431|27</td>\n",
              "      <td>0</td>\n",
              "      <td>119882</td>\n",
              "      <td>0/119882</td>\n",
              "      <td>18.789522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35842</th>\n",
              "      <td>431</td>\n",
              "      <td>431/35842</td>\n",
              "      <td>364093241631|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>35842</td>\n",
              "      <td>0/35842</td>\n",
              "      <td>18.765261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35820</th>\n",
              "      <td>431</td>\n",
              "      <td>431/35820</td>\n",
              "      <td>343533132022|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>35820</td>\n",
              "      <td>0/35820</td>\n",
              "      <td>18.731522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126710</th>\n",
              "      <td>432</td>\n",
              "      <td>432/126710</td>\n",
              "      <td>213261245131|432|35</td>\n",
              "      <td>0</td>\n",
              "      <td>126710</td>\n",
              "      <td>0/126710</td>\n",
              "      <td>18.678844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52784</th>\n",
              "      <td>431</td>\n",
              "      <td>431/52784</td>\n",
              "      <td>364083241631|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>52784</td>\n",
              "      <td>0/52784</td>\n",
              "      <td>18.582232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81780</th>\n",
              "      <td>432</td>\n",
              "      <td>432/81780</td>\n",
              "      <td>343222630230|432|27</td>\n",
              "      <td>0</td>\n",
              "      <td>81780</td>\n",
              "      <td>0/81780</td>\n",
              "      <td>18.546263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71740</th>\n",
              "      <td>431</td>\n",
              "      <td>431/71740</td>\n",
              "      <td>913023044942|431|34</td>\n",
              "      <td>0</td>\n",
              "      <td>71740</td>\n",
              "      <td>0/71740</td>\n",
              "      <td>18.482273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78828</th>\n",
              "      <td>432</td>\n",
              "      <td>432/78828</td>\n",
              "      <td>333483330838|432|36</td>\n",
              "      <td>0</td>\n",
              "      <td>78828</td>\n",
              "      <td>0/78828</td>\n",
              "      <td>18.450830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5460cf01-44a6-4daf-9da7-fb40341946c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5460cf01-44a6-4daf-9da7-fb40341946c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5460cf01-44a6-4daf-9da7-fb40341946c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0e0aeae7-9f7f-497a-90ea-a192cd2d586c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e0aeae7-9f7f-497a-90ea-a192cd2d586c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0e0aeae7-9f7f-497a-90ea-a192cd2d586c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(token_df\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"str_tokens\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"432\",\n          \"431\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unique_token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"431/35018\",\n          \"432/81780\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"264511044727|431|34\",\n          \"343222630230|432|27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32841,\n        \"min\": 31950,\n        \"max\": 126710,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          35018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"0/35018\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          21.098875045776367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interesting_feature_ids = [2, 5, 18]"
      ],
      "metadata": {
        "id": "ROqOC8q6tXy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIMduPHtn4SW",
        "outputId": "4fb2fd2d-5e6d-4b89-99cb-d3c79f163274"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: DEBUG: Visualizing Feature Activations"
      ],
      "metadata": {
        "id": "h-yvVk0v4cFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the character-to-index and index-to-character mappings from the dataset\n",
        "char2idx = train_dataset.stoi  # Character to index mapping\n",
        "idx2char = train_dataset.itos  # Index to character mapping\n",
        "\n",
        "# Define the tokenization function using the dataset's mapping\n",
        "def tokenize_text(text):\n",
        "    # Map each character to its index, default to PAD token (0) for unknown characters\n",
        "    return [char2idx.get(ch, train_dataset.char_PAD_TOKEN) for ch in text]\n",
        "\n",
        "def make_feature_vis_gradio(feature_id, starting_text=None):\n",
        "    if starting_text is None:\n",
        "        starting_text = \"Sample text for visualization\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.HTML(value=f\"Hacky Interactive Neuroscope for cursivetransformer\")\n",
        "        # The input elements\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text = gr.Textbox(label=\"Context Text\", value=starting_text)\n",
        "                feature_index = gr.Number(\n",
        "                    label=\"Feature Index\", value=feature_id, precision=0\n",
        "                )\n",
        "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
        "                inputs = [text, feature_index, max_val]\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # The output element\n",
        "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(feature_id))\n",
        "        def update_output(text, feature_index, max_val):\n",
        "            return basic_feature_vis(text, int(feature_index), max_val)\n",
        "        for inp in inputs:\n",
        "            inp.change(update_output, inputs, out)\n",
        "    demo.launch(share=True)\n",
        "\n",
        "# Visualize the feature activations\n",
        "starting_text = \"Sample text to test feature activations in cursivetransformer\"\n",
        "make_feature_vis_gradio(feature_id, starting_text)"
      ],
      "metadata": {
        "id": "KvYERycr4bz5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1f137b9e-10c0-4e25-ad26-3f0539b43abf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://a6d0fe8c441d2b13e4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6d0fe8c441d2b13e4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}