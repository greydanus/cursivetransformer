{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPk1pchpvdK1ERdhc67vuOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a12580d90ea24f5aa70b9b215e1ec8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54b3a1086df24fc1bd20b7bcbab50533",
              "IPY_MODEL_dfd0196649e0457688173afeb67fd0fa",
              "IPY_MODEL_265eebfd899f41ebb0126357084068ad"
            ],
            "layout": "IPY_MODEL_312c30811da64bb283bb47d9adfa7b69"
          }
        },
        "54b3a1086df24fc1bd20b7bcbab50533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce575fc4b7c94a3db725a9cc6efc0af2",
            "placeholder": "​",
            "style": "IPY_MODEL_56e43344d0944d1fa062c67418321ee2",
            "value": " 29%"
          }
        },
        "dfd0196649e0457688173afeb67fd0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8357835095514cb482c97bdf72d3db1a",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c28365ec2f2441a9ada611da9fb5ea1d",
            "value": 1450
          }
        },
        "265eebfd899f41ebb0126357084068ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ea53d4a9a742ba8a47bd2af4619a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_fafbcd4ee1884990a7209bf5b94a5639",
            "value": " 1450/5000 [05:52&lt;14:15,  4.15it/s]"
          }
        },
        "312c30811da64bb283bb47d9adfa7b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce575fc4b7c94a3db725a9cc6efc0af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e43344d0944d1fa062c67418321ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8357835095514cb482c97bdf72d3db1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28365ec2f2441a9ada611da9fb5ea1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68ea53d4a9a742ba8a47bd2af4619a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafbcd4ee1884990a7209bf5b94a5639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer_dictionary_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cursive Transformer Mechanistic Interpretability\n",
        "---\n",
        "## TODOs\n",
        "- [ ] Keep things on track\n",
        "  >Notice that we have somewhat lost sight of what we are trying to do via this exercise, which is to construct a dictionary of interpretible features that our cursivetransformer model has learned. Since our model is based on the original GPT-2 architecture, including the cross-attention, we will need adjust our approach to ensure our intepretation of the SAE learned features are indeed interpretible, and if they are then that they are being intepreted correctly! Namely, we need to keep in mind that we have 2 types of tokens that our transformer processes:\n",
        "  1. Ascii character-level tokens\n",
        "  2. Stroke tokens\n",
        "  >\n",
        "  >The model encoder handles the ascii tokens, and the decoder handles the stroke tokens, and these are coupled via the cross-attention heads in the transformer layers of the model. Therefore, we are primarily interested in the features learned in the stroke-attention heads, as well as the features learned by the cross-attention heads. We need to adjust the code throughout this notebook to reflect that, and adjust our development moving forward accordingly.\n",
        "\n",
        "---\n",
        "## Introduction\n",
        "The purpose of this notebook is to apply the techniques of mechanistic interpretability, in particular dictionary learning, to our cursive transformer. The goal of this exercise is two-fold:\n",
        "1. Attempt to gain insight into the structure and orientation of the features being learned by the model, in particular in the cross-attention and feed-forward layers\n",
        "2. More generally become familiar with the latest ME techniques, as they can easily transfer to be applied beyond the scope of this project to essentially ANY transformer-based model\n",
        "\n",
        "To this end, we will reference the following resources throughout this document:\n",
        "- https://transformer-circuits.pub/2023/monosemantic-features\n",
        "- https://www.alignmentforum.org/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s\n",
        "- https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing\n",
        "- https://github.com/TransformerLensOrg/TransformerLens\n",
        "- https://arena3-chapter1-transformer-interp.streamlit.app/"
      ],
      "metadata": {
        "id": "GdwBlsu_am35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JuHwKIaP3Oyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -V"
      ],
      "metadata": {
        "id": "12UmRQ9N3Rav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a136a9-a327-41fd-f9a5-30527595e494"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "5BpggN2t3Qr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "VY-xjph_3QkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e52f1fa0-f76c-4e92-c00b-007b9e7eb8af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.34.2)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.44.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading transformer_lens-2.6.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, pyarrow, fancy-einsum, docker-pycreds, dill, beartype, multiprocess, jaxtyping, gitdb, gitpython, wandb, datasets, transformer_lens\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 datasets-3.0.0 dill-0.3.8 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.11 gitpython-3.1.43 jaxtyping-0.2.34 multiprocess-0.70.16 pyarrow-17.0.0 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 transformer_lens-2.6.0 typeguard-2.13.3 wandb-0.18.1 xxhash-3.5.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.10-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.10 ruff-0.6.7 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2345, done.\u001b[K\n",
            "remote: Counting objects: 100% (480/480), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 2345 (delta 411), reused 441 (delta 381), pack-reused 1865 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2345/2345), 31.63 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1325/1325), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 1)) (2.4.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 2)) (0.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r cursivetransformer/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r cursivetransformer/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "cWhbLc-33V2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "xit1BQED3ZK_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Autoencoder"
      ],
      "metadata": {
        "id": "NkFwM1If3bbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1p4_vsXeZigM"
      },
      "outputs": [],
      "source": [
        "# Configuration for the autoencoder\n",
        "cfg = {\n",
        "    \"seed\": 49,\n",
        "    \"batch_size\": 4096,\n",
        "    \"buffer_mult\": 384,\n",
        "    \"lr\": 1e-4,\n",
        "    \"num_tokens\": int(2e9),\n",
        "    \"l1_coeff\": 3e-4,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.99,\n",
        "    \"dict_mult\": 8,\n",
        "    \"seq_len\": 256,\n",
        "    \"d_mlp\": None,  # To be set after loading the model\n",
        "    \"enc_dtype\": \"fp32\",\n",
        "    \"remove_rare_dir\": False,\n",
        "}\n",
        "\n",
        "cfg[\"model_batch_size\"] = 64\n",
        "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
        "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
        "\n",
        "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
        "        d_mlp = cfg[\"d_mlp\"]\n",
        "        l1_coeff = cfg[\"l1_coeff\"]\n",
        "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
        "        torch.manual_seed(cfg[\"seed\"])\n",
        "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
        "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
        "\n",
        "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.l1_coeff = l1_coeff\n",
        "\n",
        "        self.to(\"cuda\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_cent = x - self.b_dec\n",
        "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
        "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
        "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
        "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
        "        loss = l2_loss + l1_loss\n",
        "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def remove_parallel_component_of_grads(self):\n",
        "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
        "        self.W_dec.grad -= W_dec_grad_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n",
        "\n"
      ],
      "metadata": {
        "id": "TDksX_ZQ3lcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Reconstruction Loss"
      ],
      "metadata": {
        "id": "iDiByoQV3kvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recons_loss(num_batches=5, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    loss_list = []\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for i in range(num_batches):\n",
        "        idx = torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        context = all_contexts[idx].to(model_device)\n",
        "        targets = all_targets[idx].to(model_device)\n",
        "\n",
        "        # Capture MLP activations\n",
        "        mlp_activations = []\n",
        "\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook on the MLP output layer\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj  # Adjust layer index as needed\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass without hooks\n",
        "        logits, loss = model(tokens, context, targets=targets)\n",
        "        loss = loss.item()\n",
        "\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "\n",
        "        # Flatten activations\n",
        "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Reconstruct activations using encoder\n",
        "        loss_enc, x_reconstruct, hidden_acts, l2_loss, l1_loss = local_encoder(mlp_acts_flattened)\n",
        "\n",
        "        # Reconstructed activations reshaped back to original shape\n",
        "        reconstructed_acts = x_reconstruct.view_as(mlp_acts)\n",
        "\n",
        "        # Define the hook to replace activations\n",
        "        def reconstruction_hook(module, input, output):\n",
        "            return reconstructed_acts\n",
        "\n",
        "        # Register the reconstruction hook\n",
        "        hook_handle = mlp_layer.register_forward_hook(reconstruction_hook)\n",
        "        # Forward pass with reconstructed activations\n",
        "        logits_recons, loss_recons = model(tokens, context, targets=targets)\n",
        "        recons_loss = loss_recons.item()\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Zero ablation\n",
        "        def zero_ablation_hook(module, input, output):\n",
        "            return torch.zeros_like(output)\n",
        "\n",
        "        hook_handle = mlp_layer.register_forward_hook(zero_ablation_hook)\n",
        "        logits_zero_abl, loss_zero_abl = model(tokens, context, targets=targets)\n",
        "        zero_abl_loss = loss_zero_abl.item()\n",
        "        hook_handle.remove()\n",
        "\n",
        "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
        "    losses = torch.tensor(loss_list)\n",
        "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
        "\n",
        "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
        "    score = ((zero_abl_loss - recons_loss) / (zero_abl_loss - loss))\n",
        "    print(f\"Reconstruction Score: {score:.2%}\")\n",
        "    return score, loss, recons_loss, zero_abl_loss"
      ],
      "metadata": {
        "id": "pxQjA1Xh3pgP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Frequencies"
      ],
      "metadata": {
        "id": "j1dKdlV13rTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_freqs(num_batches=25, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
        "    total = 0\n",
        "    for i in tqdm.trange(num_batches):\n",
        "        idx = torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        context = all_contexts[idx].to(model_device)\n",
        "\n",
        "        mlp_activations = []\n",
        "\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook on the MLP output layer\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj  # Adjust layer index as needed\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(tokens, context)\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Pass through encoder\n",
        "        hidden = local_encoder(mlp_acts_flattened)[2]  # Get the activations\n",
        "\n",
        "        act_freq_scores += (hidden > 0).sum(0)\n",
        "        total += hidden.shape[0]\n",
        "    act_freq_scores /= total\n",
        "    num_dead = (act_freq_scores == 0).float().mean()\n",
        "    print(\"Num dead\", num_dead.item())\n",
        "    return act_freq_scores"
      ],
      "metadata": {
        "id": "kbDgvhpS3vnU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualise Feature Utils"
      ],
      "metadata": {
        "id": "kk1mA6FK3y0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from html import escape\n",
        "import colorsys\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "SPACE = \"·\"\n",
        "NEWLINE = \"↩\"\n",
        "TAB = \"→\"\n",
        "\n",
        "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
        "    # Escape strings to deal with tabs, newlines, etc.\n",
        "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
        "    processed_strings = [\n",
        "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
        "        for s in escaped_strings\n",
        "    ]\n",
        "\n",
        "    if isinstance(values, torch.Tensor) and len(values.shape) > 1:\n",
        "        values = values.flatten().tolist()\n",
        "\n",
        "    if not allow_different_length:\n",
        "        assert len(processed_strings) == len(values)\n",
        "\n",
        "    # Scale values\n",
        "    if max_value is None:\n",
        "        max_value = max(max(values), -min(values)) + 1e-3\n",
        "    scaled_values = [v / max_value * saturation for v in values]\n",
        "\n",
        "    # Create HTML\n",
        "    html = \"\"\n",
        "    for i, s in enumerate(processed_strings):\n",
        "        if i < len(scaled_values):\n",
        "            v = scaled_values[i]\n",
        "        else:\n",
        "            v = 0\n",
        "        if v < 0:\n",
        "            hue = 0  # Red in HSV\n",
        "        else:\n",
        "            hue = 0.66  # Blue in HSV\n",
        "        rgb_color = colorsys.hsv_to_rgb(\n",
        "            hue, v, 1\n",
        "        )\n",
        "        hex_color = \"#%02x%02x%02x\" % (\n",
        "            int(rgb_color[0] * 255),\n",
        "            int(rgb_color[1] * 255),\n",
        "            int(rgb_color[2] * 255),\n",
        "        )\n",
        "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
        "    if return_string:\n",
        "        return html\n",
        "    else:\n",
        "        display(HTML(html))\n",
        "\n",
        "# - [ ] TODO: <-DEBUG THIS!!!\n",
        "def stroke_to_svg(stroke_sequence):\n",
        "    # This function generates an SVG path for each stroke in the sequence\n",
        "    svg_output = \"\"\n",
        "    for stroke in stroke_sequence:\n",
        "        svg_output += f'<path d=\"M{stroke[0][0]},{stroke[0][1]}'\n",
        "        for point in stroke[1:]:\n",
        "            svg_output += f' L{point[0]},{point[1]}'\n",
        "        svg_output += '\" style=\"stroke:black;fill:none;\" />'\n",
        "    return svg_output\n",
        "\n",
        "def basic_feature_vis(feature_index, max_val=0):\n",
        "    feature_in = encoder.W_enc[:, feature_index]\n",
        "    feature_bias = encoder.b_enc[feature_index]\n",
        "\n",
        "    # Get a sample stroke sequence and context from the dataset\n",
        "    x, c, _ = test_dataset[0]  # You can choose any index or randomize\n",
        "    tokens = x.unsqueeze(0).to(model_device)  # Stroke tokens\n",
        "    context = c.unsqueeze(0).to(model_device)  # Context tokens\n",
        "\n",
        "    # Capture MLP activations\n",
        "    mlp_activations = []\n",
        "\n",
        "    def capture_mlp_activations(module, input, output):\n",
        "        mlp_activations.append(output.detach())\n",
        "\n",
        "    # Register the hook\n",
        "    mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "    hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, _ = model(tokens, context)\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Get the activations\n",
        "    mlp_acts = mlp_activations[0][0].cpu()  # Shape: [seq_len, d_mlp]\n",
        "    feature_acts = F.relu((mlp_acts - encoder.b_dec.cpu()) @ feature_in.cpu() + feature_bias.cpu())\n",
        "    if max_val == 0:\n",
        "        max_val = max(1e-7, feature_acts.max().item())\n",
        "\n",
        "    # Decode stroke tokens back to the original stroke format\n",
        "    stroke_sequence = [offsets_to_strokes(test_dataset.decode_stroke(t)) for t in tokens]\n",
        "\n",
        "    # Convert decoded strokes to SVG format for visualization\n",
        "    stroke_svgs = [stroke_to_svg(stroke) for stroke in stroke_sequence]\n",
        "\n",
        "    return basic_token_vis_make_str(stroke_svgs, feature_acts, max_val)\n",
        "\n",
        "\n",
        "def basic_token_vis_make_str(strings, values, max_val=None):\n",
        "    if not isinstance(strings, list):\n",
        "        strings = list(strings)\n",
        "    values = values.detach().numpy()\n",
        "    if max_val is None:\n",
        "        max_val = values.max()\n",
        "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
        "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
        "    body_string = create_html(strings, values, max_value=max_val, return_string=True)\n",
        "    return header_string + body_string"
      ],
      "metadata": {
        "id": "I9jLMFym32Yw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Token DataFrame"
      ],
      "metadata": {
        "id": "NZmEcWIx34gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_token_df(tokens, len_prefix=5, len_suffix=1):\n",
        "    str_tokens = [str(t.item()) for t in tokens]\n",
        "    unique_token = [f\"{s}/{i}\" for i, s in enumerate(str_tokens)]\n",
        "\n",
        "    context = []\n",
        "    batch = []\n",
        "    pos = []\n",
        "    label = []\n",
        "    for p in range(len(tokens)):\n",
        "        prefix = \"\".join(str_tokens[max(0, p - len_prefix):p])\n",
        "        if p == len(tokens) - 1:\n",
        "            suffix = \"\"\n",
        "        else:\n",
        "            suffix = \"\".join(str_tokens[p + 1:min(len(tokens) - 1, p + 1 + len_suffix)])\n",
        "        current = str_tokens[p]\n",
        "        context.append(f\"{prefix}|{current}|{suffix}\")\n",
        "        batch.append(0)  # Since we have a single batch\n",
        "        pos.append(p)\n",
        "        label.append(f\"{0}/{p}\")\n",
        "    return pd.DataFrame(dict(\n",
        "        str_tokens=str_tokens,\n",
        "        unique_token=unique_token,\n",
        "        context=context,\n",
        "        batch=batch,\n",
        "        pos=pos,\n",
        "        label=label,\n",
        "    ))"
      ],
      "metadata": {
        "id": "j3py2Keq37VI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "rpgE-1vu37QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)\n",
        "\n",
        "\n",
        "cfg[\"d_mlp\"] = args.n_embd  # Assuming n_embd is the MLP dimension\n",
        "args"
      ],
      "metadata": {
        "id": "Fb6qP_3i72fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c4464e-e0ce-4eee-efba-c379c71012df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load dataset file from /content/cursivetransformer/data/bigbank.json.zip\n",
            "Succeeded in loading the bigbank dataset; contains 1900 items.\n",
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n",
            "For a dataset of 95 examples we can generate 3183545 combinations of 4 examples.\n",
            "Generating 3000 4-word examples.\n",
            "Number of examples in the train dataset: 497000\n",
            "Number of examples in the test dataset: 3000\n",
            "Max token sequence length: 1000\n",
            "Number of unique characters in the ascii vocabulary: 71\n",
            "Ascii vocabulary:\n",
            "\t\" enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"'(BCQLMWYU,ZF!DXV?KPGJ\"\n",
            "Split up the dataset into 497000 training examples and 3000 test examples\n",
            "Number of Transformer parameters: 368064\n",
            "Model #params: 397184\n",
            "Finding latest checkpoint for W&B run id 6le6tujz\n",
            "  model:best_checkpoint:v70\n",
            "  model:best_checkpoint:v71\n",
            "  model:best_checkpoint:v72\n",
            "  model:best_checkpoint:v73\n",
            "  model:best_checkpoint:v74\n",
            "  model:best_checkpoint:v75\n",
            "  model:best_checkpoint:v76\n",
            "  model:best_checkpoint:v77\n",
            "  model:best_checkpoint:v78\n",
            "  model:best_checkpoint:v79\n",
            "  model:best_checkpoint:v80\n",
            "  model:best_checkpoint:v81\n",
            "  model:best_checkpoint:v82\n",
            "  model:best_checkpoint:v83\n",
            "  model:best_checkpoint:v84\n",
            "  model:best_checkpoint:v85\n",
            "  model:best_checkpoint:v86\n",
            "  model:best_checkpoint:v87\n",
            "  model:best_checkpoint:v88\n",
            "  model:best_checkpoint:v89\n",
            "  wandb-history:run-6le6tujz-history:v4\n",
            "Selected:  model:best_checkpoint:v89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(max_steps=110000,\n",
              "          print_every=100,\n",
              "          log_every=2500,\n",
              "          lr_decay=0.333,\n",
              "          step_lr_every=33000,\n",
              "          device='cuda',\n",
              "          seed=42,\n",
              "          n_layer=4,\n",
              "          n_embd=64,\n",
              "          n_embd2=64,\n",
              "          n_ctx_head=4,\n",
              "          learning_rate=0.01,\n",
              "          weight_decay=0.0001,\n",
              "          batch_size=32,\n",
              "          train_size=497000,\n",
              "          test_size=3000,\n",
              "          num_words=4,\n",
              "          max_seq_length=1000,\n",
              "          augment=True,\n",
              "          ablate_cross_attention=False,\n",
              "          downsample_mean=0.65,\n",
              "          downsample_width=0.1,\n",
              "          add_digits=True,\n",
              "          alphabet=' enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"\\'(BCQLMWYU,ZF!DXV?KPGJ',\n",
              "          dataset_name='bigbank',\n",
              "          wandb_project='bigbank_experiments',\n",
              "          wandb_entity='sam-greydanus',\n",
              "          wandb_run_name='cursivetransformer_dictionary_learning',\n",
              "          wandb_api_key=None,\n",
              "          load_from_run_id='6le6tujz',\n",
              "          sample_only=True,\n",
              "          local_checkpoint_path='best_checkpoint.pt',\n",
              "          block_size=1000,\n",
              "          context_block_size=50,\n",
              "          vocab_size=455,\n",
              "          context_vocab_size=72)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_device = 'cuda'\n",
        "model = model.to(model_device)\n",
        "model = model.eval()  # Set model to evaluation mode"
      ],
      "metadata": {
        "id": "f650Kjl_O-P4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "hTPC8yxS4DNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = cfg[\"model_batch_size\"] # - [ ] TODO: Determine if this is needed?\n",
        "num_samples = len(test_dataset)\n",
        "stroke_seqs = []\n",
        "contexts = []\n",
        "targets = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    x, c, y = test_dataset[i]\n",
        "    stroke_seqs.append(x)\n",
        "    contexts.append(c)\n",
        "    targets.append(y)\n",
        "\n",
        "all_tokens = torch.stack(stroke_seqs)\n",
        "all_contexts = torch.stack(contexts)\n",
        "all_targets = torch.stack(targets)"
      ],
      "metadata": {
        "id": "nAEcf7h_4DFN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "q7_RwgFV4Mln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Autoencoder"
      ],
      "metadata": {
        "id": "JwmkLrrc4Cv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = AutoEncoder(cfg)"
      ],
      "metadata": {
        "id": "Yiea6x6o4Q69"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Autoencoder"
      ],
      "metadata": {
        "id": "u4f-J5kyOa9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sae(model, encoder, num_batches=5000, batch_size=256):\n",
        "    optimizer = torch.optim.Adam(encoder.parameters(), lr=cfg['lr'], betas=(cfg['beta1'], cfg['beta2']))\n",
        "\n",
        "    for batch in tqdm.tqdm(range(num_batches)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sample data\n",
        "        idx = torch.randperm(len(all_tokens))[:batch_size]\n",
        "        tokens = all_tokens[idx].to(model_device)\n",
        "        contexts = all_contexts[idx].to(model_device)\n",
        "\n",
        "        # Capture MLP activations\n",
        "        mlp_activations = []\n",
        "        def capture_mlp_activations(module, input, output):\n",
        "            mlp_activations.append(output.detach())\n",
        "\n",
        "        # Register the hook\n",
        "        mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "        hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(tokens, contexts)\n",
        "\n",
        "        # Remove the hook\n",
        "        hook_handle.remove()\n",
        "\n",
        "        # Get the activations\n",
        "        mlp_acts = mlp_activations[0].reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "        # Pass through encoder\n",
        "        loss, _, _, l2_loss, l1_loss = encoder(mlp_acts)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f\"Batch {batch}, Loss: {loss.item():.4f}, L2 Loss: {l2_loss.item():.4f}, L1 Loss: {l1_loss.item():.4f}\")\n",
        "\n",
        "    print(\"SAE training completed.\")\n",
        "\n",
        "# Train the SAE\n",
        "train_sae(model, encoder)\n",
        "# Save the encoder weights\n",
        "# torch.save(encoder.state_dict(), 'encoder_weights.pth')"
      ],
      "metadata": {
        "id": "FAgTAimjOajl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a12580d90ea24f5aa70b9b215e1ec8ef",
            "54b3a1086df24fc1bd20b7bcbab50533",
            "dfd0196649e0457688173afeb67fd0fa",
            "265eebfd899f41ebb0126357084068ad",
            "312c30811da64bb283bb47d9adfa7b69",
            "ce575fc4b7c94a3db725a9cc6efc0af2",
            "56e43344d0944d1fa062c67418321ee2",
            "8357835095514cb482c97bdf72d3db1a",
            "c28365ec2f2441a9ada611da9fb5ea1d",
            "68ea53d4a9a742ba8a47bd2af4619a7d",
            "fafbcd4ee1884990a7209bf5b94a5639"
          ]
        },
        "outputId": "8b9f7c4d-2f01-4f5d-8a8c-2eef795492f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a12580d90ea24f5aa70b9b215e1ec8ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 237184.4531, L2 Loss: 72662.0000, L1 Loss: 164522.4531\n",
            "Batch 10, Loss: 206622.2031, L2 Loss: 53478.2500, L1 Loss: 153143.9531\n",
            "Batch 20, Loss: 192977.5781, L2 Loss: 43868.7305, L1 Loss: 149108.8438\n",
            "Batch 30, Loss: 172111.0469, L2 Loss: 34260.3750, L1 Loss: 137850.6719\n",
            "Batch 40, Loss: 160950.3281, L2 Loss: 29494.9727, L1 Loss: 131455.3594\n",
            "Batch 50, Loss: 145776.0625, L2 Loss: 25145.6172, L1 Loss: 120630.4453\n",
            "Batch 60, Loss: 135127.0625, L2 Loss: 22689.1172, L1 Loss: 112437.9375\n",
            "Batch 70, Loss: 128391.8750, L2 Loss: 21476.3945, L1 Loss: 106915.4766\n",
            "Batch 80, Loss: 121598.0859, L2 Loss: 20347.5762, L1 Loss: 101250.5078\n",
            "Batch 90, Loss: 113002.6875, L2 Loss: 19177.5977, L1 Loss: 93825.0859\n",
            "Batch 100, Loss: 106070.5469, L2 Loss: 18406.2793, L1 Loss: 87664.2656\n",
            "Batch 110, Loss: 99083.0469, L2 Loss: 17622.1133, L1 Loss: 81460.9375\n",
            "Batch 120, Loss: 94877.6484, L2 Loss: 17570.6973, L1 Loss: 77306.9531\n",
            "Batch 130, Loss: 88803.3125, L2 Loss: 16918.2812, L1 Loss: 71885.0312\n",
            "Batch 140, Loss: 82388.6094, L2 Loss: 16105.2930, L1 Loss: 66283.3125\n",
            "Batch 150, Loss: 79353.9141, L2 Loss: 16123.1465, L1 Loss: 63230.7695\n",
            "Batch 160, Loss: 75770.8359, L2 Loss: 15816.8711, L1 Loss: 59953.9648\n",
            "Batch 170, Loss: 72029.2109, L2 Loss: 15472.0303, L1 Loss: 56557.1797\n",
            "Batch 180, Loss: 70460.8750, L2 Loss: 15685.6133, L1 Loss: 54775.2617\n",
            "Batch 190, Loss: 65838.9844, L2 Loss: 14880.6885, L1 Loss: 50958.2969\n",
            "Batch 200, Loss: 62167.2773, L2 Loss: 14431.2305, L1 Loss: 47736.0469\n",
            "Batch 210, Loss: 60618.5625, L2 Loss: 14514.0303, L1 Loss: 46104.5312\n",
            "Batch 220, Loss: 57831.2852, L2 Loss: 14164.8604, L1 Loss: 43666.4258\n",
            "Batch 230, Loss: 56024.7656, L2 Loss: 14009.5830, L1 Loss: 42015.1836\n",
            "Batch 240, Loss: 53677.2852, L2 Loss: 13706.1211, L1 Loss: 39971.1641\n",
            "Batch 250, Loss: 52236.0781, L2 Loss: 13626.2930, L1 Loss: 38609.7852\n",
            "Batch 260, Loss: 50775.8672, L2 Loss: 13491.1162, L1 Loss: 37284.7500\n",
            "Batch 270, Loss: 49846.6641, L2 Loss: 13551.0684, L1 Loss: 36295.5977\n",
            "Batch 280, Loss: 46359.6680, L2 Loss: 12838.5830, L1 Loss: 33521.0859\n",
            "Batch 290, Loss: 45875.9766, L2 Loss: 12962.6270, L1 Loss: 32913.3477\n",
            "Batch 300, Loss: 44044.6875, L2 Loss: 12661.9385, L1 Loss: 31382.7480\n",
            "Batch 310, Loss: 42813.6406, L2 Loss: 12546.1621, L1 Loss: 30267.4805\n",
            "Batch 320, Loss: 41467.0391, L2 Loss: 12421.6123, L1 Loss: 29045.4277\n",
            "Batch 330, Loss: 39994.2422, L2 Loss: 12237.0234, L1 Loss: 27757.2207\n",
            "Batch 340, Loss: 38586.0156, L2 Loss: 12048.0684, L1 Loss: 26537.9492\n",
            "Batch 350, Loss: 37766.9922, L2 Loss: 11994.9600, L1 Loss: 25772.0332\n",
            "Batch 360, Loss: 36558.9844, L2 Loss: 11931.6289, L1 Loss: 24627.3555\n",
            "Batch 370, Loss: 36484.8711, L2 Loss: 11973.9443, L1 Loss: 24510.9277\n",
            "Batch 380, Loss: 36364.3008, L2 Loss: 11991.4150, L1 Loss: 24372.8848\n",
            "Batch 390, Loss: 36411.2852, L2 Loss: 12018.5127, L1 Loss: 24392.7715\n",
            "Batch 400, Loss: 35479.8438, L2 Loss: 11885.1934, L1 Loss: 23594.6504\n",
            "Batch 410, Loss: 33583.7148, L2 Loss: 11663.4082, L1 Loss: 21920.3066\n",
            "Batch 420, Loss: 33495.8750, L2 Loss: 11691.8223, L1 Loss: 21804.0547\n",
            "Batch 430, Loss: 32730.7773, L2 Loss: 11624.7832, L1 Loss: 21105.9941\n",
            "Batch 440, Loss: 32083.2344, L2 Loss: 11503.3467, L1 Loss: 20579.8867\n",
            "Batch 450, Loss: 30954.7598, L2 Loss: 11406.6504, L1 Loss: 19548.1094\n",
            "Batch 460, Loss: 31249.1426, L2 Loss: 11429.6602, L1 Loss: 19819.4824\n",
            "Batch 470, Loss: 30675.2070, L2 Loss: 11380.3369, L1 Loss: 19294.8711\n",
            "Batch 480, Loss: 29878.4902, L2 Loss: 11357.7988, L1 Loss: 18520.6914\n",
            "Batch 490, Loss: 29551.5703, L2 Loss: 11303.4424, L1 Loss: 18248.1289\n",
            "Batch 500, Loss: 28635.1914, L2 Loss: 11177.6592, L1 Loss: 17457.5332\n",
            "Batch 510, Loss: 27248.0371, L2 Loss: 11079.8701, L1 Loss: 16168.1670\n",
            "Batch 520, Loss: 28403.9141, L2 Loss: 11204.2422, L1 Loss: 17199.6719\n",
            "Batch 530, Loss: 26622.5996, L2 Loss: 11050.7676, L1 Loss: 15571.8320\n",
            "Batch 540, Loss: 27724.3477, L2 Loss: 11146.9971, L1 Loss: 16577.3496\n",
            "Batch 550, Loss: 26456.1992, L2 Loss: 10994.6621, L1 Loss: 15461.5371\n",
            "Batch 560, Loss: 27102.2148, L2 Loss: 11072.7666, L1 Loss: 16029.4482\n",
            "Batch 570, Loss: 26947.8281, L2 Loss: 11042.5850, L1 Loss: 15905.2432\n",
            "Batch 580, Loss: 27099.8789, L2 Loss: 11033.6846, L1 Loss: 16066.1943\n",
            "Batch 590, Loss: 25624.3828, L2 Loss: 10969.6895, L1 Loss: 14654.6924\n",
            "Batch 600, Loss: 25950.0938, L2 Loss: 10952.6143, L1 Loss: 14997.4805\n",
            "Batch 610, Loss: 25000.2383, L2 Loss: 10951.8848, L1 Loss: 14048.3525\n",
            "Batch 620, Loss: 24508.9453, L2 Loss: 10854.9219, L1 Loss: 13654.0234\n",
            "Batch 630, Loss: 25021.0469, L2 Loss: 10926.1934, L1 Loss: 14094.8535\n",
            "Batch 640, Loss: 25260.8398, L2 Loss: 10911.3652, L1 Loss: 14349.4736\n",
            "Batch 650, Loss: 23804.3965, L2 Loss: 10889.9043, L1 Loss: 12914.4922\n",
            "Batch 660, Loss: 24630.0996, L2 Loss: 10860.4023, L1 Loss: 13769.6973\n",
            "Batch 670, Loss: 23379.9453, L2 Loss: 10836.5332, L1 Loss: 12543.4111\n",
            "Batch 680, Loss: 24049.4805, L2 Loss: 10824.2881, L1 Loss: 13225.1934\n",
            "Batch 690, Loss: 24383.3242, L2 Loss: 10808.7266, L1 Loss: 13574.5967\n",
            "Batch 700, Loss: 23995.4570, L2 Loss: 10776.6689, L1 Loss: 13218.7881\n",
            "Batch 710, Loss: 23841.0488, L2 Loss: 10748.2109, L1 Loss: 13092.8379\n",
            "Batch 720, Loss: 24163.1660, L2 Loss: 10692.1270, L1 Loss: 13471.0391\n",
            "Batch 730, Loss: 22304.3438, L2 Loss: 10662.2627, L1 Loss: 11642.0811\n",
            "Batch 740, Loss: 22946.1562, L2 Loss: 10609.9883, L1 Loss: 12336.1689\n",
            "Batch 750, Loss: 23174.7852, L2 Loss: 10574.6367, L1 Loss: 12600.1494\n",
            "Batch 760, Loss: 22798.3340, L2 Loss: 10543.7363, L1 Loss: 12254.5977\n",
            "Batch 770, Loss: 22874.4141, L2 Loss: 10504.7344, L1 Loss: 12369.6797\n",
            "Batch 780, Loss: 22130.5039, L2 Loss: 10431.7432, L1 Loss: 11698.7617\n",
            "Batch 790, Loss: 21311.1328, L2 Loss: 10348.8311, L1 Loss: 10962.3027\n",
            "Batch 800, Loss: 22541.0820, L2 Loss: 10321.0977, L1 Loss: 12219.9834\n",
            "Batch 810, Loss: 22815.3047, L2 Loss: 10268.7979, L1 Loss: 12546.5068\n",
            "Batch 820, Loss: 21547.3828, L2 Loss: 10287.6182, L1 Loss: 11259.7637\n",
            "Batch 830, Loss: 21432.9492, L2 Loss: 10185.2373, L1 Loss: 11247.7129\n",
            "Batch 840, Loss: 21869.9766, L2 Loss: 10177.7354, L1 Loss: 11692.2402\n",
            "Batch 850, Loss: 20953.8867, L2 Loss: 10154.5000, L1 Loss: 10799.3877\n",
            "Batch 860, Loss: 21310.9375, L2 Loss: 10092.4482, L1 Loss: 11218.4883\n",
            "Batch 870, Loss: 21162.7402, L2 Loss: 9996.3057, L1 Loss: 11166.4346\n",
            "Batch 880, Loss: 21599.1934, L2 Loss: 10003.5400, L1 Loss: 11595.6533\n",
            "Batch 890, Loss: 20308.7148, L2 Loss: 9925.4502, L1 Loss: 10383.2656\n",
            "Batch 900, Loss: 20950.2637, L2 Loss: 9874.4443, L1 Loss: 11075.8193\n",
            "Batch 910, Loss: 20797.7148, L2 Loss: 9799.6016, L1 Loss: 10998.1143\n",
            "Batch 920, Loss: 20983.8359, L2 Loss: 9751.3828, L1 Loss: 11232.4541\n",
            "Batch 930, Loss: 20057.9180, L2 Loss: 9686.3086, L1 Loss: 10371.6104\n",
            "Batch 940, Loss: 20667.2188, L2 Loss: 9657.1104, L1 Loss: 11010.1084\n",
            "Batch 950, Loss: 20313.4375, L2 Loss: 9642.6807, L1 Loss: 10670.7568\n",
            "Batch 960, Loss: 20503.1133, L2 Loss: 9543.4365, L1 Loss: 10959.6758\n",
            "Batch 970, Loss: 20242.1094, L2 Loss: 9520.0742, L1 Loss: 10722.0342\n",
            "Batch 980, Loss: 20322.0918, L2 Loss: 9435.9160, L1 Loss: 10886.1758\n",
            "Batch 990, Loss: 19841.8008, L2 Loss: 9391.0811, L1 Loss: 10450.7188\n",
            "Batch 1000, Loss: 20518.7656, L2 Loss: 9293.1885, L1 Loss: 11225.5771\n",
            "Batch 1010, Loss: 19657.7578, L2 Loss: 9309.7588, L1 Loss: 10347.9980\n",
            "Batch 1020, Loss: 20363.8926, L2 Loss: 9268.3643, L1 Loss: 11095.5283\n",
            "Batch 1030, Loss: 19606.6641, L2 Loss: 9187.1777, L1 Loss: 10419.4863\n",
            "Batch 1040, Loss: 20130.1875, L2 Loss: 9110.8584, L1 Loss: 11019.3301\n",
            "Batch 1050, Loss: 19549.2910, L2 Loss: 9053.3701, L1 Loss: 10495.9209\n",
            "Batch 1060, Loss: 19441.3789, L2 Loss: 9053.9053, L1 Loss: 10387.4736\n",
            "Batch 1070, Loss: 18527.7695, L2 Loss: 9000.4170, L1 Loss: 9527.3525\n",
            "Batch 1080, Loss: 19100.4023, L2 Loss: 9006.4189, L1 Loss: 10093.9824\n",
            "Batch 1090, Loss: 18645.5879, L2 Loss: 8959.2227, L1 Loss: 9686.3652\n",
            "Batch 1100, Loss: 19554.2266, L2 Loss: 8772.4502, L1 Loss: 10781.7764\n",
            "Batch 1110, Loss: 18644.8867, L2 Loss: 8782.5381, L1 Loss: 9862.3496\n",
            "Batch 1120, Loss: 19205.6875, L2 Loss: 8702.1982, L1 Loss: 10503.4902\n",
            "Batch 1130, Loss: 19539.9180, L2 Loss: 8619.0557, L1 Loss: 10920.8623\n",
            "Batch 1140, Loss: 19028.4648, L2 Loss: 8580.9932, L1 Loss: 10447.4717\n",
            "Batch 1150, Loss: 18851.5312, L2 Loss: 8573.4922, L1 Loss: 10278.0391\n",
            "Batch 1160, Loss: 18505.6016, L2 Loss: 8533.3945, L1 Loss: 9972.2080\n",
            "Batch 1170, Loss: 18798.5195, L2 Loss: 8419.2764, L1 Loss: 10379.2441\n",
            "Batch 1180, Loss: 18735.7168, L2 Loss: 8379.6104, L1 Loss: 10356.1064\n",
            "Batch 1190, Loss: 18454.4648, L2 Loss: 8351.7910, L1 Loss: 10102.6748\n",
            "Batch 1200, Loss: 19186.1094, L2 Loss: 8218.3057, L1 Loss: 10967.8027\n",
            "Batch 1210, Loss: 18644.3184, L2 Loss: 8260.9678, L1 Loss: 10383.3506\n",
            "Batch 1220, Loss: 18394.1016, L2 Loss: 8188.7544, L1 Loss: 10205.3477\n",
            "Batch 1230, Loss: 18097.8926, L2 Loss: 8190.2432, L1 Loss: 9907.6494\n",
            "Batch 1240, Loss: 18912.4219, L2 Loss: 8054.0254, L1 Loss: 10858.3965\n",
            "Batch 1250, Loss: 17764.1328, L2 Loss: 8059.1641, L1 Loss: 9704.9688\n",
            "Batch 1260, Loss: 18065.3750, L2 Loss: 8005.6514, L1 Loss: 10059.7227\n",
            "Batch 1270, Loss: 17580.4492, L2 Loss: 7959.1353, L1 Loss: 9621.3135\n",
            "Batch 1280, Loss: 17547.2285, L2 Loss: 7888.6479, L1 Loss: 9658.5811\n",
            "Batch 1290, Loss: 18164.1230, L2 Loss: 7804.9673, L1 Loss: 10359.1562\n",
            "Batch 1300, Loss: 18225.0410, L2 Loss: 7820.0337, L1 Loss: 10405.0068\n",
            "Batch 1310, Loss: 17489.6680, L2 Loss: 7730.9893, L1 Loss: 9758.6787\n",
            "Batch 1320, Loss: 17858.6426, L2 Loss: 7688.4956, L1 Loss: 10170.1465\n",
            "Batch 1330, Loss: 18247.0176, L2 Loss: 7614.5093, L1 Loss: 10632.5078\n",
            "Batch 1340, Loss: 17706.2598, L2 Loss: 7628.9580, L1 Loss: 10077.3018\n",
            "Batch 1350, Loss: 17073.1797, L2 Loss: 7608.0894, L1 Loss: 9465.0908\n",
            "Batch 1360, Loss: 17782.7773, L2 Loss: 7517.7651, L1 Loss: 10265.0117\n",
            "Batch 1370, Loss: 17780.6094, L2 Loss: 7419.0513, L1 Loss: 10361.5576\n",
            "Batch 1380, Loss: 17583.7754, L2 Loss: 7397.1362, L1 Loss: 10186.6396\n",
            "Batch 1390, Loss: 17670.0742, L2 Loss: 7306.8867, L1 Loss: 10363.1865\n",
            "Batch 1400, Loss: 17202.1172, L2 Loss: 7307.4116, L1 Loss: 9894.7061\n",
            "Batch 1410, Loss: 17389.6152, L2 Loss: 7254.0571, L1 Loss: 10135.5576\n",
            "Batch 1420, Loss: 17208.6211, L2 Loss: 7223.0918, L1 Loss: 9985.5283\n",
            "Batch 1430, Loss: 17245.5234, L2 Loss: 7199.3564, L1 Loss: 10046.1680\n",
            "Batch 1440, Loss: 17222.1660, L2 Loss: 7139.5767, L1 Loss: 10082.5898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Autoencoder"
      ],
      "metadata": {
        "id": "4G3YdRYB4RZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = get_recons_loss(num_batches=5, local_encoder=encoder) # - [ ] TODO: Increase number of batches?"
      ],
      "metadata": {
        "id": "2BewccqL4Uj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Frequencies"
      ],
      "metadata": {
        "id": "9XdD3DmT4Vgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = get_freqs(num_batches=25, local_encoder=encoder)\n",
        "\n",
        "# Add 1e-6.5 so that dead features show up as log_freq -6.5\n",
        "log_freq = (freqs + 10 ** -6.5).log10().cpu().numpy()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(log_freq, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Log Frequency of Features\")\n",
        "plt.xlabel(\"Log Frequency\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmvsQC9y4a2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting A Feature"
      ],
      "metadata": {
        "id": "mdR_2rtj4cWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate a non-rare feature, for example, `feature_id = 7`."
      ],
      "metadata": {
        "id": "LfbqK50lBl-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "feature_id = 1 # Change as needed\n",
        "batch_size = 128  # Adjust as needed\n",
        "\n",
        "print(f\"Feature freq: {freqs[feature_id].item():.4f}\")\n",
        "\n",
        "# Get activations\n",
        "idx = torch.randperm(len(all_tokens))[:batch_size]\n",
        "tokens = all_tokens[idx].to(model_device)\n",
        "contexts = all_contexts[idx].to(model_device)\n",
        "targets = all_targets[idx].to(model_device)\n",
        "\n",
        "mlp_activations = []\n",
        "\n",
        "def capture_mlp_activations(module, input, output):\n",
        "    mlp_activations.append(output.detach())\n",
        "\n",
        "# Register the hook\n",
        "mlp_layer = model.transformer.h[0].mlp.c_proj\n",
        "hook_handle = mlp_layer.register_forward_hook(capture_mlp_activations)\n",
        "\n",
        "# Forward pass\n",
        "logits, _ = model(tokens, contexts)\n",
        "# Remove the hook\n",
        "hook_handle.remove()\n",
        "\n",
        "# Get the activations\n",
        "mlp_acts = mlp_activations[0]  # Shape: [batch_size, seq_len, d_mlp]\n",
        "mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "\n",
        "# Pass through encoder\n",
        "loss_enc, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
        "print(\"hidden_acts.shape\", hidden_acts.shape)\n",
        "\n",
        "# Create token dataframe\n",
        "token_df = make_token_df(tokens.cpu().flatten())\n",
        "token_df[\"feature\"] = hidden_acts[:, feature_id].detach().cpu().numpy()\n",
        "display(token_df.sort_values(\"feature\", ascending=False).head(20))"
      ],
      "metadata": {
        "id": "UYT7m6vp4cPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Visualizing Feature Activations"
      ],
      "metadata": {
        "id": "h-yvVk0v4cFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the character-to-index and index-to-character mappings from the dataset\n",
        "char2idx = train_dataset.stoi  # Character to index mapping\n",
        "idx2char = train_dataset.itos  # Index to character mapping\n",
        "\n",
        "# Define the tokenization function using the dataset's mapping\n",
        "def tokenize_text(text):\n",
        "    # Map each character to its index, default to PAD token (0) for unknown characters\n",
        "    return [char2idx.get(ch, train_dataset.char_PAD_TOKEN) for ch in text]\n",
        "\n",
        "def make_feature_vis_gradio(feature_id, starting_text=None):\n",
        "    if starting_text is None:\n",
        "        starting_text = \"Sample text for visualization\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.HTML(value=f\"Hacky Interactive Neuroscope for cursivetransformer\")\n",
        "        # The input elements\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text = gr.Textbox(label=\"Context Text\", value=starting_text)\n",
        "                feature_index = gr.Number(\n",
        "                    label=\"Feature Index\", value=feature_id, precision=0\n",
        "                )\n",
        "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
        "                inputs = [text, feature_index, max_val]\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # The output element\n",
        "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(feature_id))\n",
        "        def update_output(text, feature_index, max_val):\n",
        "            return basic_feature_vis(text, int(feature_index), max_val)\n",
        "        for inp in inputs:\n",
        "            inp.change(update_output, inputs, out)\n",
        "    demo.launch(share=True)\n",
        "\n",
        "# Visualize the feature activations\n",
        "starting_text = \"Sample text to test feature activations in cursivetransformer\"\n",
        "make_feature_vis_gradio(feature_id, starting_text)"
      ],
      "metadata": {
        "id": "KvYERycr4bz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4o5BPGlB6iD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}