{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLfSNoy0BByECRajf6HnKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/Sam's_ME_crash_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mechanistic Interpretability Crash Course\n",
        "The purpose of this notebook is to both outline the most important high-level concepts, as well as dive into the technical details of, the emerging field of mechanistic interpretability. Additionally, this notebook will serve as the gold-standard I will reference moving forward to tie together the insights that can be gained from applying ME techniques, and how these techniques can be applied to our modified transformer architecture.\n",
        "\n",
        "To this end, we will break things down into the following sections. Note that while I intend for the interested reader to cover them in order, it should not be required, as each section should contain the bulk of what is needed to build upon itself in a self-contained, yet first principled, manner:\n",
        "\n",
        "1. Transformer Circuits: What are they, and why they are useful?\n",
        "2. Induction Heads: What are they, how do they form, and how do we detect them?\n",
        "3. Polysemantic features in the neuron basis\n",
        "4. Learning the monosemantic features from their polysemantic embeddings via training a hooked SAE on the activations within an attention block\n",
        "5. How all of this applies to the `cursivetransformer` project"
      ],
      "metadata": {
        "id": "m5uC3XtSP7Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 1: Transformer Circuits: What are they, and why are they useful?"
      ],
      "metadata": {
        "id": "59JxgXWmRtiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this chapter, I will be using the following 2 papers as references:\n",
        "1. https://transformer-circuits.pub/2021/framework/index.html\n",
        "2. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"
      ],
      "metadata": {
        "id": "k8uomWyzRztP"
      }
    }
  ]
}