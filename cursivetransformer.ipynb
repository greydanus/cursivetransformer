{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/cross-attention-ablation-study/cursivetransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3v2biu1Ttg7"
      },
      "source": [
        "# Training a Cursive Transformer\n",
        "Sam Greydanus & Zach Wimpee | 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QvXUjGZHl7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50400e1f-d73b-4928-ab31-7b79aadb9c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "! pip -q install wandb\n",
        "! wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0WRHSM_Kksp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda76279-26f0-4ad4-f7a8-c78a6ad54dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "import os, sys, time, math, argparse, io, copy, json, pdb\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from math import comb\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxM2-WQjBJ8f"
      },
      "source": [
        "## Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfFC9VAC3oPz"
      },
      "outputs": [],
      "source": [
        "def plot_strokes(stroke, title, fig=None, ax=None):\n",
        "    \"\"\"Plot a single stroke\"\"\"\n",
        "    if fig is None or ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 2))\n",
        "\n",
        "    # Separate strokes based on pen lifts\n",
        "    strokes = []\n",
        "    current_stroke = []\n",
        "    for point in stroke:\n",
        "        if point[2] == 1:  # Pen is down\n",
        "            current_stroke.append(point)\n",
        "        else:  # Pen is up\n",
        "            if current_stroke:\n",
        "                strokes.append(current_stroke)\n",
        "                current_stroke = []\n",
        "    if current_stroke:\n",
        "        strokes.append(current_stroke)\n",
        "\n",
        "    # Plot each stroke\n",
        "    for stroke in strokes:\n",
        "        x, y = zip(*[(p[0], 1 - p[1]) for p in stroke])  # Invert y-axis\n",
        "        ax.plot(x, y, 'b-')\n",
        "\n",
        "    ax.set_aspect('equal') ; ax.set_title(title)\n",
        "\n",
        "    if fig is None:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def load_and_parse_data(min_ascii_length=3):\n",
        "    uploaded = files.upload()\n",
        "    file_content = next(iter(uploaded.values()))\n",
        "    data = json.loads(file_content.decode('utf-8'))\n",
        "    for i in range(len(data)):\n",
        "      strokes = np.array(data[i]['points'])\n",
        "      strokes[:,0:1] *= data[i]['metadata']['aspectRatio']\n",
        "      strokes[:, 0] -= strokes[0, 0]\n",
        "      data[i]['points'] = strokes\n",
        "    data = [d for d in data if len(d['metadata']['asciiSequence']) >= min_ascii_length]\n",
        "    return data\n",
        "\n",
        "# data = load_and_parse_data()\n",
        "# print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwDtf2gbzf9g"
      },
      "outputs": [],
      "source": [
        "def decompose_offsets(offsets):\n",
        "    dx, dy = offsets[:, 0], offsets[:, 1]\n",
        "    r = np.hypot(dx, dy)\n",
        "    theta = np.arctan2(dy, dx)\n",
        "    return np.column_stack((theta, r, offsets[:, 2]))\n",
        "\n",
        "def reconstruct_offsets(polar_data):\n",
        "    theta, r = polar_data[:, 0], polar_data[:, 1]\n",
        "    dx = r * np.cos(theta)\n",
        "    dy = r * np.sin(theta)\n",
        "    return np.column_stack((dx, dy, polar_data[:, 2]))\n",
        "\n",
        "def strokes_to_offsets(points):\n",
        "    # Calculate differences (dx, dy), not considering pen_down\n",
        "    offsets = np.zeros_like(points)\n",
        "    offsets[1:, 0:2] = np.diff(points[:, 0:2], axis=0)  # Compute dx, dy\n",
        "    offsets[:, 2] = points[:, 2]  # Copy pen_down directly\n",
        "\n",
        "    # Decouple direction from magnitude (this will help with tokenization)\n",
        "    offsets_dec = decompose_offsets(offsets)\n",
        "    return offsets_dec\n",
        "\n",
        "def offsets_to_strokes(offsets_dec):\n",
        "    # Calculate cumulative sums to get absolute positions\n",
        "    offsets = reconstruct_offsets(offsets_dec)\n",
        "    absolute_coords = np.cumsum(offsets[:, :2], axis=0)\n",
        "    stroke_data = np.hstack((absolute_coords, offsets[:, 2:3]))\n",
        "    return stroke_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jHeGQgJnxaZ"
      },
      "outputs": [],
      "source": [
        "def combine_handwriting_examples(examples, space_width=0.17):\n",
        "    assert len(set(ex['metadata']['author'] for ex in examples)) == 1, \"All examples must have the same author\"\n",
        "\n",
        "    combined_metadata = {\n",
        "        'author': examples[0]['metadata']['author'],\n",
        "        'asciiSequence': ' '.join(ex['metadata']['asciiSequence'] for ex in examples),\n",
        "        'pointCount': sum(ex['metadata']['pointCount'] for ex in examples),\n",
        "        'strokeCount': sum(ex['metadata']['strokeCount'] for ex in examples),\n",
        "        'aspectRatio': examples[0]['metadata']['aspectRatio']\n",
        "    }\n",
        "\n",
        "    combined_points, current_x_offset, total_width = [], 0, 0\n",
        "\n",
        "    for i, example in enumerate(examples):\n",
        "        points = example['points']\n",
        "        word_width = np.max(points[:, 0]) - np.min(points[:, 0])\n",
        "        total_width += word_width\n",
        "\n",
        "        normalized_points = points.copy()\n",
        "        normalized_points[:, 0] -= np.min(points[:, 0])\n",
        "        normalized_points[:, 0] += current_x_offset\n",
        "\n",
        "        combined_points.append(normalized_points)\n",
        "        current_x_offset += word_width\n",
        "\n",
        "        if i < len(examples) - 1:\n",
        "            combined_points.append(np.array([[current_x_offset + space_width, normalized_points[-1, 1], 0]]))\n",
        "            current_x_offset += space_width\n",
        "            total_width += space_width\n",
        "            combined_metadata['pointCount'] += 1\n",
        "\n",
        "    combined_points = np.vstack(combined_points)\n",
        "    return {'metadata': combined_metadata, 'points': combined_points}\n",
        "\n",
        "def rotate_points(points, max_angle=10):\n",
        "    angle = np.deg2rad(np.random.uniform(-max_angle, max_angle))\n",
        "    cos_angle, sin_angle = np.cos(angle), np.sin(angle)\n",
        "\n",
        "    x, y = points[:, 0], points[:, 1]\n",
        "    x_rot = x * cos_angle - y * sin_angle\n",
        "    y_rot = x * sin_angle + y * cos_angle\n",
        "\n",
        "    points[:, 0], points[:, 1] = x_rot, y_rot\n",
        "    return points\n",
        "\n",
        "def shear_points(points, shear_range=(-0.4, 0.4)):\n",
        "    shear_factor = np.random.uniform(*shear_range)\n",
        "    x, y = points[:, 0], points[:, 1]\n",
        "    x_sheared = x + shear_factor * y\n",
        "    points[:, 0] = x_sheared\n",
        "    return points\n",
        "\n",
        "def generate_word_combos(raw_json, desired_num_combos=10000, num_words=3, max_angle=4, shear_range=(-0.4, 0.4)):\n",
        "  num_combos = comb(len(raw_json), num_words)\n",
        "  print(f'For a dataset of {len(raw_json)} examples we can generate {num_combos} combinations of {num_words} examples.')\n",
        "  print(f'Generating {desired_num_combos} random (and thus possibly overlapping) combos...')\n",
        "  combo_json = []\n",
        "  for i in range(desired_num_combos):\n",
        "    ixs = np.random.choice(len(raw_json), size=num_words, replace=False)\n",
        "    words_to_merge = [raw_json[i] for i in ixs]\n",
        "    example = combine_handwriting_examples(words_to_merge)\n",
        "    example['points'] = shear_points(example['points'], shear_range)\n",
        "    example['points'] = rotate_points(example['points'], max_angle)\n",
        "    combo_json.append( example )\n",
        "  return combo_json\n",
        "\n",
        "def load_and_combine_examples(desired_num_combos=10000, num_words=3):\n",
        "  data = load_and_parse_data()\n",
        "  return generate_word_combos(data, desired_num_combos, num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yyu0V40F4IJ"
      },
      "outputs": [],
      "source": [
        "# combo_json = load_and_combine_examples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IccIK4-DbjyU"
      },
      "outputs": [],
      "source": [
        "# combo = combo_json[11]\n",
        "# offsets = strokes_to_offsets(combo['points'])\n",
        "# print(offsets[:10])\n",
        "# strokes = offsets_to_strokes(offsets)\n",
        "# _ = plot_strokes(strokes, title=combo['metadata']['asciiSequence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OobnYMcDKf78"
      },
      "outputs": [],
      "source": [
        "def remove_random_points(stroke, remove_percentage=0.04):\n",
        "    num_points = np.random.randint(len(stroke))\n",
        "    num_remove = int(num_points * remove_percentage)\n",
        "    indices = np.random.choice(range(1, num_points - 1), num_remove, replace=False).astype(np.int32)\n",
        "    return np.delete(stroke, indices, axis=0)\n",
        "\n",
        "def efficient_downsample(stroke, fraction=0.65):\n",
        "    n = len(stroke)\n",
        "    keep = int(n * fraction)\n",
        "    drop_indices = np.random.choice(n, n - keep, replace=False)\n",
        "    mask = np.ones(n, dtype=bool)\n",
        "    mask[drop_indices] = False\n",
        "    pen_up_mask = stroke[:, -1] == 0\n",
        "    mask |= pen_up_mask\n",
        "    return stroke[mask]\n",
        "\n",
        "\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, strokes, texts, chars, max_seq_length=1100, max_text_length=50, name='', augment=False):\n",
        "        self.name = name\n",
        "        self.strokes = strokes  # List of Nx4 arrays, each representing a cursive sentence\n",
        "        self.texts = texts  # List of corresponding text strings\n",
        "        self.chars = chars  # String of all possible characters\n",
        "        self.augment = augment\n",
        "\n",
        "        self.theta_bins = np.linspace(-np.pi, np.pi, 226)  # 100 bins for theta\n",
        "\n",
        "        # Modify mag_bins to incorporate pen_down information\n",
        "        mag_bins_pen_down = np.concatenate([\n",
        "            np.asarray([0]),\n",
        "            np.linspace(0.005, 0.050, 50),  # Close around 0.01, 30 bins\n",
        "            np.geomspace(0.051, 4, 121)[:-1]  # 150 exponential bins\n",
        "        ])\n",
        "        mag_bins_pen_up = mag_bins_pen_down + max(mag_bins_pen_down) + 1  # Offset for pen-up states\n",
        "        self.mag_bins = np.concatenate([mag_bins_pen_down, mag_bins_pen_up])\n",
        "\n",
        "        # self.feature_sizes = [len(self.dx_bins), len(self.dy_bins), len(self.mag_bins)]\n",
        "        self.feature_sizes = [len(self.theta_bins), len(self.mag_bins)]\n",
        "        self.cumulative_sizes = np.cumsum([0] + self.feature_sizes)\n",
        "\n",
        "        # Add special tokens for strokes\n",
        "        self.PAD_TOKEN = sum(self.feature_sizes)\n",
        "        self.END_TOKEN = sum(self.feature_sizes) + 1\n",
        "\n",
        "        # Character tokenization\n",
        "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:s for s,i in self.stoi.items()}\n",
        "        self.char_PAD_TOKEN = 0\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def augment_stroke(self, stroke):\n",
        "\n",
        "        # # stroke = remove_random_points(stroke, remove_percentage=0.03) # Drop some points\n",
        "        # stroke = horizontal_shear(stroke, shear_range=(-0.3, 0.3)) # Horizontal shear\n",
        "\n",
        "        stroke[:, 1:2] = stroke[:, 1:2] * np.random.uniform(0.8, 1.2)\n",
        "\n",
        "        noise = np.random.normal(0, 0.002, stroke[:, 1:2].shape) # Random noise\n",
        "        stroke[:, 1:2] += noise\n",
        "\n",
        "        # stroke = rotate_polar_stroke(stroke, max_angle=30)\n",
        "\n",
        "        # Random starting point\n",
        "        stroke = stroke[np.random.randint(1, 8):-np.random.randint(1, 8)]\n",
        "\n",
        "        # Downsample stroke\n",
        "        stroke = efficient_downsample(stroke, .4)\n",
        "        return stroke\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return sum(self.feature_sizes) + 2  # +2 for PAD and END tokens\n",
        "\n",
        "    def get_char_vocab_size(self):\n",
        "        return len(self.chars) + 1  # +1 for PAD token\n",
        "\n",
        "    def get_stroke_seq_length(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "    def get_text_seq_length(self):\n",
        "        return self.max_text_length\n",
        "\n",
        "    def encode_stroke(self, stroke):\n",
        "        theta_idx = np.digitize(stroke[:, 0], self.theta_bins) - 1\n",
        "\n",
        "        # Encode magnitude and pen state together\n",
        "        mag_idx = np.digitize(stroke[:, 1], self.mag_bins[:len(self.mag_bins)//2]) - 1\n",
        "        mag_idx[stroke[:, 2] == 0] += len(self.mag_bins) // 2  # Offset for pen-up states\n",
        "\n",
        "        encoded = np.column_stack([\n",
        "            theta_idx + self.cumulative_sizes[0],\n",
        "            mag_idx + self.cumulative_sizes[1]\n",
        "        ])\n",
        "        return encoded.flatten()\n",
        "\n",
        "    def decode_stroke(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "\n",
        "        # Remove PAD and END tokens\n",
        "        ix = ix[(ix != self.PAD_TOKEN) & (ix != self.END_TOKEN)]\n",
        "\n",
        "        # Reshape the flattened array back to Nx2\n",
        "        ix = ix[:(len(ix)//2)*2]\n",
        "        ix = ix.reshape(-1, 2)\n",
        "        theta = self.theta_bins[(ix[:, 0] - self.cumulative_sizes[0]).clip(0, len(self.theta_bins)-1)]\n",
        "\n",
        "        mag_idx = ix[:, 1] - self.cumulative_sizes[1]\n",
        "        pen = (mag_idx < len(self.mag_bins) // 2).astype(int)\n",
        "        mag_idx[pen == 0] -= len(self.mag_bins) // 2\n",
        "        mag = self.mag_bins[:len(self.mag_bins)//2][mag_idx.clip(0, len(self.mag_bins)//2 - 1)]\n",
        "\n",
        "        return np.column_stack([theta, mag, pen])\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return torch.tensor([self.stoi.get(ch, self.char_PAD_TOKEN) for ch in text], dtype=torch.long)\n",
        "\n",
        "    def decode_text(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "        return ''.join([self.itos.get(i, '') for i in ix if i != self.char_PAD_TOKEN])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stroke = self.strokes[idx]\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            stroke = self.augment_stroke(stroke.copy())\n",
        "\n",
        "        # Encode stroke\n",
        "        stroke_offsets = self.strokes_to_polar_offsets(stroke)\n",
        "        encoded_stroke = self.encode_stroke(stroke_offsets)\n",
        "        x = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "        y = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "\n",
        "        seq_len = min(len(encoded_stroke), self.max_seq_length - 1)  # -1 to leave room for END token\n",
        "        x[:seq_len] = torch.tensor(encoded_stroke[:seq_len], dtype=torch.long)\n",
        "        x[seq_len] = self.END_TOKEN\n",
        "\n",
        "        y[:seq_len] = x[1:seq_len+1]\n",
        "        y[seq_len] = self.END_TOKEN\n",
        "\n",
        "        # Encode text (context) and pad to max_text_length of 30\n",
        "        encoded_text = self.encode_text(text)\n",
        "        c = torch.full((self.max_text_length,), self.char_PAD_TOKEN, dtype=torch.long)\n",
        "        text_len = min(len(encoded_text), self.max_text_length)\n",
        "        c[:text_len] = encoded_text[:text_len]\n",
        "        return x, c, y\n",
        "\n",
        "    @staticmethod\n",
        "    def strokes_to_polar_offsets(stroke):\n",
        "        offsets = np.diff(stroke[:, :2], axis=0)\n",
        "        theta = np.arctan2(offsets[:, 1], offsets[:, 0])\n",
        "        r = np.hypot(offsets[:, 0], offsets[:, 1])\n",
        "        pen_state = stroke[1:, 2]  # pen state corresponds to the end point of each offset\n",
        "        return np.column_stack((theta, r, pen_state))\n",
        "\n",
        "\n",
        "def create_datasets(augment=True, max_seq_length=1100, num_words=3):\n",
        "  raw_json = load_and_parse_data()\n",
        "\n",
        "  # partition the input data into a training and the test set\n",
        "  test_set_size = min(1000, int(len(raw_json) * 0.10)) # 10% of the training set, or up to 1000 examples\n",
        "  rp = torch.randperm(len(raw_json)).tolist()\n",
        "  train_examples = [raw_json[i] for i in rp[:-test_set_size]]\n",
        "  test_examples = [raw_json[i] for i in rp[-test_set_size:]]\n",
        "\n",
        "  train_examples = generate_word_combos(train_examples, desired_num_combos=98000, num_words=num_words)\n",
        "  test_examples = generate_word_combos(test_examples, desired_num_combos=2000, num_words=num_words)\n",
        "\n",
        "  train_strokes = [copy.deepcopy(v['points']) for v in train_examples]\n",
        "  train_texts = [copy.deepcopy(v['metadata']['asciiSequence']) for v in train_examples]\n",
        "\n",
        "  test_strokes = [copy.deepcopy(v['points']) for v in test_examples]\n",
        "  test_texts = [copy.deepcopy(v['metadata']['asciiSequence']) for v in test_examples]\n",
        "\n",
        "  chars = \"abcdefghijklmnopqrstuvwxyz \"\n",
        "  print(f\"Number of examples in the train dataset: {len(train_examples)}\")\n",
        "  print(f\"Number of examples in the test dataset: {len(test_examples)}\")\n",
        "  print(f\"Max token sequence length: {max_seq_length}\")\n",
        "  print(f\"Number of unique characters in the ascii vocabulary: {len(chars)}\")\n",
        "  print(\"Ascii vocabulary:\")\n",
        "  print(f'\\t\"{chars}\"')\n",
        "\n",
        "  print(f\"Split up the dataset into {len(train_examples)} training examples and {len(test_examples)} test examples\")\n",
        "\n",
        "  # wrap in dataset objects\n",
        "  train_dataset = StrokeDataset(train_strokes, train_texts, chars, max_seq_length, name='train', augment=augment)\n",
        "  test_dataset = StrokeDataset(test_strokes, test_texts, chars, max_seq_length, name='test', augment=augment)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader:\n",
        "    \"\"\"\n",
        "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
        "    a better way in PyTorch to just create an infinite dataloader?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
        "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
        "        self.data_iter = iter(self.train_loader)\n",
        "\n",
        "    def next(self):\n",
        "        try:\n",
        "            batch = next(self.data_iter)\n",
        "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
        "            self.data_iter = iter(self.train_loader)\n",
        "            batch = next(self.data_iter)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRH6-roo_bZQ"
      },
      "outputs": [],
      "source": [
        "# dataset, _ = create_datasets(augment=True, max_seq_length=1300, num_words=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.seed(0)\n",
        "# example_ix = 18\n",
        "# x1, c1, y1 = dataset[example_ix]  # Get tokenized version of the second example\n",
        "# x2, c2, y2 = dataset[example_ix]\n",
        "\n",
        "# o1 = dataset.decode_stroke(x1)\n",
        "# r1 = offsets_to_strokes(o1)\n",
        "# fig, ax = plot_strokes(r1, title='Reconstructed text (data augmentation seed 1)')\n",
        "\n",
        "# o2 = dataset.decode_stroke(x2)\n",
        "# print(o2.shape[0]*2)\n",
        "# r2 = offsets_to_strokes(o2)\n",
        "# fig, ax = plot_strokes(r2, title='Reconstructed from tokens (data augmentation seed 2)')"
      ],
      "metadata": {
        "id": "RYzjLVpM66s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoded_stroke = o2#dataset.decode_stroke(x)\n",
        "# decoded_text = c1 #dataset.decode_text(c)\n",
        "\n",
        "# print(\"Original stroke shape:\", dataset.strokes[example_ix].shape)\n",
        "# print(\"Encoded stroke shape:\", x1.shape)\n",
        "# print(\"Encoded text shape:\", c1.shape)\n",
        "# print(\"Decoded stroke shape:\", decoded_stroke.shape)\n",
        "\n",
        "# print(\"\\nEncoded strokes (first 10 tokens, aka 5 pen strokes):\")\n",
        "# print([v.item() for v in x1[:10]])\n",
        "# print(\"\\n\\nEncoded text (first 10 tokens, aka ascii chars):\")\n",
        "# print(c1[:10])\n",
        "# print(\"\\nDecoded stroke (first 5 rows, aka 5 pen steps):\")\n",
        "# print(decoded_stroke[:5])\n",
        "# print(\"\\nInferred number of tokens required to represent:\")\n",
        "# print(decoded_stroke.shape[0]*2)\n",
        "# print(\"\\nDecoded text:\")\n",
        "# print(dataset.decode_text(c1))"
      ],
      "metadata": {
        "id": "6jVSOP1Etmz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXAlpngErwVo"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azf2fXB3MeRL"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        # query projections for all heads\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # key, value projections for all heads\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        # calculate query for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # calculate key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "\n",
        "        # cross-attention; (B, nh, T, hs) x (B, nh, hs, T_ctx) -> (B, nh, T, T_ctx)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T_ctx) x (B, nh, T_ctx, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x74SZP_qymr"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Transformer Language Model (*exactly* as used in GPT-2)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = CrossAttention(config) # NEW\n",
        "        self.ln_3 = nn.LayerNorm(config.n_embd) # NEW\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.cross_attn(self.ln_2(x), context)\n",
        "        x = x + self.mlpf(self.ln_3(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wce = nn.Embedding(config.context_vocab_size, config.n_embd), # NEW\n",
        "            wcpe = nn.Embedding(config.context_block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"Number of Transformer parameters: {:.0f}\".format(n_params,))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def forward(self, idx, context, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        context_t = context.size(-1)\n",
        "        context_pos = torch.arange(0, context_t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "        context_emb = self.transformer.wce(context) # context embeddings of shape (b, t_ctx, n_embd)\n",
        "        context_pos_emb = self.transformer.wcpe(context_pos)\n",
        "        c = context_emb + context_pos_emb\n",
        "\n",
        "        if self.config.ablate_cross_attention:\n",
        "          c = torch.zeros_like(c)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, c)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = None # length of the input sequences of integers\n",
        "    context_block_size: int = None\n",
        "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
        "    context_vocab_size: int = None # size of the context vocabulary (ASCII characters)\n",
        "    context_length: int = None # maximum length of the context sequence\n",
        "    # parameters below control the sizes of each model slightly differently\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    n_ctx_head: int = 4 # number of heads for cross-attention\n",
        "    ablate_cross_attention: bool = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoVuOu7ZB3HI"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exe5-vbitBJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d338c99f-7946-4642-bf16-47545906dbcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-895ab440-0acc-4d40-a670-d7b7f6025b7c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-895ab440-0acc-4d40-a670-d7b7f6025b7c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1197-bigbank-cursive.json to 1197-bigbank-cursive.json\n",
            "For a dataset of 1078 examples we can generate 208207076 combinations of 3 examples.\n",
            "Generating 98000 random (and thus possibly overlapping) combos...\n",
            "For a dataset of 119 examples we can generate 273819 combinations of 3 examples.\n",
            "Generating 2000 random (and thus possibly overlapping) combos...\n",
            "Number of examples in the train dataset: 98000\n",
            "Number of examples in the test dataset: 2000\n",
            "Max token sequence length: 1300\n",
            "Number of unique characters in the ascii vocabulary: 27\n",
            "Ascii vocabulary:\n",
            "\t\"abcdefghijklmnopqrstuvwxyz \"\n",
            "Split up the dataset into 98000 training examples and 2000 test examples\n",
            "Dataset determined that: vocab_size=570, block_size=1300\n",
            "Number of Transformer parameters: 391808\n",
            "Model #params: 428288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "def get_time_string(fmt='%m%d_%H%M'):\n",
        "    return datetime.now().strftime(fmt)\n",
        "\n",
        "project_name = \"cursivetransformer\"\n",
        "run_tag = \"ablate_cross_attention\"\n",
        "\n",
        "@dataclass\n",
        "class AppConfig:\n",
        "    # system/input/output\n",
        "    input_file: str = 'names.txt'\n",
        "    work_dir: str = 'out'\n",
        "    resume: bool = False\n",
        "    sample_only: bool = False\n",
        "    num_workers: int = 1 # 4\n",
        "    max_steps: int = 30000\n",
        "    device: str = 'cuda'\n",
        "    seed: int = 3407\n",
        "\n",
        "    # sampling\n",
        "    top_k: int = -1\n",
        "\n",
        "    # model configuration\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    ablate_cross_attention: bool = True  # New flag to ablate cross-attention\n",
        "    augment: bool = True\n",
        "    max_seq_length: int = 1300\n",
        "\n",
        "    # optimization\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 3e-3\n",
        "    weight_decay: float = 1e-2\n",
        "\n",
        "    # wandb parameters\n",
        "    wandb_project: str = project_name\n",
        "    wandb_entity: str = 'cursivetransformer'  # Set this to your wandb username or team name\n",
        "    wandb_run_name: str = f\"{get_time_string()}_{run_tag}\"\n",
        "\n",
        "args = AppConfig()\n",
        "\n",
        "# system inits\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "os.makedirs(args.work_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=args.work_dir)\n",
        "\n",
        "# init datasets\n",
        "train_dataset, test_dataset = create_datasets(augment=args.augment, max_seq_length=args.max_seq_length)\n",
        "vocab_size = train_dataset.get_vocab_size()\n",
        "block_size = train_dataset.get_stroke_seq_length()\n",
        "context_block_size = train_dataset.get_text_seq_length()\n",
        "context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "print(f\"Dataset determined that: {vocab_size=}, {block_size=}\")\n",
        "\n",
        "# init model\n",
        "config = ModelConfig(vocab_size=vocab_size,\n",
        "                     block_size=block_size,\n",
        "                     context_block_size=context_block_size,\n",
        "                     context_vocab_size=context_vocab_size,\n",
        "                     n_layer=args.n_layer, n_head=args.n_head,\n",
        "                     n_embd=args.n_embd, n_embd2=args.n_embd2,\n",
        "                     ablate_cross_attention=args.ablate_cross_attention,\n",
        "                     n_ctx_head=args.n_head,)\n",
        "model = Transformer(config)\n",
        "model.to(args.device)\n",
        "print(f\"Model #params: {sum(p.numel() for p in model.parameters())}\")\n",
        "if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n",
        "    print(\"resuming from existing model in the workdir\")\n",
        "    model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n",
        "if args.sample_only:\n",
        "    # save_samples(num=50)\n",
        "    print('This functionality is temporarily commented out')\n",
        "    sys.exit()\n",
        "\n",
        "# init optimizer and batch loader\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
        "batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=args.wandb_project,\n",
        "    entity=args.wandb_entity,\n",
        "    name=args.wandb_run_name,\n",
        "    config=args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "h_C1rMgEACQC",
        "outputId": "24273f70-745b-40a2-bd27-81537aa4ff42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzwimpee\u001b[0m (\u001b[33mcursivetransformer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240803_232417-myhzxxkf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf' target=\"_blank\">0803_2323_ablate_cross_attention</a></strong> to <a href='https://wandb.ai/cursivetransformer/ablate_cross_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cursivetransformer/ablate_cross_attention' target=\"_blank\">https://wandb.ai/cursivetransformer/ablate_cross_attention</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf' target=\"_blank\">https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b83fe1930d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MvHVOusgH_z8"
      },
      "outputs": [],
      "source": [
        "# Splitting this to debug...\n",
        "wandb.config.update({\n",
        "    \"n_layer\": config.n_layer,\n",
        "    \"n_head\": config.n_head,\n",
        "    \"n_embd\": config.n_embd,\n",
        "    \"learning_rate\": args.learning_rate,\n",
        "    \"weight_decay\": args.weight_decay,\n",
        "    \"batch_size\": args.batch_size,\n",
        "    \"ablate_cross_attention\": args.ablate_cross_attention,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_EKkV_W1tZu"
      },
      "source": [
        "## Sample from the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RplUgs12B8xB"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
        "    losses = []\n",
        "    for i, batch in enumerate(loader):\n",
        "        batch = [t.to(args.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "        logits, loss = model(X, C, Y)\n",
        "        losses.append(loss.item())\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "    mean_loss = torch.tensor(losses).mean().item()\n",
        "    model.train() # reset model back to training mode\n",
        "    return mean_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, idx, context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    steps = max(0, max_new_tokens-idx.size(1))\n",
        "    for i in range(steps):\n",
        "        # if the sequence context is growing too long we must crop it at block_size\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "        # forward the model to get the logits for the index in the sequence\n",
        "        logits, _ = model(idx_cond, context)\n",
        "        # pluck the logits at the final step and scale by desired temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop the logits to only the top k options\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        # apply softmax to convert logits to (normalized) probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # either sample from the distribution or take the most likely element\n",
        "        if do_sample:\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "        # append sampled index to the running sequence and continue\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def save_samples(model, dataset, num=2, model_device='cpu', warmup_steps=100, do_sample=False):\n",
        "    \"\"\" samples from the model and plots the decoded strokes \"\"\"\n",
        "    model_device = list(model.parameters())[0].device # hacky\n",
        "\n",
        "    stroke_seq, context = [], []\n",
        "    for i in range(num):\n",
        "      x, c, y = dataset[i]\n",
        "      stroke_seq.append(x) ; context.append(c)\n",
        "\n",
        "    X_init = torch.stack(stroke_seq).to(model_device)[:,:warmup_steps]\n",
        "    context = torch.stack(context).long().to(model_device)\n",
        "    top_k = None\n",
        "    steps = dataset.get_stroke_seq_length() - 1  # -1 because we already start with the first token\n",
        "\n",
        "    X_samp = generate(model, X_init, context, steps, top_k=top_k, do_sample=do_sample).to('cpu')\n",
        "\n",
        "    for i in range(X_samp.size(0)):\n",
        "        # get the i'th row of sampled integers, as python list\n",
        "        row = X_samp[i].detach().cpu().numpy()\n",
        "        offset_samp = dataset.decode_stroke(row)\n",
        "        point_samp = offsets_to_strokes(offset_samp)\n",
        "        decoded_ascii = dataset.decode_text(context[i])\n",
        "\n",
        "        # Plot the stroke\n",
        "        fig, ax = plot_strokes(point_samp, f'Sample {i+1}: \"{decoded_ascii}\"') #plt.axis('off')\n",
        "        tag = 'sample' if do_sample else 'topk'\n",
        "        fig.savefig(f\"{dataset.name}_{tag}_{i+1}.png\")\n",
        "        wandb.log({f\"{dataset.name}_{tag}_{i+1}\": wandb.Image(f\"{dataset.name}_{tag}_{i+1}.png\")})\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved {dataset.name}_{tag}_{i+1}.png\")\n",
        "\n",
        "    print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1ntAoG1vqh"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7otFYXQhr9lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a52d3d6-42a7-43c6-c3f4-a75475c41ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 6.7126 | step time 1659.53ms\n",
            "step 100 | loss 2.7971 | step time 148.65ms\n",
            "step 200 | loss 2.6739 | step time 148.52ms\n",
            "step 300 | loss 2.6174 | step time 148.60ms\n",
            "step 400 | loss 2.6994 | step time 148.65ms\n",
            "step 500 | loss 2.6272 | step time 148.37ms\n",
            "step 600 | loss 2.5939 | step time 148.07ms\n",
            "step 700 | loss 2.5567 | step time 148.76ms\n",
            "step 800 | loss 2.5931 | step time 148.05ms\n",
            "step 900 | loss 2.7124 | step time 148.70ms\n",
            "step 1000 | loss 2.4982 | step time 148.73ms\n",
            "step 1100 | loss 2.5303 | step time 148.72ms\n",
            "step 1200 | loss 2.3772 | step time 148.72ms\n",
            "step 1300 | loss 2.3412 | step time 148.69ms\n",
            "step 1400 | loss 2.2076 | step time 148.68ms\n",
            "step 1500 | loss 2.2654 | step time 148.51ms\n",
            "step 1600 | loss 2.2219 | step time 149.09ms\n",
            "step 1700 | loss 2.1142 | step time 148.66ms\n",
            "step 1800 | loss 2.1784 | step time 148.78ms\n",
            "step 1900 | loss 2.2178 | step time 148.31ms\n",
            "step 2000 | loss 2.2772 | step time 148.66ms\n",
            "step 2000 train loss: 2.2185 test loss: 2.2730\n",
            "Test loss 2.2730 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 2100 | loss 2.2602 | step time 148.72ms\n",
            "step 2200 | loss 2.1607 | step time 148.64ms\n",
            "step 2300 | loss 2.2388 | step time 148.87ms\n",
            "step 2400 | loss 2.1551 | step time 148.71ms\n",
            "step 2500 | loss 2.2044 | step time 148.64ms\n",
            "step 2600 | loss 2.0991 | step time 148.60ms\n",
            "step 2700 | loss 2.2108 | step time 148.36ms\n",
            "step 2800 | loss 2.1575 | step time 148.64ms\n",
            "step 2900 | loss 2.1352 | step time 148.68ms\n",
            "step 3000 | loss 2.1826 | step time 148.58ms\n",
            "step 3100 | loss 2.1879 | step time 148.74ms\n",
            "step 3200 | loss 2.1821 | step time 148.40ms\n",
            "step 3300 | loss 2.1874 | step time 148.94ms\n",
            "step 3400 | loss 2.2136 | step time 149.39ms\n",
            "step 3500 | loss 2.1503 | step time 148.29ms\n",
            "step 3600 | loss 2.1673 | step time 148.53ms\n",
            "step 3700 | loss 2.0510 | step time 148.78ms\n",
            "step 3800 | loss 2.1513 | step time 148.63ms\n",
            "step 3900 | loss 2.0810 | step time 148.76ms\n",
            "step 4000 | loss 2.0586 | step time 148.55ms\n",
            "step 4000 train loss: 2.1505 test loss: 2.2147\n",
            "Test loss 2.2147 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 4100 | loss 1.9932 | step time 148.60ms\n",
            "step 4200 | loss 2.1151 | step time 148.50ms\n",
            "step 4300 | loss 2.1953 | step time 148.65ms\n",
            "step 4400 | loss 2.1110 | step time 148.54ms\n",
            "step 4500 | loss 2.1762 | step time 148.46ms\n",
            "step 4600 | loss 2.1471 | step time 149.57ms\n",
            "step 4700 | loss 2.1307 | step time 148.72ms\n",
            "step 4800 | loss 2.1458 | step time 148.53ms\n",
            "step 4900 | loss 2.0067 | step time 148.36ms\n",
            "step 5000 | loss 2.1833 | step time 148.58ms\n",
            "step 5100 | loss 2.1164 | step time 148.51ms\n",
            "step 5200 | loss 2.0749 | step time 148.91ms\n",
            "step 5300 | loss 2.0614 | step time 148.50ms\n",
            "step 5400 | loss 2.0985 | step time 148.79ms\n",
            "step 5500 | loss 2.2216 | step time 148.82ms\n",
            "step 5600 | loss 2.1086 | step time 148.32ms\n",
            "step 5700 | loss 2.1224 | step time 148.81ms\n",
            "step 5800 | loss 2.1887 | step time 148.94ms\n",
            "step 5900 | loss 2.0728 | step time 148.56ms\n",
            "step 6000 | loss 2.1529 | step time 148.79ms\n",
            "step 6000 train loss: 2.1028 test loss: 2.1456\n",
            "Test loss 2.1456 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 6100 | loss 2.2530 | step time 148.72ms\n",
            "step 6200 | loss 2.0622 | step time 148.52ms\n",
            "step 6300 | loss 2.0310 | step time 148.90ms\n",
            "step 6400 | loss 2.0263 | step time 148.33ms\n",
            "step 6500 | loss 2.0562 | step time 148.79ms\n",
            "step 6600 | loss 2.0633 | step time 149.10ms\n",
            "step 6700 | loss 2.1156 | step time 148.62ms\n",
            "step 6800 | loss 2.2009 | step time 148.75ms\n",
            "step 6900 | loss 2.0974 | step time 148.79ms\n",
            "step 7000 | loss 2.1443 | step time 148.46ms\n",
            "step 7100 | loss 2.1497 | step time 148.98ms\n",
            "step 7200 | loss 2.2074 | step time 148.86ms\n",
            "step 7300 | loss 2.1204 | step time 148.38ms\n",
            "step 7400 | loss 2.1229 | step time 148.22ms\n",
            "step 7500 | loss 2.1510 | step time 148.28ms\n",
            "step 7600 | loss 2.0136 | step time 148.66ms\n",
            "step 7700 | loss 2.0573 | step time 148.85ms\n",
            "step 7800 | loss 2.0504 | step time 148.74ms\n",
            "step 7900 | loss 2.0116 | step time 148.51ms\n",
            "step 8000 | loss 2.0537 | step time 148.38ms\n",
            "step 8000 train loss: 2.0902 test loss: 2.1413\n",
            "Test loss 2.1413 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 8100 | loss 2.1114 | step time 148.68ms\n",
            "step 8200 | loss 2.1356 | step time 148.46ms\n",
            "step 8300 | loss 2.1040 | step time 148.71ms\n",
            "step 8400 | loss 2.1688 | step time 148.62ms\n",
            "step 8500 | loss 2.0837 | step time 148.64ms\n",
            "step 8600 | loss 2.0151 | step time 148.68ms\n",
            "step 8700 | loss 2.0359 | step time 148.60ms\n",
            "step 8800 | loss 2.0183 | step time 148.71ms\n",
            "step 8900 | loss 2.1830 | step time 148.93ms\n",
            "step 9000 | loss 2.0461 | step time 148.71ms\n",
            "step 9100 | loss 2.1266 | step time 148.52ms\n",
            "step 9200 | loss 1.9742 | step time 148.28ms\n",
            "step 9300 | loss 2.1234 | step time 148.64ms\n",
            "step 9400 | loss 2.0588 | step time 148.50ms\n",
            "step 9500 | loss 2.0763 | step time 148.80ms\n",
            "step 9600 | loss 2.1066 | step time 148.75ms\n",
            "step 9700 | loss 2.2399 | step time 148.84ms\n",
            "step 9800 | loss 2.1083 | step time 148.41ms\n",
            "step 9900 | loss 2.0625 | step time 148.42ms\n",
            "step 10000 | loss 2.1725 | step time 148.61ms\n",
            "step 10000 train loss: 2.0740 test loss: 2.1309\n",
            "Test loss 2.1309 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 10100 | loss 2.1783 | step time 148.18ms\n",
            "step 10200 | loss 2.0871 | step time 148.34ms\n",
            "step 10300 | loss 2.1690 | step time 149.01ms\n",
            "step 10400 | loss 2.1786 | step time 148.91ms\n",
            "step 10500 | loss 2.0927 | step time 148.91ms\n",
            "step 10600 | loss 2.0398 | step time 148.50ms\n",
            "step 10700 | loss 2.1416 | step time 148.31ms\n",
            "step 10800 | loss 2.1230 | step time 148.61ms\n",
            "step 10900 | loss 2.0611 | step time 148.77ms\n",
            "step 11000 | loss 2.1717 | step time 148.49ms\n",
            "step 11100 | loss 2.1384 | step time 148.63ms\n",
            "step 11200 | loss 2.1028 | step time 148.78ms\n",
            "step 11300 | loss 2.2316 | step time 148.39ms\n",
            "step 11400 | loss 2.0344 | step time 148.86ms\n",
            "step 11500 | loss 2.1121 | step time 148.39ms\n",
            "step 11600 | loss 2.1046 | step time 148.06ms\n",
            "step 11700 | loss 2.0576 | step time 147.93ms\n",
            "step 11800 | loss 2.0957 | step time 148.49ms\n",
            "step 11900 | loss 2.0752 | step time 149.01ms\n",
            "step 12000 | loss 2.0948 | step time 148.41ms\n",
            "step 12000 train loss: 2.0922 test loss: 2.1353\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 12100 | loss 2.0877 | step time 148.89ms\n",
            "step 12200 | loss 1.9858 | step time 148.46ms\n",
            "step 12300 | loss 2.0007 | step time 148.35ms\n",
            "step 12400 | loss 2.1234 | step time 149.39ms\n",
            "step 12500 | loss 2.1023 | step time 148.11ms\n",
            "step 12600 | loss 2.0657 | step time 148.14ms\n",
            "step 12700 | loss 2.0337 | step time 149.35ms\n",
            "step 12800 | loss 2.0023 | step time 148.39ms\n",
            "step 12900 | loss 2.0946 | step time 148.68ms\n",
            "step 13000 | loss 2.0797 | step time 148.48ms\n",
            "step 13100 | loss 2.0866 | step time 148.56ms\n",
            "step 13200 | loss 2.1054 | step time 148.66ms\n",
            "step 13300 | loss 2.0724 | step time 148.50ms\n",
            "step 13400 | loss 2.1335 | step time 148.59ms\n",
            "step 13500 | loss 2.1654 | step time 148.04ms\n",
            "step 13600 | loss 2.0353 | step time 148.83ms\n",
            "step 13700 | loss 1.9800 | step time 148.79ms\n",
            "step 13800 | loss 1.9976 | step time 148.80ms\n",
            "step 13900 | loss 1.9965 | step time 148.47ms\n",
            "step 14000 | loss 2.0592 | step time 148.39ms\n",
            "step 14000 train loss: 2.0810 test loss: 2.1243\n",
            "Test loss 2.1243 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 14100 | loss 1.9744 | step time 148.67ms\n",
            "step 14200 | loss 2.1498 | step time 148.34ms\n",
            "step 14300 | loss 2.0781 | step time 148.31ms\n",
            "step 14400 | loss 1.9510 | step time 148.64ms\n",
            "step 14500 | loss 2.0662 | step time 148.71ms\n",
            "step 14600 | loss 2.0222 | step time 148.85ms\n",
            "step 14700 | loss 2.0043 | step time 148.63ms\n",
            "step 14800 | loss 2.0023 | step time 148.36ms\n",
            "step 14900 | loss 2.0803 | step time 148.37ms\n",
            "step 15000 | loss 2.0844 | step time 148.34ms\n",
            "step 15100 | loss 2.0859 | step time 148.67ms\n",
            "step 15200 | loss 2.0617 | step time 148.79ms\n",
            "step 15300 | loss 2.0353 | step time 148.21ms\n",
            "step 15400 | loss 2.0640 | step time 148.31ms\n",
            "step 15500 | loss 2.0588 | step time 148.41ms\n",
            "step 15600 | loss 2.0496 | step time 149.00ms\n",
            "step 15700 | loss 2.0002 | step time 148.66ms\n",
            "step 15800 | loss 2.0896 | step time 148.91ms\n",
            "step 15900 | loss 2.0045 | step time 148.73ms\n",
            "step 16000 | loss 2.0231 | step time 148.36ms\n",
            "step 16000 train loss: 2.0681 test loss: 2.1234\n",
            "Test loss 2.1234 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 16100 | loss 2.0402 | step time 148.27ms\n",
            "step 16200 | loss 2.0089 | step time 148.59ms\n",
            "step 16300 | loss 2.1363 | step time 148.55ms\n",
            "step 16400 | loss 2.0841 | step time 148.58ms\n",
            "step 16500 | loss 2.0532 | step time 148.71ms\n",
            "step 16600 | loss 2.0864 | step time 148.31ms\n",
            "step 16700 | loss 2.0039 | step time 148.22ms\n",
            "step 16800 | loss 2.0142 | step time 148.46ms\n",
            "step 16900 | loss 2.0137 | step time 148.81ms\n",
            "step 17000 | loss 2.1670 | step time 148.63ms\n",
            "step 17100 | loss 2.0623 | step time 148.81ms\n",
            "step 17200 | loss 2.1741 | step time 148.58ms\n",
            "step 17300 | loss 2.0115 | step time 148.46ms\n",
            "step 17400 | loss 2.1798 | step time 148.38ms\n",
            "step 17500 | loss 2.0475 | step time 148.92ms\n",
            "step 17600 | loss 2.0790 | step time 148.11ms\n",
            "step 17700 | loss 2.1092 | step time 148.34ms\n",
            "step 17800 | loss 2.1517 | step time 148.42ms\n",
            "step 17900 | loss 1.9621 | step time 148.75ms\n",
            "step 18000 | loss 2.1272 | step time 148.87ms\n",
            "step 18000 train loss: 2.0644 test loss: 2.1068\n",
            "Test loss 2.1068 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 18100 | loss 2.0963 | step time 148.39ms\n",
            "step 18200 | loss 2.0077 | step time 148.62ms\n",
            "step 18300 | loss 2.0124 | step time 148.64ms\n",
            "step 18400 | loss 2.1664 | step time 148.60ms\n",
            "step 18500 | loss 2.1120 | step time 148.29ms\n",
            "step 18600 | loss 2.0891 | step time 148.65ms\n",
            "step 18700 | loss 2.2076 | step time 148.64ms\n",
            "step 18800 | loss 2.0226 | step time 148.99ms\n",
            "step 18900 | loss 2.1496 | step time 148.63ms\n",
            "step 19000 | loss 2.0722 | step time 148.59ms\n",
            "step 19100 | loss 2.1569 | step time 148.63ms\n",
            "step 19200 | loss 1.9978 | step time 148.25ms\n",
            "step 19300 | loss 2.0752 | step time 148.63ms\n",
            "step 19400 | loss 2.0273 | step time 148.68ms\n",
            "step 19500 | loss 1.9577 | step time 148.73ms\n",
            "step 19600 | loss 2.0867 | step time 148.34ms\n",
            "step 19700 | loss 2.0579 | step time 148.23ms\n",
            "step 19800 | loss 2.0521 | step time 148.60ms\n",
            "step 19900 | loss 2.0840 | step time 148.61ms\n",
            "step 20000 | loss 2.0787 | step time 148.46ms\n",
            "step 20000 train loss: 2.0396 test loss: 2.1193\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 20100 | loss 2.0435 | step time 148.51ms\n",
            "step 20200 | loss 2.0075 | step time 148.78ms\n",
            "step 20300 | loss 2.0609 | step time 148.48ms\n",
            "step 20400 | loss 2.0981 | step time 148.52ms\n",
            "step 20500 | loss 2.0062 | step time 148.20ms\n",
            "step 20600 | loss 2.0649 | step time 148.56ms\n",
            "step 20700 | loss 2.0095 | step time 148.68ms\n",
            "step 20800 | loss 2.0022 | step time 148.73ms\n",
            "step 20900 | loss 2.1322 | step time 148.36ms\n",
            "step 21000 | loss 1.9971 | step time 148.77ms\n",
            "step 21100 | loss 2.1232 | step time 148.70ms\n",
            "step 21200 | loss 2.0463 | step time 148.65ms\n",
            "step 21300 | loss 2.0605 | step time 148.47ms\n",
            "step 21400 | loss 1.9652 | step time 148.43ms\n",
            "step 21500 | loss 2.0357 | step time 148.43ms\n",
            "step 21600 | loss 2.1045 | step time 148.48ms\n",
            "step 21700 | loss 2.1086 | step time 148.80ms\n",
            "step 21800 | loss 2.0690 | step time 148.98ms\n",
            "step 21900 | loss 2.0305 | step time 148.54ms\n",
            "step 22000 | loss 1.9624 | step time 148.19ms\n",
            "step 22000 train loss: 2.0357 test loss: 2.1049\n",
            "Test loss 2.1049 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 22100 | loss 2.0070 | step time 148.53ms\n",
            "step 22200 | loss 2.0489 | step time 147.91ms\n",
            "step 22300 | loss 2.0615 | step time 149.13ms\n",
            "step 22400 | loss 2.0617 | step time 148.61ms\n",
            "step 22500 | loss 2.0628 | step time 148.55ms\n",
            "step 22600 | loss 2.0075 | step time 148.72ms\n",
            "step 22700 | loss 2.0653 | step time 148.61ms\n",
            "step 22800 | loss 2.0414 | step time 149.03ms\n",
            "step 22900 | loss 2.0846 | step time 148.71ms\n",
            "step 23000 | loss 2.1277 | step time 148.36ms\n",
            "step 23100 | loss 2.0117 | step time 148.56ms\n",
            "step 23200 | loss 2.0700 | step time 148.92ms\n",
            "step 23300 | loss 2.0506 | step time 148.44ms\n",
            "step 23400 | loss 2.1726 | step time 148.59ms\n",
            "step 23500 | loss 2.1658 | step time 148.14ms\n",
            "step 23600 | loss 2.0974 | step time 148.50ms\n",
            "step 23700 | loss 2.0459 | step time 148.51ms\n",
            "step 23800 | loss 2.0782 | step time 148.30ms\n",
            "step 23900 | loss 2.1042 | step time 149.01ms\n",
            "step 24000 | loss 2.0685 | step time 148.01ms\n",
            "step 24000 train loss: 2.0585 test loss: 2.1012\n",
            "Test loss 2.1012 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 24100 | loss 2.1490 | step time 147.89ms\n",
            "step 24200 | loss 2.0740 | step time 148.03ms\n",
            "step 24300 | loss 2.0169 | step time 148.60ms\n",
            "step 24400 | loss 1.9957 | step time 148.52ms\n",
            "step 24500 | loss 2.0842 | step time 148.35ms\n",
            "step 24600 | loss 2.0343 | step time 148.51ms\n",
            "step 24700 | loss 2.1430 | step time 148.18ms\n",
            "step 24800 | loss 1.9656 | step time 148.70ms\n",
            "step 24900 | loss 2.1098 | step time 148.26ms\n",
            "step 25000 | loss 2.1424 | step time 148.48ms\n",
            "step 25100 | loss 2.0943 | step time 148.05ms\n",
            "step 25200 | loss 1.9894 | step time 148.18ms\n",
            "step 25300 | loss 2.0870 | step time 148.57ms\n",
            "step 25400 | loss 2.0378 | step time 147.97ms\n",
            "step 25500 | loss 2.0979 | step time 148.70ms\n",
            "step 25600 | loss 2.1945 | step time 148.05ms\n",
            "step 25700 | loss 2.1307 | step time 148.18ms\n",
            "step 25800 | loss 2.0192 | step time 148.07ms\n",
            "step 25900 | loss 1.9762 | step time 148.19ms\n",
            "step 26000 | loss 2.1293 | step time 148.64ms\n",
            "step 26000 train loss: 2.0570 test loss: 2.1011\n",
            "Test loss 2.1011 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 26100 | loss 2.0555 | step time 148.28ms\n",
            "step 26200 | loss 2.1021 | step time 148.25ms\n",
            "step 26300 | loss 2.0301 | step time 148.74ms\n",
            "step 26400 | loss 2.0922 | step time 148.92ms\n",
            "step 26500 | loss 1.9822 | step time 148.29ms\n",
            "step 26600 | loss 2.0017 | step time 148.34ms\n",
            "step 26700 | loss 1.9277 | step time 148.14ms\n",
            "step 26800 | loss 2.1123 | step time 148.67ms\n",
            "step 26900 | loss 2.0279 | step time 148.66ms\n",
            "step 27000 | loss 2.1268 | step time 148.51ms\n",
            "step 27100 | loss 2.0895 | step time 148.16ms\n",
            "step 27200 | loss 1.9628 | step time 148.27ms\n",
            "step 27300 | loss 2.0178 | step time 148.76ms\n",
            "step 27400 | loss 2.0498 | step time 148.51ms\n",
            "step 27500 | loss 1.9733 | step time 148.34ms\n",
            "step 27600 | loss 1.9106 | step time 148.38ms\n",
            "step 27700 | loss 2.1009 | step time 147.95ms\n",
            "step 27800 | loss 2.1489 | step time 148.32ms\n",
            "step 27900 | loss 2.0213 | step time 148.73ms\n",
            "step 28000 | loss 2.1272 | step time 148.26ms\n",
            "step 28000 train loss: 2.0273 test loss: 2.0885\n",
            "Test loss 2.0885 is the best so far, saving model to out/model.pt\n",
            "Saved test_sample_1.png\n",
            "Saved test_sample_2.png\n",
            "Saved test_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved test_topk_1.png\n",
            "Saved test_topk_2.png\n",
            "Saved test_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_sample_1.png\n",
            "Saved train_sample_2.png\n",
            "Saved train_sample_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "Saved train_topk_1.png\n",
            "Saved train_topk_2.png\n",
            "Saved train_topk_3.png\n",
            "--------------------------------------------------------------------------------\n",
            "step 28100 | loss 1.8834 | step time 148.53ms\n",
            "step 28200 | loss 1.9725 | step time 148.95ms\n",
            "step 28300 | loss 2.0051 | step time 148.35ms\n",
            "step 28400 | loss 1.9915 | step time 148.11ms\n",
            "step 28500 | loss 2.0649 | step time 147.91ms\n",
            "step 28600 | loss 1.9815 | step time 148.10ms\n",
            "step 28700 | loss 2.0711 | step time 148.96ms\n",
            "step 28800 | loss 2.0300 | step time 148.34ms\n",
            "step 28900 | loss 2.0109 | step time 148.31ms\n",
            "step 29000 | loss 1.9490 | step time 147.93ms\n",
            "step 29100 | loss 2.1109 | step time 149.03ms\n",
            "step 29200 | loss 1.9957 | step time 149.09ms\n",
            "step 29300 | loss 2.0555 | step time 148.69ms\n",
            "step 29400 | loss 2.0566 | step time 148.70ms\n",
            "step 29500 | loss 2.0218 | step time 148.24ms\n",
            "step 29600 | loss 2.0680 | step time 147.87ms\n",
            "step 29700 | loss 2.1279 | step time 147.95ms\n",
            "step 29800 | loss 2.1183 | step time 148.58ms\n",
            "step 29900 | loss 2.0075 | step time 148.71ms\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "best_loss = None\n",
        "step = 0\n",
        "while True:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # get the next batch, ship to device, and unpack it to input and target\n",
        "    batch = batch_loader.next()\n",
        "    batch = [t.to(args.device) for t in batch]\n",
        "    X, C, Y = batch\n",
        "\n",
        "    # feed into the model\n",
        "    logits, loss = model(X, C, Y)\n",
        "\n",
        "    # calculate the gradient, update the weights\n",
        "    model.zero_grad(set_to_none=True) ; loss.backward()\n",
        "    optimizer.step()\n",
        "    wandb.log({\"train_loss_step\": loss.item(), \"step\": step})\n",
        "\n",
        "    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n",
        "    if args.device.startswith('cuda'):\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # logging\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
        "\n",
        "    # evaluate the model\n",
        "    if step > 0 and step % 2000 == 0:\n",
        "        train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
        "        test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n",
        "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss, \"step\": step })\n",
        "        print(f\"step {step} train loss: {train_loss:.4f} test loss: {test_loss:.4f}\")\n",
        "        # save the model to disk if it has improved\n",
        "        if best_loss is None or test_loss < best_loss:\n",
        "            out_path = os.path.join(args.work_dir, \"model.pt\")\n",
        "            print(f\"Test loss {test_loss:.4f} is the best so far, saving model to {out_path}\")\n",
        "            torch.save(model.state_dict(), out_path)\n",
        "            #wandb.save(out_path)\n",
        "            best_loss = test_loss\n",
        "\n",
        "    # sample from the model\n",
        "    if step > 0 and step % 2000 == 0:\n",
        "        save_samples(model, test_dataset, num=3, do_sample=True)\n",
        "        save_samples(model, test_dataset, num=3, do_sample=False)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=True)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=False)\n",
        "    step += 1\n",
        "    # termination conditions\n",
        "    if args.max_steps >= 0 and step >= args.max_steps:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q83i143gIwwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285,
          "referenced_widgets": [
            "2280102a6bfd4b2f94b23241d2a0f49e",
            "f5216a020775433b9ac2c93f6a3760e7",
            "64fd7e3291084d7a9d3954d597450160",
            "77002e4b14f242a1824dabbeb41b27e9",
            "99714faa24cf498eb9262efe85076a5f",
            "f37d1ef460da4c318d0d6747e8cc47f4",
            "b064eb9654d24414b92682ed758309cd",
            "87495a0c060346adbb1ac90cbf2d365d"
          ]
        },
        "outputId": "d51ea5a8-5fa5-456f-b379-a4edff657216"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='3.491 MB of 3.527 MB uploaded\\r'), FloatProgress(value=0.9897111546809501, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2280102a6bfd4b2f94b23241d2a0f49e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▆▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▃▃▂▂▁▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▃▃▃▄▃▂▂▃▂▂▂▂▃▃▃▂▂▂▂▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>29999</td></tr><tr><td>test_loss</td><td>2.08854</td></tr><tr><td>train_loss</td><td>2.0273</td></tr><tr><td>train_loss_step</td><td>2.06806</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">0803_2323_ablate_cross_attention</strong> at: <a href='https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf' target=\"_blank\">https://wandb.ai/cursivetransformer/ablate_cross_attention/runs/myhzxxkf</a><br/> View project at: <a href='https://wandb.ai/cursivetransformer/ablate_cross_attention' target=\"_blank\">https://wandb.ai/cursivetransformer/ablate_cross_attention</a><br/>Synced 5 W&B file(s), 168 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240803_232417-myhzxxkf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtO814TOsWU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "062c3717-e935-4ea1-b353-e44b77384489"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1280x960 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(dpi=200)\n",
        "img = mpimg.imread('test_sample_1.png')\n",
        "plt.imshow(img) ; plt.axis('off') ; plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zach's TODO (In decreasing order of priority)\n",
        "\n",
        "- [ ] Fix wandb bug\n",
        "    - [ ]  **Weights and Biases project/run logging**\n",
        "        - **Problem(s)**\n",
        "            - [ ] The project/run name doesn't appear to be getting logged properly in wandb <i>(hotfix implemented, ready to test)</i>\n",
        "\n",
        "\n",
        "- [x] Write out observations on ablating the cross attention\n",
        "    - [x] On inspection, it would appear that the cross attention is vital to retaining any kind of semantic context in the model outputs. This is really interesting.\n",
        "\n",
        "\n",
        "- [ ] Perform ablation study on the impact of masking the cross attention in a couple of different ways:\n",
        "    - [ ] Causal masking (i.e. standard cross-attention implementation)\n",
        "    - [ ] Causal w/ dropout\n",
        "    - [ ] MLM (i.e. masking without preventing attending to future tokens, a la BERT/T5/FLAN/etc.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- [ ] Write out the math behind what is unique about our approach using proper notation, including the connection to the Anthropic interpretibility (sp?) work. This will focus on the data manifold over which our model is being trained, and how this shows (to our knowledge for the first time) that the original transformer architecture as proposed in the GPT-2 paper, with some minor modifications, can map from a discrete space of tokens to a continuous output space, in this case the cursive and/or printed words passed to the model as input.\n",
        "\n",
        "\n",
        "\n",
        "- [x] Go buy a new Apple Pencil so I can construct my own dataset.\n",
        "\n",
        "\n",
        "\n",
        "- [ ] Figure out how to construct dataset.\n",
        "\n",
        "\n",
        "- [ ] Replicate (or beat 😉) Sam's results\n",
        "\n",
        "\n",
        "- [ ] Hook up neel-danda's hooked transformer library and train a SAE.\n",
        "\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "tDIVNNArIpNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Attention Ablation Study\n",
        "- Zach Wimpee, `2024-08-03`\n",
        "\n",
        "Link to Claude conversation: https://claude.ai/chat/34feb203-9d77-41cc-96ec-d38b2eff18c7"
      ],
      "metadata": {
        "id": "L_ORXmVCEZs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoV4m0tGoedt"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"cursivetransformer.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer.ipynb\n",
        "\n",
        "# Training a Cursive Transformer\n",
        "Sam Greydanus & Zach Wimpee | 2024\n",
        "\"\"\"\n",
        "\n",
        "! pip -q install wandb\n",
        "! wandb login --relogin\n",
        "\n",
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "import os, sys, time, math, argparse, io, copy, json, pdb\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from math import comb\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)\n",
        "\n",
        "\"\"\"## Preprocessing and Tokenization\"\"\"\n",
        "\n",
        "# [Keep all existing preprocessing and tokenization code]\n",
        "\n",
        "\"\"\"## Model\"\"\"\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        # query projections for all heads\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # key, value projections for all heads\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        # calculate query for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # calculate key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "\n",
        "        # cross-attention; (B, nh, T, hs) x (B, nh, hs, T_ctx) -> (B, nh, T, T_ctx)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T_ctx) x (B, nh, T_ctx, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CausalCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Apply causal mask\n",
        "        mask = torch.tril(torch.ones(T, T_ctx)).view(1, 1, T, T_ctx).to(x.device)\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLMCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.mask_prob = config.mlm_mask_prob\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        # Apply MLM masking\n",
        "        mask = torch.bernoulli(torch.full((B, 1, 1, T_ctx), 1 - self.mask_prob)).to(x.device)\n",
        "        k = k * mask\n",
        "        v = v * mask\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CausalDropoutCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = nn.Dropout(config.attention_dropout)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Apply causal mask with dropout\n",
        "        mask = torch.tril(torch.ones(T, T_ctx)).view(1, 1, T, T_ctx).to(x.device)\n",
        "        mask = self.dropout(mask)\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "# [Keep existing CausalSelfAttention, NewGELU, and Block classes]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wce = nn.Embedding(config.context_vocab_size, config.n_embd),\n",
        "            wcpe = nn.Embedding(config.context_block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"Number of Transformer parameters: {:.0f}\".format(n_params))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def forward(self, idx, context, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        context_t = context.size(-1)\n",
        "        context_pos = torch.arange(0, context_t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        context_emb = self.transformer.wce(context)\n",
        "        context_pos_emb = self.transformer.wcpe(context_pos)\n",
        "        c = context_emb + context_pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, c)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = None\n",
        "    context_block_size: int = None\n",
        "    vocab_size: int = None\n",
        "    context_vocab_size: int = None\n",
        "    context_length: int = None\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    n_ctx_head: int = 4\n",
        "    cross_attention_type: str = 'standard'\n",
        "    mlm_mask_prob: float = 0.15\n",
        "    attention_dropout: float = 0.1\n",
        "\n",
        "\"\"\"## Hyperparameters\"\"\"\n",
        "\n",
        "def get_time_string(fmt='%m%d_%H%M'):\n",
        "    return datetime.now().strftime(fmt)\n",
        "\n",
        "project_name = \"cursivetransformer\"\n",
        "run_tag = \"cross_attention_ablation\"\n",
        "\n",
        "@dataclass\n",
        "class AppConfig:\n",
        "    # [Keep existing AppConfig fields]\n",
        "\n",
        "    # New fields for cross-attention ablation study\n",
        "    cross_attention_types: List[str] = ('standard', 'causal', 'mlm', 'causal_dropout')\n",
        "    mlm_mask_prob: float = 0.15\n",
        "    attention_dropout: float = 0.1\n",
        "\n",
        "args = AppConfig()\n",
        "\n",
        "# [Keep existing system inits and dataset creation]\n",
        "\n",
        "def run_experiment(cross_attention_type, num_steps=30000):\n",
        "    config = ModelConfig(\n",
        "        vocab_size=vocab_size,\n",
        "        block_size=block_size,\n",
        "        context_block_size=context_block_size,\n",
        "        context_vocab_size=context_vocab_size,\n",
        "        n_layer=args.n_layer,\n",
        "        n_head=args.n_head,\n",
        "        n_embd=args.n_embd,\n",
        "        n_embd2=args.n_embd2,\n",
        "        cross_attention_type=cross_attention_type,\n",
        "        mlm_mask_prob=args.mlm_mask_prob,\n",
        "        attention_dropout=args.attention_dropout,\n",
        "    )\n",
        "    model = Transformer(config).to(args.device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
        "    batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.wandb_project,\n",
        "        entity=args.wandb_entity,\n",
        "        name=f\"{args.wandb_run_name}_{cross_attention_type}\",\n",
        "        config=args.__dict__,\n",
        "        group=\"cross_attention_ablation\",\n",
        "    )\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for step in range(num_steps):\n",
        "        t0 = time.time()\n",
        "\n",
        "        batch = batch_loader.next()\n",
        "        batch = [t.to(args.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "\n",
        "        logits, loss = model(X, C, Y)\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if args.device.startswith('cuda'):\n",
        "            torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"train_loss_step\": loss.item(),\n",
        "            \"step\": step,\n",
        "            \"step_time_ms\": (t1-t0)*1000\n",
        "        })\n",
        "\n",
        "        if step > 0 and step % 2000 == 0:\n",
        "            train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
        "            test_loss = evaluate(model, test_dataset, batch_size=100, max_batches=10)\n",
        "            wandb.log({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"step\": step\n",
        "            })\n",
        "            print(f\"step {step} train loss: {train_loss:.4f} test loss: {test_loss:.4f}\")\n",
        "\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                torch.save(model.state_dict(), f\"best_model_{cross_attention_type}.pt\")\n",
        "                print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
        "\n",
        "            save_samples(model, test_dataset, num=3, do_sample=True)\n",
        "            save_samples(model, test_dataset, num=3, do_sample=False)\n",
        "\n",
        "    wandb.finish()\n",
        "    return model, best_loss\n",
        "\n",
        "# Main experiment loop\n",
        "results = {}\n",
        "\n",
        "for attention_type in args.cross_attention_types:\n",
        "    print(f\"\\nRunning experiment with {attention_type} cross-attention\")\n",
        "    model, best_loss = run_experiment(attention_type)\n",
        "    results[attention_type] = {\n",
        "        'best_test_loss': best_loss,\n",
        "        'final_train_loss': evaluate(model, train_dataset, batch_size=100, max_batches=10),\n",
        "        'final_test_loss': evaluate(model, test_dataset, batch_size=100, max_batches=10)\n",
        "    }\n",
        "\n",
        "# Print and log final results\n",
        "print(\"\\nExperiment Results:\")\n",
        "for attention_type, metrics in results.items():\n",
        "    print(f\"{attention_type.capitalize()} Cross-Attention:\")\n",
        "    print(f\"  Best Test Loss: {metrics['best_test_loss']:.4f}\")\n",
        "    print(f\"  Final Train Loss: {metrics['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Test Loss: {metrics['final_test_loss']:.4f}\")\n",
        "\n",
        "wandb.init(\n",
        "    project=args.wandb_project,\n",
        "    entity=args.wandb_entity,\n",
        "    name=f\"{args.wandb_run_name}_summary\",\n",
        "    job_type=\"summary\"\n",
        ")\n",
        "wandb.log(results)\n",
        "wandb.finish()\n",
        "\n",
        "# Save final results to a JSON file\n",
        "with open('ablation_study_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Ablation study complete. Results saved to 'ablation_study_results.json'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2280102a6bfd4b2f94b23241d2a0f49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5216a020775433b9ac2c93f6a3760e7",
              "IPY_MODEL_64fd7e3291084d7a9d3954d597450160"
            ],
            "layout": "IPY_MODEL_77002e4b14f242a1824dabbeb41b27e9"
          }
        },
        "f5216a020775433b9ac2c93f6a3760e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99714faa24cf498eb9262efe85076a5f",
            "placeholder": "​",
            "style": "IPY_MODEL_f37d1ef460da4c318d0d6747e8cc47f4",
            "value": "3.527 MB of 3.527 MB uploaded\r"
          }
        },
        "64fd7e3291084d7a9d3954d597450160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b064eb9654d24414b92682ed758309cd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87495a0c060346adbb1ac90cbf2d365d",
            "value": 1
          }
        },
        "77002e4b14f242a1824dabbeb41b27e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99714faa24cf498eb9262efe85076a5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37d1ef460da4c318d0d6747e8cc47f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b064eb9654d24414b92682ed758309cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87495a0c060346adbb1ac90cbf2d365d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}