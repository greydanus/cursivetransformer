{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Cursive Transformer\n",
        "Sam Greydanus & Zach Wimpee | 2024"
      ],
      "metadata": {
        "id": "vij3QzRGOWtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO (In decreasing order of priority)"
      ],
      "metadata": {
        "id": "H3n-xaj-uK5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. [ ] Rerun cross-attention ablation study\n",
        "\n",
        "\n",
        "2. [ ] Write out the math behind what is unique about our approach using proper notation, including the connection to the Anthropic interpretibility (sp?) work. This will focus on the data manifold over which our model is being trained, and how this shows (to our knowledge for the first time) that the original transformer architecture as proposed in the GPT-2 paper, with some minor modifications, can map from a discrete space of tokens to a continuous output space, in this case the cursive and/or printed words passed to the model as input.\n",
        "    - [ ] Read Alex Graves paper [Generating Cursive Handwriting with Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. [ ] Construct a new dataset\n",
        "\n",
        "\n",
        "\n",
        "4. [ ] Replicate (or beat ðŸ˜‰) Sam\"s results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. [ ] Hook up neel-danda\"s hooked transformer library and train a SAE.\n",
        "\n",
        "\n",
        "6. [ ] Integrate with `llm_lab` / `Artisight-LLM-Lab` for simplified distributed pipeline training and inference\n",
        "    - [x] Create model card (DONE: https://huggingface.co/zwimpee/cursivetransformer)\n",
        "    - [ ] Upload model and dataset (the latter being optional) to huggingface (ensuring to keep everything locked behind private git repos)\n",
        "    - [ ] Create json config for cursivetransformer in `llm_lab`\n",
        "    - [ ] Update model (and dataset, if applicable) cards"
      ],
      "metadata": {
        "id": "D9PxyoIaSzUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "du8-ZtPKTO61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install wandb\n",
        "! wandb login # --relogin"
      ],
      "metadata": {
        "id": "rEopVbtiOWlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132c3116-8231-43d6-be41-92cc967a4c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzwimpee\u001b[0m (\u001b[33mcursivetransformer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0WRHSM_Kksp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc90b915-7425-4f46-8c6f-ba247df1bbe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "import os, sys, time, math, argparse, io, copy, json, pdb\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Any, AnyStr, List, Dict\n",
        "from math import comb\n",
        "\n",
        "import wandb\n",
        "from wandb.errors import CommError\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(\"Using:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configs"
      ],
      "metadata": {
        "id": "DXCMOHU7vUE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_string(fmt=\"%m%d_%H%M\"):\n",
        "    return datetime.now().strftime(fmt)\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    experiment_type: str = \"pretraining\"\n",
        "    wandb_project: str = \"cursivetransformer\"\n",
        "    wandb_entity: str = \"cursivetransformer\"\n",
        "    wandb_run_name: str = field(init=False)\n",
        "    work_dir: str = \"out\"\n",
        "    resume: bool = False\n",
        "    sample_only: bool = False\n",
        "    num_workers: int = 1\n",
        "    max_steps: int = 50000\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 42069\n",
        "    top_k: int = -1\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    augment: bool = True\n",
        "    max_seq_length: int = 1500\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 1e-2\n",
        "    weight_decay: float = 1e-5\n",
        "    cross_attention_types: List[str] = field(default_factory=lambda: [\"standard\", \"causal\"])\n",
        "    cross_attention_type: str = \"standard\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.experiment_type == \"cross_attention_ablation\":\n",
        "            self.wandb_run_name = f\"{get_time_string()}_{self.experiment_type}_{self.cross_attention_type}\"\n",
        "        else:\n",
        "            self.wandb_run_name = f\"{get_time_string()}_{self.experiment_type}\"\n",
        "\n",
        "    def __json__(self):\n",
        "        return {\n",
        "            k: str(v)\n",
        "            if isinstance(v, torch.device) else v\n",
        "            for k, v in asdict(self).items() if not k.startswith('_')\n",
        "        }\n",
        "\n",
        "    def update(self, new_config: Dict[str, Any]) -> 'ExperimentConfig':\n",
        "        for key, value in new_config.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "        return self\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    block_size: int = field(init=False)\n",
        "    context_block_size: int = field(init=False)\n",
        "    vocab_size: int = field(init=False)\n",
        "    context_vocab_size: int = field(init=False)\n",
        "    context_length: int = field(init=False)\n",
        "    train_dataset: Any = field(init=False)\n",
        "    test_dataset: Any = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.train_dataset, self.test_dataset = create_datasets(augment=True, max_seq_length=1500)\n",
        "        self.vocab_size = self.train_dataset.get_vocab_size()\n",
        "        self.block_size = self.train_dataset.get_stroke_seq_length()\n",
        "        self.context_block_size = self.train_dataset.get_text_seq_length()\n",
        "        self.context_vocab_size = self.train_dataset.get_char_vocab_size()\n",
        "        self.context_length = self.train_dataset.get_text_seq_length()\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = None\n",
        "    context_block_size: int = None\n",
        "    vocab_size: int = None\n",
        "    context_vocab_size: int = None\n",
        "    context_length: int = None\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    n_ctx_head: int = 4\n",
        "    cross_attention_type: str = \"standard\"  # Options: \"standard\", \"causal\""
      ],
      "metadata": {
        "id": "BP7P5d9gvT5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "oFQZUZUkvacC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experiment_config() -> ExperimentConfig:\n",
        "    return ExperimentConfig()\n",
        "\n",
        "def get_data_config() -> DataConfig:\n",
        "    return DataConfig()\n",
        "\n",
        "def get_latest_checkpoint(exp_config):\n",
        "    api = wandb.Api()\n",
        "    runs = api.runs(\n",
        "      f\"{exp_config.wandb_entity}/{exp_config.wandb_project}\",\n",
        "        {\n",
        "          \"$and\": [\n",
        "            {\"config.experiment_type\": exp_config.experiment_type},\n",
        "            {\"config.experiment_params\": exp_config},\n",
        "            {\"state\": {\"$in\": [\"running\", \"finished\"]}}\n",
        "          ]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if not runs:\n",
        "        return None, None\n",
        "\n",
        "    latest_run = max(runs, key=lambda run: run.created_at)\n",
        "    try:\n",
        "        artifacts = latest_run.logged_artifacts()\n",
        "        checkpoints = [artifact for artifact in artifacts if artifact.type == \"model-checkpoint\"]\n",
        "        if not checkpoints:\n",
        "            return latest_run, None\n",
        "        latest_checkpoint = max(checkpoints, key=lambda c: c.version)\n",
        "        return latest_run, latest_checkpoint\n",
        "    except CommError:\n",
        "        return latest_run, None\n",
        "\n",
        "def run_experiment(exp_config: ExperimentConfig, data_config: DataConfig):\n",
        "    # Check for existing run and checkpoint\n",
        "    existing_run, checkpoint = get_latest_checkpoint(exp_config)\n",
        "\n",
        "    if existing_run and existing_run.state == \"finished\":\n",
        "        print(f\"Experiment {exp_config.experiment_type} with params {exp_config} has already been completed.\")\n",
        "        return None, existing_run.summary.get(\"best_loss\", float(\"inf\"))\n",
        "\n",
        "    # Set up the experiment based on the experiment type\n",
        "    if exp_config.experiment_type == \"pretraining\":\n",
        "        config = ModelConfig(\n",
        "            vocab_size=data_config.vocab_size,\n",
        "            block_size=data_config.block_size,\n",
        "            context_block_size=data_config.context_block_size,\n",
        "            context_vocab_size=data_config.context_vocab_size,\n",
        "            n_layer=exp_config.n_layer,\n",
        "            n_head=exp_config.n_head,\n",
        "            n_embd=exp_config.n_embd,\n",
        "            n_embd2=exp_config.n_embd2\n",
        "        )\n",
        "    elif exp_config.experiment_type == \"cross_attention_ablation\":\n",
        "        config = ModelConfig(\n",
        "            vocab_size=data_config.vocab_size,\n",
        "            block_size=data_config.block_size,\n",
        "            context_block_size=data_config.context_block_size,\n",
        "            context_vocab_size=data_config.context_vocab_size,\n",
        "            n_layer=exp_config.n_layer,\n",
        "            n_head=exp_config.n_head,\n",
        "            n_embd=exp_config.n_embd,\n",
        "            n_embd2=exp_config.n_embd2,\n",
        "            cross_attention_type=exp_config.cross_attention_type\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown experiment type: {exp_config.experiment_type}\")\n",
        "\n",
        "    model = Transformer(config).to(exp_config.device)\n",
        "\n",
        "    # TODO: Make optimization configurable\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=exp_config.learning_rate, weight_decay=exp_config.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
        "\n",
        "    start_step = 0\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    if existing_run and exp_config.resume:\n",
        "        run = wandb.init(id=existing_run.id, resume=\"must\")\n",
        "        print(f\"Resuming run {run.name}\")\n",
        "        if checkpoint:\n",
        "            checkpoint_dir = checkpoint.download()\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, \"model_checkpoint.pt\")\n",
        "            checkpoint_data = torch.load(checkpoint_path)\n",
        "            model.load_state_dict(checkpoint_data[\"model_state_dict\"])\n",
        "            optimizer.load_state_dict(checkpoint_data[\"optimizer_state_dict\"])\n",
        "            start_step = checkpoint_data[\"step\"]\n",
        "            best_loss = checkpoint_data[\"best_loss\"]\n",
        "            print(f\"Loaded checkpoint from step {start_step}\")\n",
        "    else:\n",
        "        run = wandb.init(\n",
        "            project=exp_config.wandb_project,\n",
        "            entity=exp_config.wandb_entity,\n",
        "            name=exp_config.wandb_run_name,\n",
        "            config={**exp_config.__dict__, \"experiment_type\": exp_config.experiment_type},\n",
        "            group=exp_config.experiment_type,\n",
        "        )\n",
        "\n",
        "    batch_loader = InfiniteDataLoader(data_config.train_dataset, batch_size=exp_config.batch_size, pin_memory=True, num_workers=exp_config.num_workers)\n",
        "\n",
        "    for step in range(start_step, exp_config.max_steps):\n",
        "        t0 = time.time()\n",
        "\n",
        "        batch = batch_loader.next()\n",
        "        batch = [t.to(exp_config.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "\n",
        "        logits, loss = model(X, C, Y)\n",
        "\n",
        "        # TODO: Implement gradient accumulation.\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if exp_config.device.startswith(\"cuda\"):\n",
        "            torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"train_loss_step\": loss.item(),\n",
        "            \"step\": step,\n",
        "            \"step_time_ms\": (t1-t0)*1000\n",
        "        })\n",
        "\n",
        "        if step > 0 and step % 2000 == 0:\n",
        "            # TODO: Add train and evaluation batch size to experiment config\n",
        "            train_loss = evaluate(model, exp_config, data_config.train_dataset, batch_size=100, max_batches=10)\n",
        "            test_loss = evaluate(model, exp_config, data_config.test_dataset, batch_size=100, max_batches=10)\n",
        "            wandb.log({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"step\": step\n",
        "            })\n",
        "            print(f\"step {step} train loss: {train_loss:.4f} test loss: {test_loss:.4f}\")\n",
        "\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                checkpoint_path = f\"best_model_{exp_config.wandb_run_name}.pt\"\n",
        "                torch.save({\n",
        "                    \"step\": step,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": loss.item(),\n",
        "                    \"best_loss\": best_loss,\n",
        "                    },\n",
        "                    checkpoint_path\n",
        "                )\n",
        "                artifact = wandb.Artifact(f\"model-checkpoint-{step}\", type=\"model-checkpoint\")\n",
        "                artifact.add_file(checkpoint_path)\n",
        "                run.log_artifact(artifact)\n",
        "                print(f\"New best model saved with test loss: {best_loss:.4f}\")\n",
        "\n",
        "            save_samples(model, data_config.test_dataset, num=3, do_sample=True)\n",
        "            save_samples(model, data_config.test_dataset, num=3, do_sample=False)\n",
        "\n",
        "    wandb.finish()\n",
        "    return model, best_loss\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model, exp_config, dataset, batch_size=50, max_batches=None):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
        "    losses = []\n",
        "    for i, batch in enumerate(loader):\n",
        "        batch = [t.to(exp_config.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "        logits, loss = model(X, C, Y)\n",
        "        losses.append(loss.item())\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "    mean_loss = torch.tensor(losses).mean().item()\n",
        "    model.train() # reset model back to training mode\n",
        "    return mean_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, idx, context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "    Most likely you\"ll want to make sure to be in model.eval() mode of operation for this.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    steps = max(0, max_new_tokens-idx.size(1))\n",
        "    for i in range(steps):\n",
        "        # if the sequence context is growing too long we must crop it at block_size\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "        # forward the model to get the logits for the index in the sequence\n",
        "        logits, _ = model(idx_cond, context)\n",
        "        # pluck the logits at the final step and scale by desired temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop the logits to only the top k options\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
        "        # apply softmax to convert logits to (normalized) probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # either sample from the distribution or take the most likely element\n",
        "        if do_sample:\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "        # append sampled index to the running sequence and continue\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def save_samples(model, dataset, num=2, model_device=\"cpu\", warmup_steps=100, do_sample=False):\n",
        "    \"\"\" samples from the model and plots the decoded strokes \"\"\"\n",
        "    model_device = list(model.parameters())[0].device # hacky\n",
        "\n",
        "    stroke_seq, context = [], []\n",
        "    for i in range(num):\n",
        "      x, c, y = dataset[i]\n",
        "      stroke_seq.append(x) ; context.append(c)\n",
        "\n",
        "    X_init = torch.stack(stroke_seq).to(model_device)[:,:warmup_steps]\n",
        "    context = torch.stack(context).long().to(model_device)\n",
        "    top_k = None\n",
        "    steps = dataset.get_stroke_seq_length() - 1  # -1 because we already start with the first token\n",
        "\n",
        "    X_samp = generate(model, X_init, context, steps, top_k=top_k, do_sample=do_sample).to(\"cpu\")\n",
        "\n",
        "    for i in range(X_samp.size(0)):\n",
        "        # get the i\"th row of sampled integers, as python list\n",
        "        row = X_samp[i].detach().cpu().numpy()\n",
        "        offset_samp = dataset.decode_stroke(row)\n",
        "        point_samp = offsets_to_strokes(offset_samp)\n",
        "        decoded_ascii = dataset.decode_text(context[i])\n",
        "\n",
        "        # Plot the stroke\n",
        "        fig, ax = plot_strokes(point_samp, f\"Sample {i+1}: '{decoded_ascii}'\") #plt.axis(\"off\")\n",
        "        tag = \"sample\" if do_sample else \"topk\"\n",
        "        fig.savefig(f\"{dataset.name}_{tag}_{i+1}.png\")\n",
        "        wandb.log({f\"{dataset.name}_{tag}_{i+1}\": wandb.Image(f\"{dataset.name}_{tag}_{i+1}.png\")})\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved {dataset.name}_{tag}_{i+1}.png\")\n",
        "\n",
        "    print(\"-\"*80)\n",
        "\n",
        "def plot_strokes(stroke, title, fig=None, ax=None):\n",
        "    \"\"\"Plot a single stroke\"\"\"\n",
        "    if fig is None or ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 2))\n",
        "\n",
        "    # Separate strokes based on pen lifts\n",
        "    strokes = []\n",
        "    current_stroke = []\n",
        "    for point in stroke:\n",
        "        if point[2] == 1:  # Pen is down\n",
        "            current_stroke.append(point)\n",
        "        else:  # Pen is up\n",
        "            if current_stroke:\n",
        "                strokes.append(current_stroke)\n",
        "                current_stroke = []\n",
        "    if current_stroke:\n",
        "        strokes.append(current_stroke)\n",
        "\n",
        "    # Plot each stroke\n",
        "    for stroke in strokes:\n",
        "        x, y = zip(*[(p[0], 1 - p[1]) for p in stroke])  # Invert y-axis\n",
        "        ax.plot(x, y, \"b-\")\n",
        "\n",
        "    ax.set_aspect(\"equal\") ; ax.set_title(title)\n",
        "\n",
        "    if fig is None:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def load_and_parse_data(min_ascii_length=3):\n",
        "    uploaded = files.upload()\n",
        "    file_content = next(iter(uploaded.values()))\n",
        "    data = json.loads(file_content.decode(\"utf-8\"))\n",
        "    for i in range(len(data)):\n",
        "      strokes = np.array(data[i][\"points\"])\n",
        "      strokes[:,0:1] *= data[i][\"metadata\"][\"aspectRatio\"]\n",
        "      strokes[:, 0] -= strokes[0, 0]\n",
        "      data[i][\"points\"] = strokes\n",
        "    data = [d for d in data if len(d[\"metadata\"][\"asciiSequence\"]) >= min_ascii_length]\n",
        "    return data\n",
        "\n",
        "def decompose_offsets(offsets):\n",
        "    dx, dy = offsets[:, 0], offsets[:, 1]\n",
        "    r = np.hypot(dx, dy)\n",
        "    theta = np.arctan2(dy, dx)\n",
        "    return np.column_stack((theta, r, offsets[:, 2]))\n",
        "\n",
        "def reconstruct_offsets(polar_data):\n",
        "    theta, r = polar_data[:, 0], polar_data[:, 1]\n",
        "    dx = r * np.cos(theta)\n",
        "    dy = r * np.sin(theta)\n",
        "    return np.column_stack((dx, dy, polar_data[:, 2]))\n",
        "\n",
        "def strokes_to_offsets(points):\n",
        "    # Calculate differences (dx, dy), not considering pen_down\n",
        "    offsets = np.zeros_like(points)\n",
        "    offsets[1:, 0:2] = np.diff(points[:, 0:2], axis=0)  # Compute dx, dy\n",
        "    offsets[:, 2] = points[:, 2]  # Copy pen_down directly\n",
        "\n",
        "    # Decouple direction from magnitude (this will help with tokenization)\n",
        "    offsets_dec = decompose_offsets(offsets)\n",
        "    return offsets_dec\n",
        "\n",
        "def offsets_to_strokes(offsets_dec):\n",
        "    # Calculate cumulative sums to get absolute positions\n",
        "    offsets = reconstruct_offsets(offsets_dec)\n",
        "    absolute_coords = np.cumsum(offsets[:, :2], axis=0)\n",
        "    stroke_data = np.hstack((absolute_coords, offsets[:, 2:3]))\n",
        "    return stroke_data\n",
        "\n",
        "def combine_handwriting_examples(examples, space_width=0.17):\n",
        "    assert len(set(ex[\"metadata\"][\"author\"] for ex in examples)) == 1, \"All examples must have the same author\"\n",
        "\n",
        "    combined_metadata = {\n",
        "        \"author\": examples[0][\"metadata\"][\"author\"],\n",
        "        \"asciiSequence\": \" \".join(ex[\"metadata\"][\"asciiSequence\"] for ex in examples),\n",
        "        \"pointCount\": sum(ex[\"metadata\"][\"pointCount\"] for ex in examples),\n",
        "        \"strokeCount\": sum(ex[\"metadata\"][\"strokeCount\"] for ex in examples),\n",
        "        \"aspectRatio\": examples[0][\"metadata\"][\"aspectRatio\"]\n",
        "    }\n",
        "\n",
        "    combined_points, current_x_offset, total_width = [], 0, 0\n",
        "\n",
        "    for i, example in enumerate(examples):\n",
        "        points = example[\"points\"]\n",
        "        word_width = np.max(points[:, 0]) - np.min(points[:, 0])\n",
        "        total_width += word_width\n",
        "\n",
        "        normalized_points = points.copy()\n",
        "        normalized_points[:, 0] -= np.min(points[:, 0])\n",
        "        normalized_points[:, 0] += current_x_offset\n",
        "\n",
        "        combined_points.append(normalized_points)\n",
        "        current_x_offset += word_width\n",
        "\n",
        "        if i < len(examples) - 1:\n",
        "            combined_points.append(np.array([[current_x_offset + space_width, normalized_points[-1, 1], 0]]))\n",
        "            current_x_offset += space_width\n",
        "            total_width += space_width\n",
        "            combined_metadata[\"pointCount\"] += 1\n",
        "\n",
        "    combined_points = np.vstack(combined_points)\n",
        "    return {\"metadata\": combined_metadata, \"points\": combined_points}\n",
        "\n",
        "def rotate_points(points, max_angle=10):\n",
        "    angle = np.deg2rad(np.random.uniform(-max_angle, max_angle))\n",
        "    cos_angle, sin_angle = np.cos(angle), np.sin(angle)\n",
        "\n",
        "    x, y = points[:, 0], points[:, 1]\n",
        "    x_rot = x * cos_angle - y * sin_angle\n",
        "    y_rot = x * sin_angle + y * cos_angle\n",
        "\n",
        "    points[:, 0], points[:, 1] = x_rot, y_rot\n",
        "    return points\n",
        "\n",
        "def shear_points(points, shear_range=(-0.4, 0.4)):\n",
        "    shear_factor = np.random.uniform(*shear_range)\n",
        "    x, y = points[:, 0], points[:, 1]\n",
        "    x_sheared = x + shear_factor * y\n",
        "    points[:, 0] = x_sheared\n",
        "    return points\n",
        "\n",
        "def generate_word_combos(raw_json, desired_num_combos=10000, num_words=3, max_angle=4, shear_range=(-0.4, 0.4)):\n",
        "  num_combos = comb(len(raw_json), num_words)\n",
        "  print(f\"For a dataset of {len(raw_json)} examples we can generate {num_combos} combinations of {num_words} examples.\")\n",
        "  print(f\"Generating {desired_num_combos} random (and thus possibly overlapping) combos...\")\n",
        "  combo_json = []\n",
        "  for i in range(desired_num_combos):\n",
        "    ixs = np.random.choice(len(raw_json), size=num_words, replace=False)\n",
        "    words_to_merge = [raw_json[i] for i in ixs]\n",
        "    example = combine_handwriting_examples(words_to_merge)\n",
        "    example[\"points\"] = shear_points(example[\"points\"], shear_range)\n",
        "    example[\"points\"] = rotate_points(example[\"points\"], max_angle)\n",
        "    combo_json.append( example )\n",
        "  return combo_json\n",
        "\n",
        "def load_and_combine_examples(desired_num_combos=10000, num_words=3):\n",
        "  data = load_and_parse_data()\n",
        "  return generate_word_combos(data, desired_num_combos, num_words)\n",
        "\n",
        "def remove_random_points(stroke, remove_percentage=0.04):\n",
        "    num_points = np.random.randint(len(stroke))\n",
        "    num_remove = int(num_points * remove_percentage)\n",
        "    indices = np.random.choice(range(1, num_points - 1), num_remove, replace=False).astype(np.int32)\n",
        "    return np.delete(stroke, indices, axis=0)\n",
        "\n",
        "def efficient_downsample(stroke, fraction=0.65):\n",
        "    n = len(stroke)\n",
        "    keep = int(n * fraction)\n",
        "    drop_indices = np.random.choice(n, n - keep, replace=False)\n",
        "    mask = np.ones(n, dtype=bool)\n",
        "    mask[drop_indices] = False\n",
        "    pen_up_mask = stroke[:, -1] == 0\n",
        "    mask |= pen_up_mask\n",
        "    return stroke[mask]\n",
        "\n",
        "\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, strokes, texts, chars, max_seq_length=1100, max_text_length=50, name=\"\", augment=False):\n",
        "        self.name = name\n",
        "        self.strokes = strokes  # List of Nx4 arrays, each representing a cursive sentence\n",
        "        self.texts = texts  # List of corresponding text strings\n",
        "        self.chars = chars  # String of all possible characters\n",
        "        self.augment = augment\n",
        "\n",
        "        self.theta_bins = np.linspace(-np.pi, np.pi, 226)  # 100 bins for theta\n",
        "\n",
        "        # Modify mag_bins to incorporate pen_down information\n",
        "        mag_bins_pen_down = np.concatenate([\n",
        "            np.asarray([0]),\n",
        "            np.linspace(0.005, 0.050, 50),  # Close around 0.01, 30 bins\n",
        "            np.geomspace(0.051, 4, 121)[:-1]  # 150 exponential bins\n",
        "        ])\n",
        "        mag_bins_pen_up = mag_bins_pen_down + max(mag_bins_pen_down) + 1  # Offset for pen-up states\n",
        "        self.mag_bins = np.concatenate([mag_bins_pen_down, mag_bins_pen_up])\n",
        "\n",
        "        # self.feature_sizes = [len(self.dx_bins), len(self.dy_bins), len(self.mag_bins)]\n",
        "        self.feature_sizes = [len(self.theta_bins), len(self.mag_bins)]\n",
        "        self.cumulative_sizes = np.cumsum([0] + self.feature_sizes)\n",
        "\n",
        "        # Add special tokens for strokes\n",
        "        self.PAD_TOKEN = sum(self.feature_sizes)\n",
        "        self.END_TOKEN = sum(self.feature_sizes) + 1\n",
        "\n",
        "        # Character tokenization\n",
        "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:s for s,i in self.stoi.items()}\n",
        "        self.char_PAD_TOKEN = 0\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def augment_stroke(self, stroke):\n",
        "\n",
        "        # # stroke = remove_random_points(stroke, remove_percentage=0.03) # Drop some points\n",
        "        # stroke = horizontal_shear(stroke, shear_range=(-0.3, 0.3)) # Horizontal shear\n",
        "\n",
        "        stroke[:, 1:2] = stroke[:, 1:2] * np.random.uniform(0.8, 1.2)\n",
        "\n",
        "        noise = np.random.normal(0, 0.002, stroke[:, 1:2].shape) # Random noise\n",
        "        stroke[:, 1:2] += noise\n",
        "\n",
        "        # stroke = rotate_polar_stroke(stroke, max_angle=30)\n",
        "\n",
        "        # Random starting point\n",
        "        stroke = stroke[np.random.randint(1, 8):-np.random.randint(1, 8)]\n",
        "\n",
        "        # Downsample stroke\n",
        "        stroke = efficient_downsample(stroke, .4)\n",
        "        return stroke\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return sum(self.feature_sizes) + 2  # +2 for PAD and END tokens\n",
        "\n",
        "    def get_char_vocab_size(self):\n",
        "        return len(self.chars) + 1  # +1 for PAD token\n",
        "\n",
        "    def get_stroke_seq_length(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "    def get_text_seq_length(self):\n",
        "        return self.max_text_length\n",
        "\n",
        "    def encode_stroke(self, stroke):\n",
        "        theta_idx = np.digitize(stroke[:, 0], self.theta_bins) - 1\n",
        "\n",
        "        # Encode magnitude and pen state together\n",
        "        mag_idx = np.digitize(stroke[:, 1], self.mag_bins[:len(self.mag_bins)//2]) - 1\n",
        "        mag_idx[stroke[:, 2] == 0] += len(self.mag_bins) // 2  # Offset for pen-up states\n",
        "\n",
        "        encoded = np.column_stack([\n",
        "            theta_idx + self.cumulative_sizes[0],\n",
        "            mag_idx + self.cumulative_sizes[1]\n",
        "        ])\n",
        "        return encoded.flatten()\n",
        "\n",
        "    def decode_stroke(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "\n",
        "        # Remove PAD and END tokens\n",
        "        ix = ix[(ix != self.PAD_TOKEN) & (ix != self.END_TOKEN)]\n",
        "\n",
        "        # Reshape the flattened array back to Nx2\n",
        "        ix = ix[:(len(ix)//2)*2]\n",
        "        ix = ix.reshape(-1, 2)\n",
        "        theta = self.theta_bins[(ix[:, 0] - self.cumulative_sizes[0]).clip(0, len(self.theta_bins)-1)]\n",
        "\n",
        "        mag_idx = ix[:, 1] - self.cumulative_sizes[1]\n",
        "        pen = (mag_idx < len(self.mag_bins) // 2).astype(int)\n",
        "        mag_idx[pen == 0] -= len(self.mag_bins) // 2\n",
        "        mag = self.mag_bins[:len(self.mag_bins)//2][mag_idx.clip(0, len(self.mag_bins)//2 - 1)]\n",
        "\n",
        "        return np.column_stack([theta, mag, pen])\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return torch.tensor([self.stoi.get(ch, self.char_PAD_TOKEN) for ch in text], dtype=torch.long)\n",
        "\n",
        "    def decode_text(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "        return \"\".join([self.itos.get(i, \"\") for i in ix if i != self.char_PAD_TOKEN])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stroke = self.strokes[idx]\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            stroke = self.augment_stroke(stroke.copy())\n",
        "\n",
        "        # Encode stroke\n",
        "        stroke_offsets = self.strokes_to_polar_offsets(stroke)\n",
        "        encoded_stroke = self.encode_stroke(stroke_offsets)\n",
        "        x = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "        y = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "\n",
        "        seq_len = min(len(encoded_stroke), self.max_seq_length - 1)  # -1 to leave room for END token\n",
        "        x[:seq_len] = torch.tensor(encoded_stroke[:seq_len], dtype=torch.long)\n",
        "        x[seq_len] = self.END_TOKEN\n",
        "\n",
        "        y[:seq_len] = x[1:seq_len+1]\n",
        "        y[seq_len] = self.END_TOKEN\n",
        "\n",
        "        # Encode text (context) and pad to max_text_length of 30\n",
        "        encoded_text = self.encode_text(text)\n",
        "        c = torch.full((self.max_text_length,), self.char_PAD_TOKEN, dtype=torch.long)\n",
        "        text_len = min(len(encoded_text), self.max_text_length)\n",
        "        c[:text_len] = encoded_text[:text_len]\n",
        "        return x, c, y\n",
        "\n",
        "    @staticmethod\n",
        "    def strokes_to_polar_offsets(stroke):\n",
        "        offsets = np.diff(stroke[:, :2], axis=0)\n",
        "        theta = np.arctan2(offsets[:, 1], offsets[:, 0])\n",
        "        r = np.hypot(offsets[:, 0], offsets[:, 1])\n",
        "        pen_state = stroke[1:, 2]  # pen state corresponds to the end point of each offset\n",
        "        return np.column_stack((theta, r, pen_state))\n",
        "\n",
        "\n",
        "def create_datasets(augment=True, max_seq_length=1100, num_words=3):\n",
        "  raw_json = load_and_parse_data()\n",
        "\n",
        "  # partition the input data into a training and the test set\n",
        "  test_set_size = min(1000, int(len(raw_json) * 0.10)) # 10% of the training set, or up to 1000 examples\n",
        "  rp = torch.randperm(len(raw_json)).tolist()\n",
        "  train_examples = [raw_json[i] for i in rp[:-test_set_size]]\n",
        "  test_examples = [raw_json[i] for i in rp[-test_set_size:]]\n",
        "\n",
        "  train_examples = generate_word_combos(train_examples, desired_num_combos=98000, num_words=num_words)\n",
        "  test_examples = generate_word_combos(test_examples, desired_num_combos=2000, num_words=num_words)\n",
        "\n",
        "  train_strokes = [copy.deepcopy(v[\"points\"]) for v in train_examples]\n",
        "  train_texts = [copy.deepcopy(v[\"metadata\"][\"asciiSequence\"]) for v in train_examples]\n",
        "\n",
        "  test_strokes = [copy.deepcopy(v[\"points\"]) for v in test_examples]\n",
        "  test_texts = [copy.deepcopy(v[\"metadata\"][\"asciiSequence\"]) for v in test_examples]\n",
        "\n",
        "  chars = \"abcdefghijklmnopqrstuvwxyz \"\n",
        "  print(f\"Number of examples in the train dataset: {len(train_examples)}\")\n",
        "  print(f\"Number of examples in the test dataset: {len(test_examples)}\")\n",
        "  print(f\"Max token sequence length: {max_seq_length}\")\n",
        "  print(f\"Number of unique characters in the ascii vocabulary: {len(chars)}\")\n",
        "  print(\"Ascii vocabulary:\")\n",
        "  print(f\"\\t'{chars}'\")\n",
        "\n",
        "  print(f\"Split up the dataset into {len(train_examples)} training examples and {len(test_examples)} test examples\")\n",
        "\n",
        "  # wrap in dataset objects\n",
        "  train_dataset = StrokeDataset(train_strokes, train_texts, chars, max_seq_length, name=\"train\", augment=augment)\n",
        "  test_dataset = StrokeDataset(test_strokes, test_texts, chars, max_seq_length, name=\"test\", augment=augment)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader:\n",
        "    \"\"\"\n",
        "    this is really hacky and I\"m not proud of it, but there doesn\"t seem to be\n",
        "    a better way in PyTorch to just create an infinite dataloader?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
        "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
        "        self.data_iter = iter(self.train_loader)\n",
        "\n",
        "    def next(self):\n",
        "        try:\n",
        "            batch = next(self.data_iter)\n",
        "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
        "            self.data_iter = iter(self.train_loader)\n",
        "            batch = next(self.data_iter)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "5OVvHUe4vaSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXAlpngErwVo"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azf2fXB3MeRL"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        # query projections for all heads\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # key, value projections for all heads\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        # calculate query for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # calculate key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "\n",
        "        # cross-attention; (B, nh, T, hs) x (B, nh, hs, T_ctx) -> (B, nh, T, T_ctx)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T_ctx) x (B, nh, T_ctx, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CausalCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Apply causal mask\n",
        "        mask = torch.tril(torch.ones(T, T_ctx)).view(1, 1, T, T_ctx).to(x.device)\n",
        "        att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "# TODO: DEBUG THIS!\n",
        "class MLMCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.mask_prob = config.mlm_mask_prob\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        # Apply MLM masking\n",
        "        mask = torch.bernoulli(torch.full((B, 1, 1, T_ctx), 1 - self.mask_prob)).to(x.device)\n",
        "        k = k * mask\n",
        "        v = v * mask\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CausalDropoutCrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = nn.Dropout(config.attention_dropout)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size()\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "        # Apply causal mask with dropout\n",
        "        mask = torch.tril(torch.ones(T, T_ctx)).view(1, 1, T, T_ctx).to(x.device)\n",
        "        mask = self.dropout(mask)\n",
        "        att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config, cross_attention_class):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = cross_attention_class(config) # NEW\n",
        "        self.ln_3 = nn.LayerNorm(config.n_embd) # NEW\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.cross_attn(self.ln_2(x), context)\n",
        "        x = x + self.mlpf(self.ln_3(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.config = config\n",
        "\n",
        "        if config.cross_attention_type == \"causal\":\n",
        "            cross_attention_class = CausalCrossAttention\n",
        "        elif config.cross_attention_type == \"standard\":\n",
        "            cross_attention_class = CrossAttention\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Cross Attention type {config.cross_attention_type} not implemented!\")\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wce = nn.Embedding(config.context_vocab_size, config.n_embd),\n",
        "            wcpe = nn.Embedding(config.context_block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config, cross_attention_class) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"Number of Transformer parameters: {:.0f}\".format(n_params))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def forward(self, idx, context, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        context_t = context.size(-1)\n",
        "        context_pos = torch.arange(0, context_t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        context_emb = self.transformer.wce(context)\n",
        "        context_pos_emb = self.transformer.wcpe(context_pos)\n",
        "        c = context_emb + context_pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, c)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1ntAoG1vqh"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7otFYXQhr9lz"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    exp_config = get_experiment_config()\n",
        "    data_config = get_data_config()\n",
        "    torch.manual_seed(exp_config.seed)\n",
        "    torch.cuda.manual_seed_all(exp_config.seed)\n",
        "    os.makedirs(exp_config.work_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir=exp_config.work_dir)\n",
        "    results = {}\n",
        "\n",
        "    if exp_config.experiment_type == \"pretraining\":\n",
        "        print(f\"\\nRunning {exp_config.experiment_type} experiment\")\n",
        "        model, best_loss = run_experiment(exp_config, data_config)\n",
        "        if model is not None:\n",
        "            results[f\"{exp_config.wandb_run_name}\"] = {\n",
        "                \"best_test_loss\": best_loss,\n",
        "                \"final_train_loss\": evaluate(model, exp_config, data_config.train_dataset, batch_size=100, max_batches=10),\n",
        "                \"final_test_loss\": evaluate(model, exp_config, data_config.test_dataset, batch_size=100, max_batches=10),\n",
        "                \"config\": asdict(exp_config)\n",
        "            }\n",
        "        else:\n",
        "            results[f\"{exp_config.wandb_run_name}\"] = {\n",
        "                \"best_test_loss\": best_loss,\n",
        "                \"final_train_loss\": None,\n",
        "                \"final_test_loss\": None,\n",
        "                \"status\": \"Already completed\",\n",
        "                \"config\": asdict(exp_config)\n",
        "            }\n",
        "    elif exp_config.experiment_type == \"cross_attention_ablation\":\n",
        "        for attention_type in exp_config.cross_attention_types:\n",
        "            try:\n",
        "              print(f\"\\nRunning {exp_config.experiment_type} experiment with {attention_type} cross-attention\")\n",
        "              exp_config = exp_config.update({\"cross_attention_type\": attention_type})\n",
        "              model, best_loss = run_experiment(exp_config, data_config)\n",
        "              if model is not None:\n",
        "                  results[f\"{exp_config.wandb_run_name}\"] = {\n",
        "                      \"cross_attention_type\": attention_type,\n",
        "                      \"best_test_loss\": best_loss,\n",
        "                      \"final_train_loss\": evaluate(model, exp_config, data_config.train_dataset, batch_size=100, max_batches=10),\n",
        "                      \"final_test_loss\": evaluate(model, exp_config, data_config.test_dataset, batch_size=100, max_batches=10),\n",
        "                      \"config\": asdict(exp_config)\n",
        "                  }\n",
        "              else:\n",
        "                  results[f\"{exp_config.wandb_run_name}\"] = {\n",
        "                      \"cross_attention_type\": attention_type,\n",
        "                      \"best_test_loss\": best_loss,\n",
        "                      \"final_train_loss\": None,\n",
        "                      \"final_test_loss\": None,\n",
        "                      \"status\": \"Already completed\",\n",
        "                      \"config\": asdict(exp_config)\n",
        "                  }\n",
        "            except Exception as e:\n",
        "                results[f\"{exp_config.wandb_run_name}\"] = {\n",
        "                    \"best_test_loss\": None,\n",
        "                    \"final_train_loss\": None,\n",
        "                    \"final_test_loss\": None,\n",
        "                    \"status\": \"failed\",\n",
        "                    \"error\": str(e),\n",
        "                    \"config\": asdict(exp_config)\n",
        "                }\n",
        "\n",
        "        with open(\"ablation_study_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        print(\"Ablation study complete. Results saved to 'ablation_study_results.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "v1EnyC0G0LtU",
        "outputId": "2764a007-dc69-4a02-aacb-7a3709cbd05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e92aa00a-d01a-4a0c-a021-51b09c05d6e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e92aa00a-d01a-4a0c-a021-51b09c05d6e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1197-bigbank-cursive.json to 1197-bigbank-cursive (1).json\n",
            "For a dataset of 1078 examples we can generate 208207076 combinations of 3 examples.\n",
            "Generating 98000 random (and thus possibly overlapping) combos...\n",
            "For a dataset of 119 examples we can generate 273819 combinations of 3 examples.\n",
            "Generating 2000 random (and thus possibly overlapping) combos...\n",
            "Number of examples in the train dataset: 98000\n",
            "Number of examples in the test dataset: 2000\n",
            "Max token sequence length: 1500\n",
            "Number of unique characters in the ascii vocabulary: 27\n",
            "Ascii vocabulary:\n",
            "\t'abcdefghijklmnopqrstuvwxyz '\n",
            "Split up the dataset into 98000 training examples and 2000 test examples\n",
            "\n",
            "Running experiment with standard cross-attention\n",
            "\n",
            "Running experiment with causal cross-attention\n",
            "Ablation study complete. Results saved to 'ablation_study_results.json'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}