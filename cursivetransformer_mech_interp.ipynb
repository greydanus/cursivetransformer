{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPP161RNY7F/Y+YYzLQyQJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer_mech_interp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "\n",
        "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "import os\n",
        "import sys; sys.path.append('/content/cursivetransformer');\n",
        "import copy\n",
        "import types\n",
        "from typing import List, Callable, Dict, Optional, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import circuitsvis as cv\n",
        "\n",
        "from IPython.display import display\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint, get_latest_checkpoint_artifact\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes, strokes_to_offsets\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "from cursivetransformer.mech_interp import (\n",
        "    HookedCursiveTransformer,\n",
        "    HookedCursiveTransformerConfig,\n",
        "    convert_cursivetransformer_model_config,\n",
        "    visualize_attention\n",
        ")\n",
        "\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "model = HookedCursiveTransformer.from_pretrained(\"cursivetransformer\", cfg)\n",
        "\n",
        "# Induction Circuit Investigation and Analysis\n",
        "\n",
        "## 1. Get an example from the dataset and create repeated pattern\n",
        "\n",
        "def create_repeated_sequence(stroke_sequence, pad_token, repeat_count=2):\n",
        "    pad_tensor = torch.tensor([pad_token])\n",
        "    sequences = [stroke_sequence if i % 2 == 0 else pad_tensor for i in range(repeat_count * 2 - 1)]\n",
        "    return torch.cat(sequences).unsqueeze(0)\n",
        "\n",
        "batch_size = 10\n",
        "index = 0\n",
        "stroke_tensor, ascii_tensor, y = test_dataset[index]\n",
        "stroke_tensor = stroke_tensor.unsqueeze(0) # Shape: [1, 1000]\n",
        "ascii_tensor = ascii_tensor.unsqueeze(0) # Shape: [1, 50]\n",
        "y = y.unsqueeze(0) # Shape: [1, 1000]\n",
        "\n",
        "### Create repeated sequence of stroke and ascii tokens (improved approach for reusability)\n",
        "\n",
        "stroke_sequence = stroke_tensor[0][:50]\n",
        "ascii_sequence = ascii_tensor[0][:2]\n",
        "\n",
        "repeated_stroke_sequence = create_repeated_sequence(stroke_sequence, test_dataset.PAD_TOKEN)\n",
        "repeated_ascii_sequence = einops.repeat(ascii_sequence, \"seq_len -> (2 seq_len)\")\n",
        "\n",
        "# - [x] TODO: FIX THIS!\n",
        "_ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(stroke_sequence)), test_dataset.decode_text(ascii_sequence))\n",
        "_ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(repeated_stroke_sequence)), test_dataset.decode_text(repeated_ascii_sequence))\n",
        "\n",
        "## 2. Run the model over the input token sequences\n",
        "with torch.no_grad():\n",
        "    outputs, cache = model.run_with_cache(\n",
        "        repeated_stroke_sequence, repeated_ascii_sequence,\n",
        "        return_type=\"both\", per_token_loss=True\n",
        "    )\n",
        "\n",
        "## 3. Compute and plot per-token loss by position\n",
        "\n",
        "loss_by_position = outputs[1].detach().cpu().numpy()  # Shape: [batch_size, sequence_length]\n",
        "px.line(y=loss_by_position[0], labels={\"x\": \"Position\", \"y\": \"Loss\"}, title=\"Loss by position on repeated token sequence\").show()\n",
        "\n",
        "# # Store induction scores\n",
        "# induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "\n",
        "# def induction_score_hook(pattern: torch.Tensor, hook: HookPoint):\n",
        "#     induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1 - stroke_sequence.shape[0])\n",
        "#     induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "#     induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# # Updated hook filter for both self-attention and cross-attention patterns\n",
        "# pattern_hook_names_filter = lambda name: name.endswith(\"attn.hook_pattern\") or name.endswith(\"cross_attn.hook_pattern\")\n",
        "\n",
        "# # Run with hooks to collect induction scores\n",
        "# _ = model.run_with_hooks(\n",
        "#     repeated_tokens,\n",
        "#     repeated_context,\n",
        "#     fwd_hooks=[(\n",
        "#         pattern_hook_names_filter,\n",
        "#         induction_score_hook\n",
        "#     )]\n",
        "# )\n",
        "\n",
        "# # Visualize induction score by head\n",
        "# px.imshow(induction_score_store.detach().cpu(), labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Induction Score by Head\").show()\n",
        "\n",
        "# # Visualization of Attention Pattern\n",
        "# def visualize_pattern_hook(pattern: torch.Tensor, hook: HookPoint):\n",
        "#     display(\n",
        "#         cv.attention.attention_patterns(\n",
        "#             tokens=repeated_tokens,\n",
        "#             attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# induction_head_layer = 3\n",
        "# induction_head_index = 1\n",
        "# model.run_with_hooks(\n",
        "#     repeated_tokens,\n",
        "#     repeated_context,\n",
        "#     fwd_hooks=[(\n",
        "#         'blocks.{}.cross_attn.hook_pattern'.format(induction_head_layer),\n",
        "#         visualize_pattern_hook\n",
        "#     )]\n",
        "# )\n",
        "\n",
        "# # Activation Patching\n",
        "\n",
        "# # Activation Patching - Updated for Cross-Attention\n",
        "# def activation_patching(\n",
        "#     model: HookedCursiveTransformer,\n",
        "#     x_clean: torch.Tensor,\n",
        "#     c_clean: torch.Tensor,\n",
        "#     x_corrupted: torch.Tensor,\n",
        "#     c_corrupted: torch.Tensor,\n",
        "#     patching_nodes: List[str],\n",
        "#     patch_positions: Optional[torch.Tensor] = None,\n",
        "# ):\n",
        "#     _, cache_corrupted = model.run_with_cache(x_corrupted, c_corrupted, return_type=\"both\")\n",
        "\n",
        "#     def patching_hook(act, hook):\n",
        "#         act_corrupted = cache_corrupted[hook.name]\n",
        "#         if patch_positions is not None:\n",
        "#             act[:, patch_positions, :] = act_corrupted[:, patch_positions, :]\n",
        "#         else:\n",
        "#             act[:] = act_corrupted\n",
        "#         return act\n",
        "\n",
        "#     hooks = [(node, patching_hook) for node in patching_nodes]\n",
        "#     logits_patched = model.run_with_hooks(x_clean, c_clean, fwd_hooks=hooks, return_type=\"logits\")\n",
        "#     return logits_patched\n",
        "\n",
        "# # Example Activation Patching\n",
        "# x_clean = stroke_sequence.unsqueeze(0)\n",
        "# c_clean = ascii_sequence.unsqueeze(0)\n",
        "\n",
        "# x_corrupted = x_clean.clone()\n",
        "# x_corrupted = (x_corrupted + 1) % model.cfg.d_vocab\n",
        "# c_corrupted = c_clean.clone()\n",
        "\n",
        "# patching_nodes = ['blocks.0.cross_attn.hook_result']\n",
        "# logits_patched = activation_patching(\n",
        "#     model,\n",
        "#     x_clean,\n",
        "#     c_clean,\n",
        "#     x_corrupted,\n",
        "#     c_corrupted,\n",
        "#     patching_nodes,\n",
        "# )\n",
        "\n",
        "# # Get predictions from patched logits\n",
        "# predictions_patched = logits_patched.argmax(dim=-1)\n",
        "\n",
        "# # Run the clean input without patches\n",
        "# logits_clean = model(x_clean, c_clean)\n",
        "# predictions_clean = logits_clean.argmax(dim=-1)\n",
        "\n",
        "# # Run the corrupted input without patches\n",
        "# logits_corrupted = model(x_corrupted, c_corrupted)\n",
        "# predictions_corrupted = logits_corrupted.argmax(dim=-1)\n",
        "\n",
        "# # Compare predictions\n",
        "# print(\"Clean Predictions:\", predictions_clean)\n",
        "# print(\"Corrupted Predictions:\", predictions_corrupted)\n",
        "# print(\"Patched Predictions:\", predictions_patched)\n",
        "\n",
        "# # Visualize the strokes if applicable\n",
        "# _ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(stroke_sequence)), test_dataset.decode_text(ascii_sequence))\n",
        "# _ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(x_corrupted[0])), test_dataset.decode_text(c_corrupted[0]))"
      ],
      "metadata": {
        "id": "m-8YVXW7WC4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}