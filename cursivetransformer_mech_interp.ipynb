{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/cursivetransformer_mech_interp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsUHANK_Y4Xx"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "\n",
        "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8bSOiNpY7GZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys; sys.path.append('/content/cursivetransformer');\n",
        "import copy\n",
        "import types\n",
        "from typing import List, Callable, Dict, Optional, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import circuitsvis as cv\n",
        "\n",
        "from IPython.display import display\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint, get_latest_checkpoint_artifact\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes, strokes_to_offsets\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "from cursivetransformer.mech_interp import (\n",
        "    HookedCursiveTransformer,\n",
        "    HookedCursiveTransformerConfig,\n",
        "    convert_cursivetransformer_model_config,\n",
        "    visualize_attention\n",
        ")\n",
        "\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import ActivationCache\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTlI-5Vqei9_"
      },
      "outputs": [],
      "source": [
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeOQnX8OZACh"
      },
      "outputs": [],
      "source": [
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "model = HookedCursiveTransformer.from_pretrained(\"cursivetransformer\", cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArxUFBWtZFzH"
      },
      "source": [
        "# Induction Circuit Investigation and Analysis\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-853bKwXfzy"
      },
      "outputs": [],
      "source": [
        "test_dataset.feature_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIqqgt1jXkA_"
      },
      "outputs": [],
      "source": [
        "test_dataset.cumulative_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "586I_B5ZZJMb"
      },
      "outputs": [],
      "source": [
        "def generate_repeated_stroke_tokens(\n",
        "    model,\n",
        "    seq_len: int,\n",
        "    n_repeats: int,\n",
        "    batch_size: int = 1\n",
        ") -> Int[torch.Tensor, \"batch_size full_seq_len\"]:\n",
        "    \"\"\"\n",
        "    Generates a sequence of repeated stroke tokens, alternating between θ and r tokens.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        seq_len: Number of (θ, r) pairs in the initial sequence.\n",
        "        n_repeats: Number of times to repeat the sequence.\n",
        "        batch_size: Batch size.\n",
        "\n",
        "    Returns:\n",
        "        rep_tokens: Tensor of shape [batch_size, n_repeats * 2 * seq_len]\n",
        "    \"\"\"\n",
        "    device = model.cfg.device\n",
        "    feature_sizes = model.feature_sizes  # [size_r_bins, size_theta_bins]\n",
        "    cumulative_sizes = model.cumulative_sizes  # cumulative indices for token types\n",
        "\n",
        "    # Get valid indices for θ and r tokens\n",
        "    theta_token_indices = torch.arange(\n",
        "        cumulative_sizes[1],\n",
        "        cumulative_sizes[2],\n",
        "        device=device\n",
        "    )\n",
        "    r_token_indices = torch.arange(\n",
        "        cumulative_sizes[0],\n",
        "        cumulative_sizes[1],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Generate random θ and r tokens\n",
        "    random_theta_tokens = theta_token_indices[\n",
        "        torch.randint(\n",
        "            low=0,\n",
        "            high=feature_sizes[1],\n",
        "            size=(batch_size, seq_len),\n",
        "            device=device\n",
        "        )\n",
        "    ]\n",
        "    random_r_tokens = r_token_indices[\n",
        "        torch.randint(\n",
        "            low=0,\n",
        "            high=feature_sizes[0],\n",
        "            size=(batch_size, seq_len),\n",
        "            device=device\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Alternate between θ and r tokens\n",
        "    stroke_tokens_half = torch.zeros(batch_size, seq_len * 2, dtype=torch.long, device=device)\n",
        "    stroke_tokens_half[:, 0::2] = random_theta_tokens\n",
        "    stroke_tokens_half[:, 1::2] = random_r_tokens\n",
        "\n",
        "    # Repeat the sequence\n",
        "    rep_tokens = stroke_tokens_half.repeat(1, n_repeats)\n",
        "\n",
        "    return rep_tokens\n",
        "\n",
        "def generate_random_ascii_context(\n",
        "    model,\n",
        "    batch_size: int = 1\n",
        ") -> Int[torch.Tensor, \"batch_size context_seq_len\"]:\n",
        "    \"\"\"\n",
        "    Generates a random ASCII context sequence.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        batch_size: Batch size.\n",
        "\n",
        "    Returns:\n",
        "        context_tokens: Tensor of shape [batch_size, context_seq_len]\n",
        "    \"\"\"\n",
        "    device = model.cfg.device\n",
        "    context_seq_len = model.cfg.context_block_size\n",
        "    context_vocab_size = model.cfg.context_vocab_size\n",
        "\n",
        "    context_tokens = torch.randint(\n",
        "        low=0,\n",
        "        high=context_vocab_size - 1,  # Exclude PAD token\n",
        "        size=(batch_size, context_seq_len),\n",
        "        dtype=torch.long,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return context_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFZ00s2aSpeK"
      },
      "outputs": [],
      "source": [
        "def run_and_cache_model_repeated_tokens(\n",
        "    model,\n",
        "    rep_tokens: torch.Tensor,\n",
        "    context_tokens: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, ActivationCache]:\n",
        "    \"\"\"\n",
        "    Runs the model on repeated tokens and caches activations.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        rep_tokens: Input stroke tokens of shape [batch_size, seq_len]\n",
        "        context_tokens: Input context tokens of shape [batch_size, context_seq_len]\n",
        "\n",
        "    Returns:\n",
        "        logits: Model output logits.\n",
        "        cache: Activation cache.\n",
        "    \"\"\"\n",
        "    # Shift inputs to create targets\n",
        "    inputs = rep_tokens[:, :-1]\n",
        "    targets = rep_tokens[:, 1:]\n",
        "\n",
        "    # Run model with cache\n",
        "    logits, cache = model.run_with_cache(\n",
        "        tokens=inputs,\n",
        "        context=context_tokens,\n",
        "        return_type=\"both\"\n",
        "    )\n",
        "\n",
        "    return logits, targets, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH6etw03SpeL"
      },
      "outputs": [],
      "source": [
        "def compute_induction_scores(\n",
        "    model,\n",
        "    rep_tokens: torch.Tensor,\n",
        "    cache: ActivationCache\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes induction scores for all attention heads, accounting for the alternating tokens.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        rep_tokens: Input stroke tokens of shape [batch_size, seq_len]\n",
        "        cache: Activation cache.\n",
        "\n",
        "    Returns:\n",
        "        induction_scores: Tensor of shape [num_layers, num_heads]\n",
        "    \"\"\"\n",
        "    num_layers = model.cfg.n_layers\n",
        "    num_heads = model.cfg.n_heads\n",
        "    induction_scores = torch.zeros(num_layers, num_heads, device=model.cfg.device)\n",
        "\n",
        "    batch_size, seq_len = rep_tokens.shape\n",
        "\n",
        "    # Group tokens into pairs of (θ, r)\n",
        "    token_pairs = rep_tokens.view(batch_size, seq_len // 2, 2)  # Shape: [batch_size, seq_len_pairs, 2]\n",
        "\n",
        "    # Represent token pairs as tuples for comparison\n",
        "    token_pair_tuples = [tuple(pair.tolist()) for pair in token_pairs[0]]\n",
        "\n",
        "    # For each position in the second half, find matching token in the first half\n",
        "    half_point = len(token_pair_tuples) // 2\n",
        "    for layer in range(num_layers):\n",
        "        attn_patterns = cache[\"pattern\", layer]  # Shape: [batch_size, num_heads, seq_len_q, seq_len_k]\n",
        "        for head in range(num_heads):\n",
        "            attn = attn_patterns[0, head]  # Shape: [seq_len_q, seq_len_k]\n",
        "            scores = []\n",
        "            for i in range(half_point, len(token_pair_tuples) - 1):\n",
        "                current_pair = token_pair_tuples[i]\n",
        "                # Find the last occurrence of the same token pair before position i\n",
        "                try:\n",
        "                    k_pos = max(j for j in range(i) if token_pair_tuples[j] == current_pair)\n",
        "                    # Since each token pair corresponds to 2 tokens, adjust positions\n",
        "                    q_pos = i * 2  # Query position in token indices\n",
        "                    k_pos = k_pos * 2  # Key position in token indices\n",
        "                    # Get attention weight from q_pos to k_pos (for both θ and r tokens)\n",
        "                    attn_weight_theta = attn[q_pos - 1, k_pos - 1]\n",
        "                    attn_weight_r = attn[q_pos, k_pos]\n",
        "                    # Average the attention weights for θ and r tokens\n",
        "                    attn_weight = (attn_weight_theta + attn_weight_r) / 2\n",
        "                    scores.append(attn_weight.item())\n",
        "                except ValueError:\n",
        "                    # No previous matching token pair found\n",
        "                    continue\n",
        "            if scores:\n",
        "                induction_scores[layer, head] = torch.tensor(scores).mean()\n",
        "    return induction_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0REWk_PFSpeL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_induction_scores(induction_scores: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of induction scores.\n",
        "\n",
        "    Args:\n",
        "        induction_scores: Tensor of shape [num_layers, num_heads]\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(\n",
        "        induction_scores.cpu().numpy(),\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        cmap=\"YlGnBu\",\n",
        "        xticklabels=[f\"H{h}\" for h in range(induction_scores.shape[1])],\n",
        "        yticklabels=[f\"L{l}\" for l in range(induction_scores.shape[0])]\n",
        "    )\n",
        "    plt.title(\"Induction Scores per Head\")\n",
        "    plt.xlabel(\"Heads\")\n",
        "    plt.ylabel(\"Layers\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3nOdMpbSpeL"
      },
      "outputs": [],
      "source": [
        "def plot_head_attention_pattern(\n",
        "    cache: ActivationCache,\n",
        "    layer: int,\n",
        "    head: int,\n",
        "    seq_len: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots the attention pattern of a specific head.\n",
        "\n",
        "    Args:\n",
        "        cache: Activation cache.\n",
        "        layer: Layer index.\n",
        "        head: Head index.\n",
        "        seq_len: Total sequence length.\n",
        "    \"\"\"\n",
        "    attn = cache[\"pattern\", layer][0, head].detach().cpu().numpy()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(attn, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(f\"Attention Pattern - Layer {layer}, Head {head}\")\n",
        "    plt.xlabel(\"Key Positions\")\n",
        "    plt.ylabel(\"Query Positions\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApexXrcfSpeL"
      },
      "outputs": [],
      "source": [
        "def compute_cross_attention_induction_scores(\n",
        "    model,\n",
        "    context_tokens: torch.Tensor,\n",
        "    cache: ActivationCache\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes induction-like scores for cross-attention heads.\n",
        "\n",
        "    Args:\n",
        "        model: The model instance.\n",
        "        context_tokens: Context tokens of shape [batch_size, context_seq_len]\n",
        "        cache: Activation cache.\n",
        "\n",
        "    Returns:\n",
        "        cross_induction_scores: Tensor of shape [num_layers, num_heads]\n",
        "    \"\"\"\n",
        "    num_layers = model.cfg.n_layers\n",
        "    num_heads = model.cfg.n_heads\n",
        "    cross_induction_scores = torch.zeros(num_layers, num_heads, device=model.cfg.device)\n",
        "\n",
        "    batch_size, context_seq_len = context_tokens.shape\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        attn_patterns = cache[\"pattern\", layer, \"cross_attn\"]  # Need to access cross-attention patterns\n",
        "        for head in range(num_heads):\n",
        "            attn = attn_patterns[0, head]  # Shape: [stroke_seq_len, context_seq_len]\n",
        "            # For this example, we might need more specific analysis based on the use case\n",
        "            # Placeholder for cross-attention induction score computation\n",
        "            cross_induction_scores[layer, head] = attn.mean().item()\n",
        "    return cross_induction_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-8YVXW7WC4h"
      },
      "outputs": [],
      "source": [
        "# Store induction scores\n",
        "# induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "\n",
        "# def induction_score_hook(pattern: torch.Tensor, hook: HookPoint):\n",
        "#     induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1 - stroke_sequence.shape[0])\n",
        "#     induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "#     induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# # Updated hook filter for both self-attention and cross-attention patterns\n",
        "# pattern_hook_names_filter = lambda name: name.endswith(\"attn.hook_pattern\") or name.endswith(\"cross_attn.hook_pattern\")\n",
        "\n",
        "# # Run with hooks to collect induction scores\n",
        "# _ = model.run_with_hooks(\n",
        "#     repeated_tokens,\n",
        "#     repeated_context,\n",
        "#     fwd_hooks=[(\n",
        "#         pattern_hook_names_filter,\n",
        "#         induction_score_hook\n",
        "#     )]\n",
        "# )\n",
        "\n",
        "# # Visualize induction score by head\n",
        "# px.imshow(induction_score_store.detach().cpu(), labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Induction Score by Head\").show()\n",
        "\n",
        "# # Visualization of Attention Pattern\n",
        "# def visualize_pattern_hook(pattern: torch.Tensor, hook: HookPoint):\n",
        "#     display(\n",
        "#         cv.attention.attention_patterns(\n",
        "#             tokens=repeated_tokens,\n",
        "#             attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# induction_head_layer = 3\n",
        "# induction_head_index = 1\n",
        "# model.run_with_hooks(\n",
        "#     repeated_tokens,\n",
        "#     repeated_context,\n",
        "#     fwd_hooks=[(\n",
        "#         'blocks.{}.cross_attn.hook_pattern'.format(induction_head_layer),\n",
        "#         visualize_pattern_hook\n",
        "#     )]\n",
        "# )\n",
        "\n",
        "# # Activation Patching\n",
        "\n",
        "# # Activation Patching - Updated for Cross-Attention\n",
        "# def activation_patching(\n",
        "#     model: HookedCursiveTransformer,\n",
        "#     x_clean: torch.Tensor,\n",
        "#     c_clean: torch.Tensor,\n",
        "#     x_corrupted: torch.Tensor,\n",
        "#     c_corrupted: torch.Tensor,\n",
        "#     patching_nodes: List[str],\n",
        "#     patch_positions: Optional[torch.Tensor] = None,\n",
        "# ):\n",
        "#     _, cache_corrupted = model.run_with_cache(x_corrupted, c_corrupted, return_type=\"both\")\n",
        "\n",
        "#     def patching_hook(act, hook):\n",
        "#         act_corrupted = cache_corrupted[hook.name]\n",
        "#         if patch_positions is not None:\n",
        "#             act[:, patch_positions, :] = act_corrupted[:, patch_positions, :]\n",
        "#         else:\n",
        "#             act[:] = act_corrupted\n",
        "#         return act\n",
        "\n",
        "#     hooks = [(node, patching_hook) for node in patching_nodes]\n",
        "#     logits_patched = model.run_with_hooks(x_clean, c_clean, fwd_hooks=hooks, return_type=\"logits\")\n",
        "#     return logits_patched\n",
        "\n",
        "# # Example Activation Patching\n",
        "# x_clean = stroke_sequence.unsqueeze(0)\n",
        "# c_clean = ascii_sequence.unsqueeze(0)\n",
        "\n",
        "# x_corrupted = x_clean.clone()\n",
        "# x_corrupted = (x_corrupted + 1) % model.cfg.d_vocab\n",
        "# c_corrupted = c_clean.clone()\n",
        "\n",
        "# patching_nodes = ['blocks.0.cross_attn.hook_result']\n",
        "# logits_patched = activation_patching(\n",
        "#     model,\n",
        "#     x_clean,\n",
        "#     c_clean,\n",
        "#     x_corrupted,\n",
        "#     c_corrupted,\n",
        "#     patching_nodes,\n",
        "# )\n",
        "\n",
        "# # Get predictions from patched logits\n",
        "# predictions_patched = logits_patched.argmax(dim=-1)\n",
        "\n",
        "# # Run the clean input without patches\n",
        "# logits_clean = model(x_clean, c_clean)\n",
        "# predictions_clean = logits_clean.argmax(dim=-1)\n",
        "\n",
        "# # Run the corrupted input without patches\n",
        "# logits_corrupted = model(x_corrupted, c_corrupted)\n",
        "# predictions_corrupted = logits_corrupted.argmax(dim=-1)\n",
        "\n",
        "# # Compare predictions\n",
        "# print(\"Clean Predictions:\", predictions_clean)\n",
        "# print(\"Corrupted Predictions:\", predictions_corrupted)\n",
        "# print(\"Patched Predictions:\", predictions_patched)\n",
        "\n",
        "# # Visualize the strokes if applicable\n",
        "# _ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(stroke_sequence)), test_dataset.decode_text(ascii_sequence))\n",
        "# _ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(x_corrupted[0])), test_dataset.decode_text(c_corrupted[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}