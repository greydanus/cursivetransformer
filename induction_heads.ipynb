{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOULuhJ4lTtNVwSTG7im+ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/induction_heads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Induction Heads in `cursivetransformer`\n",
        "\n",
        "In this notebook, we aim to detect **induction heads** in the `cursivetransformer` model. We'll adapt methods from Neel Nanda's work on reverse-engineering induction circuits to our model, which is trained to generate cursive handwriting. Our goal is to identify components within the model that perform induction-like mechanisms, specifically focusing on how the model might use previous tokens to inform the generation of subsequent ones.\n",
        "\n",
        "---\n",
        "\n",
        "## **Table of Contents**\n",
        "\n",
        "1. [Introduction to Induction Heads](#introduction)\n",
        "2. [Setting Up the Environment](#setup)\n",
        "3. [Loading the Model and Data](#loading)\n",
        "4. [Analyzing Attention Patterns](#attention)\n",
        "5. [Identifying Candidate Induction Heads](#candidate_heads)\n",
        "6. [Reverse-Engineering the Induction Circuit](#reverse_engineering)\n",
        "    - [QK and OV Circuits](#qk_ov)\n",
        "    - [Analyzing the QK Circuit](#qk_analysis)\n",
        "    - [Analyzing the OV Circuit](#ov_analysis)\n",
        "7. [Computing Composition Scores](#composition_scores)\n",
        "8. [Visualizing and Interpreting Results](#visualization)\n",
        "9. [Conclusion](#conclusion)\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"introduction\"></a>\n",
        "## 1. Introduction to Induction Heads\n",
        "\n",
        "**Induction heads** are specialized attention heads in transformer models that enable the model to perform **in-context learning**. They allow the model to recognize and replicate patterns by attending to previous occurrences of tokens in the sequence.\n",
        "\n",
        "In the context of the `cursivetransformer`, which generates cursive handwriting, induction heads may help the model maintain consistent handwriting styles or replicate patterns in stroke sequences.\n",
        "\n",
        "**Our Objectives:**\n",
        "\n",
        "- Identify attention heads in the `cursivetransformer` that function as induction heads.\n",
        "- Reverse-engineer the circuits involved to understand how they operate.\n",
        "- Analyze the QK (query-key) and OV (output-value) circuits to see how they contribute to induction behavior.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"setup\"></a>\n",
        "## 2. Setting Up the Environment\n",
        "\n",
        "First, let's import necessary libraries and set up the environment.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import einops\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import tqdm\n",
        "\n",
        "# If using Jupyter, uncomment the next line\n",
        "# %matplotlib inline\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"loading\"></a>\n",
        "## 3. Loading the Model and Data\n",
        "\n",
        "We'll load the `cursivetransformer` model and the associated dataset.\n",
        "\n",
        "**Note:** Ensure that you have the model and dataset available in your environment. Adjust paths as necessary.\n",
        "\n",
        "```python\n",
        "# Assume the model and dataset classes are defined in cursivetransformer.py\n",
        "# You may need to adjust the import statements based on your project structure\n",
        "\n",
        "from cursivetransformer import CursiveTransformer, CursiveDataset\n",
        "\n",
        "# Load the model\n",
        "model = CursiveTransformer.load_from_checkpoint('path_to_model_checkpoint.ckpt')\n",
        "model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = CursiveDataset(split='train')\n",
        "test_dataset = CursiveDataset(split='test')\n",
        "\n",
        "# Define data loaders if necessary\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64  # Adjust as needed\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"attention\"></a>\n",
        "## 4. Analyzing Attention Patterns\n",
        "\n",
        "To detect induction heads, we'll start by analyzing the attention patterns of each head in the model. Induction heads typically display a **distinctive attention pattern**: they attend from a token to previous occurrences of the same token or related tokens.\n",
        "\n",
        "### Extracting Attention Patterns\n",
        "\n",
        "We'll define a function to extract and visualize attention patterns for a given layer and head.\n",
        "\n",
        "```python\n",
        "def get_attention_patterns(model, data_loader, layer_idx, head_idx, num_batches=1):\n",
        "    attention_patterns = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        # Output shape: [batch_size, num_heads, seq_len, seq_len]\n",
        "        attention_patterns.append(output.detach().cpu())\n",
        "\n",
        "    hook_handle = model.transformer.layers[layer_idx].self_attn.attn_drop.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Run a few batches to collect attention patterns\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "        inputs, _ = batch  # Adjust unpacking based on your dataset\n",
        "        inputs = inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(inputs)\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Concatenate attention patterns from all batches\n",
        "    attention_patterns = torch.cat(attention_patterns, dim=0)  # Shape: [total_samples, num_heads, seq_len, seq_len]\n",
        "\n",
        "    # Extract patterns for the specified head\n",
        "    patterns = attention_patterns[:, head_idx, :, :]  # Shape: [total_samples, seq_len, seq_len]\n",
        "\n",
        "    return patterns\n",
        "```\n",
        "\n",
        "### Visualizing Attention Patterns\n",
        "\n",
        "We'll plot the average attention pattern for the specified head.\n",
        "\n",
        "```python\n",
        "def plot_attention_pattern(patterns, layer_idx, head_idx):\n",
        "    # Compute the average attention pattern over samples\n",
        "    avg_pattern = patterns.mean(dim=0)  # Shape: [seq_len, seq_len]\n",
        "\n",
        "    fig = px.imshow(\n",
        "        avg_pattern,\n",
        "        labels={'x': 'Key Position', 'y': 'Query Position', 'color': 'Attention Weight'},\n",
        "        title=f'Average Attention Pattern for Layer {layer_idx}, Head {head_idx}',\n",
        "        color_continuous_scale='Blues'\n",
        "    )\n",
        "    fig.update_layout(width=600, height=600)\n",
        "    fig.show()\n",
        "```\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "Let's analyze and plot attention patterns for all heads in a specific layer.\n",
        "\n",
        "```python\n",
        "layer_idx = 0  # Adjust as needed\n",
        "num_heads = model.transformer.layers[layer_idx].self_attn.num_heads\n",
        "\n",
        "for head_idx in range(num_heads):\n",
        "    patterns = get_attention_patterns(model, test_loader, layer_idx, head_idx, num_batches=1)\n",
        "    plot_attention_pattern(patterns, layer_idx, head_idx)\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- Look for heads where the attention pattern shows a diagonal offset, indicating that the head is attending to previous tokens in a specific way.\n",
        "- Heads that show such patterns are candidates for being induction heads.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"candidate_heads\"></a>\n",
        "## 5. Identifying Candidate Induction Heads\n",
        "\n",
        "Based on the attention patterns, we can identify candidate heads that might be functioning as induction heads.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Suppose we observe that head 3 in layer 0 shows a diagonal attention pattern with an offset, suggesting it might be an induction head.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"reverse_engineering\"></a>\n",
        "## 6. Reverse-Engineering the Induction Circuit\n",
        "\n",
        "To confirm that a head is an induction head, we'll analyze its **QK** and **OV** circuits. This involves examining the weight matrices and understanding how they contribute to the attention mechanism.\n",
        "\n",
        "<a name=\"qk_ov\"></a>\n",
        "### Understanding QK and OV Circuits\n",
        "\n",
        "- **QK Circuit:** Determines the attention scores by projecting queries and keys.\n",
        "- **OV Circuit:** Determines how information is aggregated and written back to the residual stream.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"qk_analysis\"></a>\n",
        "### Analyzing the QK Circuit\n",
        "\n",
        "#### Extracting Q and K Matrices\n",
        "\n",
        "```python\n",
        "def get_QK_matrices(model, layer_idx, head_idx):\n",
        "    W_Q = model.transformer.layers[layer_idx].self_attn.q_proj.weight  # Shape: [d_model, d_head * num_heads]\n",
        "    W_K = model.transformer.layers[layer_idx].self_attn.k_proj.weight  # Shape: [d_model, d_head * num_heads]\n",
        "\n",
        "    d_head = model.transformer.layers[layer_idx].self_attn.head_dim\n",
        "    W_Q = W_Q[:, head_idx * d_head: (head_idx + 1) * d_head]  # Shape: [d_model, d_head]\n",
        "    W_K = W_K[:, head_idx * d_head: (head_idx + 1) * d_head]\n",
        "\n",
        "    return W_Q, W_K\n",
        "```\n",
        "\n",
        "#### Computing the QK Circuit\n",
        "\n",
        "The QK circuit is given by \\( W_Q W_K^T \\).\n",
        "\n",
        "```python\n",
        "def compute_QK_circuit(W_Q, W_K):\n",
        "    return W_Q @ W_K.T  # Shape: [d_model, d_model]\n",
        "```\n",
        "\n",
        "#### Visualizing the QK Circuit\n",
        "\n",
        "We can visualize the QK circuit to understand what features the head is focusing on.\n",
        "\n",
        "```python\n",
        "def plot_QK_circuit(QK_circuit):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(QK_circuit.detach().cpu().numpy(), cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('QK Circuit')\n",
        "    plt.xlabel('Key Features')\n",
        "    plt.ylabel('Query Features')\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "#### Example Usage\n",
        "\n",
        "```python\n",
        "W_Q, W_K = get_QK_matrices(model, layer_idx=0, head_idx=3)\n",
        "QK_circuit = compute_QK_circuit(W_Q, W_K)\n",
        "plot_QK_circuit(QK_circuit)\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- Look for patterns that indicate the head is attending to previous tokens or specific features that align with induction behavior.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"ov_analysis\"></a>\n",
        "### Analyzing the OV Circuit\n",
        "\n",
        "#### Extracting V and O Matrices\n",
        "\n",
        "```python\n",
        "def get_VO_matrices(model, layer_idx, head_idx):\n",
        "    W_V = model.transformer.layers[layer_idx].self_attn.v_proj.weight  # Shape: [d_model, d_head * num_heads]\n",
        "    W_O = model.transformer.layers[layer_idx].self_attn.out_proj.weight  # Shape: [d_model, d_head * num_heads]\n",
        "\n",
        "    d_head = model.transformer.layers[layer_idx].self_attn.head_dim\n",
        "    W_V = W_V[:, head_idx * d_head: (head_idx + 1) * d_head]  # Shape: [d_model, d_head]\n",
        "    W_O = W_O[head_idx * d_head: (head_idx + 1) * d_head, :]  # Shape: [d_head, d_model]\n",
        "\n",
        "    return W_V, W_O\n",
        "```\n",
        "\n",
        "#### Computing the OV Circuit\n",
        "\n",
        "The OV circuit is given by \\( W_V W_O \\).\n",
        "\n",
        "```python\n",
        "def compute_OV_circuit(W_V, W_O):\n",
        "    return W_V @ W_O  # Shape: [d_model, d_model]\n",
        "```\n",
        "\n",
        "#### Visualizing the OV Circuit\n",
        "\n",
        "```python\n",
        "def plot_OV_circuit(OV_circuit):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(OV_circuit.detach().cpu().numpy(), cmap='coolwarm')\n",
        "    plt.colorbar()\n",
        "    plt.title('OV Circuit')\n",
        "    plt.xlabel('Output Features')\n",
        "    plt.ylabel('Value Features')\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "#### Example Usage\n",
        "\n",
        "```python\n",
        "W_V, W_O = get_VO_matrices(model, layer_idx=0, head_idx=3)\n",
        "OV_circuit = compute_OV_circuit(W_V, W_O)\n",
        "plot_OV_circuit(OV_circuit)\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- The OV circuit shows how the head writes information back to the residual stream.\n",
        "- An induction head often copies information from the key positions to the query positions.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"composition_scores\"></a>\n",
        "## 7. Computing Composition Scores\n",
        "\n",
        "Composition scores help us quantify how much two components (e.g., heads) are interacting or composing with each other.\n",
        "\n",
        "### Defining the Composition Score\n",
        "\n",
        "We can define the composition score between two matrices \\( W_A \\) and \\( W_B \\) as:\n",
        "\n",
        "\\[ \\text{Comp\\_Score} = \\frac{\\| W_A W_B \\|_F}{\\| W_A \\|_F \\| W_B \\|_F} \\]\n",
        "\n",
        "where \\( \\| \\cdot \\|_F \\) denotes the Frobenius norm.\n",
        "\n",
        "### Computing Composition Scores Between Heads\n",
        "\n",
        "Suppose we suspect that head 3 in layer 0 is composing with head 5 in layer 1.\n",
        "\n",
        "```python\n",
        "def get_composition_score(W_A, W_B):\n",
        "    numerator = torch.norm(W_A @ W_B, p='fro')\n",
        "    denominator = torch.norm(W_A, p='fro') * torch.norm(W_B, p='fro')\n",
        "    return (numerator / denominator).item()\n",
        "```\n",
        "\n",
        "#### Example Usage\n",
        "\n",
        "```python\n",
        "# Get OV circuit of head 3 in layer 0\n",
        "W_V0, W_O0 = get_VO_matrices(model, layer_idx=0, head_idx=3)\n",
        "W_OV0 = W_V0 @ W_O0  # Shape: [d_model, d_model]\n",
        "\n",
        "# Get QK circuit of head 5 in layer 1\n",
        "W_Q1, W_K1 = get_QK_matrices(model, layer_idx=1, head_idx=5)\n",
        "W_QK1 = W_Q1 @ W_K1.T  # Shape: [d_model, d_model]\n",
        "\n",
        "# Compute composition score\n",
        "comp_score = get_composition_score(W_OV0, W_QK1)\n",
        "print(f\"Composition Score between Layer 0 Head 3 and Layer 1 Head 5: {comp_score:.4f}\")\n",
        "```\n",
        "\n",
        "**Interpreting the Score:**\n",
        "\n",
        "- A higher composition score suggests that the two heads are interacting significantly.\n",
        "- We can compute composition scores for all pairs of heads to identify significant interactions.\n",
        "\n",
        "### Computing Composition Scores for All Head Pairs\n",
        "\n",
        "```python\n",
        "num_layers = len(model.transformer.layers)\n",
        "num_heads = model.transformer.layers[0].self_attn.num_heads\n",
        "\n",
        "# Initialize a matrix to hold composition scores\n",
        "composition_scores = np.zeros((num_heads, num_heads))\n",
        "\n",
        "for head_idx0 in range(num_heads):\n",
        "    W_V0, W_O0 = get_VO_matrices(model, layer_idx=0, head_idx=head_idx0)\n",
        "    W_OV0 = W_V0 @ W_O0  # Shape: [d_model, d_model]\n",
        "\n",
        "    for head_idx1 in range(num_heads):\n",
        "        W_Q1, W_K1 = get_QK_matrices(model, layer_idx=1, head_idx=head_idx1)\n",
        "        W_QK1 = W_Q1 @ W_K1.T  # Shape: [d_model, d_model]\n",
        "\n",
        "        # Compute composition score\n",
        "        comp_score = get_composition_score(W_OV0, W_QK1)\n",
        "        composition_scores[head_idx0, head_idx1] = comp_score\n",
        "\n",
        "# Plot the composition scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(composition_scores, cmap='plasma')\n",
        "plt.colorbar()\n",
        "plt.title('Composition Scores Between Layer 0 and Layer 1 Heads')\n",
        "plt.xlabel('Layer 1 Heads')\n",
        "plt.ylabel('Layer 0 Heads')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- Look for pairs with high composition scores.\n",
        "- These pairs are candidates for being involved in induction circuits.\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"visualization\"></a>\n",
        "## 8. Visualizing and Interpreting Results\n",
        "\n",
        "### Attention Pattern Visualization\n",
        "\n",
        "Revisiting the attention patterns, we can overlay the positions where high attention occurs with the tokens in the sequence.\n",
        "\n",
        "Suppose we have stroke sequences and corresponding context text; we can map attention weights back to the strokes.\n",
        "\n",
        "**Note:** The actual implementation would depend on how strokes and tokens are represented in your model.\n",
        "\n",
        "### Analyzing Specific Sequences\n",
        "\n",
        "We can select specific sequences where induction behavior is expected, such as sequences where certain patterns repeat.\n",
        "\n",
        "```python\n",
        "# Example: Select a sequence with repeating patterns\n",
        "sequence_idx = 0  # Adjust as needed\n",
        "inputs, targets = test_dataset[sequence_idx]\n",
        "inputs = inputs.unsqueeze(0).to(device)\n",
        "\n",
        "# Run the model and extract attention patterns\n",
        "def extract_attention_patterns(module, input, output):\n",
        "    attention_patterns.append(output.detach().cpu())\n",
        "\n",
        "attention_patterns = []\n",
        "hook_handle = model.transformer.layers[0].self_attn.attn_drop.register_forward_hook(extract_attention_patterns)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "hook_handle.remove()\n",
        "\n",
        "# Process and visualize the attention patterns as before\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"conclusion\"></a>\n",
        "## 9. Conclusion\n",
        "\n",
        "By adapting methods from Neel Nanda's notebook, we've attempted to detect induction heads in the `cursivetransformer` model. Through analyzing attention patterns, computing QK and OV circuits, and calculating composition scores, we've identified candidate heads that may function as induction heads.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "- **Validate Findings:** Perform further experiments to confirm the induction behavior, such as ablation studies.\n",
        "- **Interpret Features:** Investigate what specific features the induction heads are focusing on, possibly relating to specific strokes or handwriting styles.\n",
        "- **Extend Analysis:** Apply similar methods to other components or layers of the model to gain deeper insights.\n",
        "\n",
        "---\n",
        "\n",
        "**References:**\n",
        "\n",
        "- Neel Nanda's work on reverse-engineering induction circuits.\n",
        "- [Transformer Circuits Thread](https://transformer-circuits.pub/2021/framework/index.html)\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** The actual implementation details may vary based on the specific architecture and data representations used in the `cursivetransformer` model. Adjustments may be necessary to accommodate differences."
      ],
      "metadata": {
        "id": "82mTIzE68FXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM5XGlaS8EaN"
      },
      "outputs": [],
      "source": []
    }
  ]
}