{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOW+8xxrrMtadGUiq8bhlaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HookedCursiveTransformer"
      ],
      "metadata": {
        "id": "9N5SAaDHtG0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "6hl6g0b0fz2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "import os\n",
        "import einops\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import types\n",
        "import matplotlib.pyplot as plt\n",
        "import circuitsvis as cv\n",
        "from IPython.display import display\n",
        "import seaborn as sns\n",
        "\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint, get_latest_checkpoint_artifact\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.utils import get_act_name\n",
        "\n",
        "from transformer_lens.utilities.attention import simple_attn_linear\n",
        "\n",
        "from typing import Callable, Dict, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "from transformer_lens.components import (\n",
        "    Attention,\n",
        "    GroupedQueryAttention,\n",
        "    LayerNorm,\n",
        "    LayerNormPre,\n",
        "    RMSNorm,\n",
        "    RMSNormPre,\n",
        "    Embed,\n",
        "    Unembed\n",
        ")\n",
        "from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP\n",
        "# from transformer_lens.factories.mlp_factory import MLPFactory\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCacheEntry\n",
        "from transformer_lens.utils import repeat_along_head_dimension\n",
        "\n",
        "from typing import Dict, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from jaxtyping import Float\n",
        "\n",
        "from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "from transformer_lens.utilities.addmm import batch_addmm"
      ],
      "metadata": {
        "id": "fZ-PLgwnfu9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()"
      ],
      "metadata": {
        "id": "JyoojEqSFrGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HookedCursiveTransformerConfig(HookedTransformerConfig):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Extract custom arguments\n",
        "        self.d_model_c = kwargs.pop('d_model_c', None)\n",
        "        self.context_block_size = kwargs.pop('context_block_size', None)\n",
        "        self.context_vocab_size = kwargs.pop('context_vocab_size', None)\n",
        "        self.use_cross_attention = kwargs.pop('use_cross_attention', True)\n",
        "\n",
        "        # W&B specific parameters\n",
        "        self.wandb_entity = kwargs.pop('wandb_entity', None)\n",
        "        self.wandb_project = kwargs.pop('wandb_project', None)\n",
        "        self.load_from_run_id = kwargs.pop('load_from_run_id', None)\n",
        "\n",
        "        # Now, call the superclass constructor with the remaining kwargs\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict):\n",
        "        return cls(**config_dict)\n",
        "\n",
        "class HookedCursiveTransformer(HookedTransformer):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embed = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
        "        self.pos_embed = nn.Embedding(cfg.n_ctx, cfg.d_model)\n",
        "        self.embed_c = nn.Embedding(cfg.context_vocab_size, cfg.d_model_c)\n",
        "        self.pos_embed_c = nn.Embedding(cfg.context_block_size, cfg.d_model_c)\n",
        "\n",
        "        # Projection layer if d_model_c != d_model\n",
        "        if cfg.d_model_c != cfg.d_model:\n",
        "            self.context_proj = nn.Linear(cfg.d_model_c, cfg.d_model)\n",
        "        else:\n",
        "            self.context_proj = nn.Identity()\n",
        "\n",
        "        # Blocks\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg, block_index=_) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        # Final layers\n",
        "        self.ln_final = nn.LayerNorm(cfg.d_model)\n",
        "        self.unembed = nn.Linear(cfg.d_model, cfg.d_vocab, bias=True)\n",
        "\n",
        "        # Hook points\n",
        "        self.hook_embed = HookPoint()\n",
        "        self.hook_pos_embed = HookPoint()\n",
        "        self.hook_embed_c = HookPoint()\n",
        "        self.hook_pos_embed_c = HookPoint()\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def forward(self, tokens, context, return_type=\"logits\"):\n",
        "        B, T = tokens.shape\n",
        "        B_c, T_c = context.shape\n",
        "\n",
        "        token_embed = self.hook_embed(self.embed(tokens))\n",
        "        pos_embed = self.hook_pos_embed(self.pos_embed(torch.arange(T, device=tokens.device)))\n",
        "        x = token_embed + pos_embed\n",
        "\n",
        "        context_embed = self.hook_embed_c(self.embed_c(context))\n",
        "        context_pos_embed = self.hook_pos_embed_c(self.pos_embed_c(torch.arange(T_c, device=context.device)))\n",
        "        c = context_embed + context_pos_embed\n",
        "        c = self.context_proj(c)\n",
        "\n",
        "        # Generate causal mask\n",
        "        causal_mask = torch.tril(torch.ones((T, T), device=tokens.device)).unsqueeze(0).unsqueeze(0)\n",
        "        causal_mask = causal_mask.to(dtype=torch.bool)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c, attention_mask=causal_mask)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.unembed(x)\n",
        "\n",
        "        if return_type == \"logits\":\n",
        "            return logits\n",
        "        elif return_type == \"loss\":\n",
        "            return self.loss_fn(logits, tokens[:, 1:])\n",
        "        elif return_type == \"both\":\n",
        "            return logits, self.loss_fn(logits, tokens[:, 1:])\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid return_type {return_type}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        model_name: str,\n",
        "        cfg,\n",
        "        tokenizer=None,\n",
        "        **from_pretrained_kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a pretrained CursiveTransformer model into the HookedCursiveTransformer format.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name or path of the pretrained model.\n",
        "            cfg: The configuration object for the model.\n",
        "            tokenizer: The tokenizer to use (optional).\n",
        "            **from_pretrained_kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            HookedCursiveTransformer: The loaded model.\n",
        "        \"\"\"\n",
        "        print(f\"Loading pretrained model {model_name}\")\n",
        "\n",
        "        # Initialize the HookedCursiveTransformer with the given config\n",
        "        model = cls(cfg)\n",
        "\n",
        "        # Load the state dict from the wandb artifact\n",
        "        state_dict = cls.load_state_dict_from_wandb(cfg)\n",
        "\n",
        "        # Convert the state dict to match HookedCursiveTransformer format\n",
        "        converted_state_dict = cls.convert_cursivetransformer_weights(state_dict, cfg)\n",
        "\n",
        "        # Load the converted state dict into the model\n",
        "        model.load_state_dict(converted_state_dict, strict=False)\n",
        "\n",
        "        if tokenizer is not None:\n",
        "            model.tokenizer = tokenizer\n",
        "\n",
        "        print(f\"Successfully loaded pretrained model {model_name}\")\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def load_state_dict_from_wandb(args):\n",
        "        artifact = get_latest_checkpoint_artifact(args)\n",
        "        artifact_dir = artifact.download()\n",
        "        checkpoint = torch.load(os.path.join(artifact_dir, \"best_checkpoint.pt\"), weights_only=True)\n",
        "        return checkpoint['model_state_dict']\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_cursivetransformer_weights(state_dict, cfg):\n",
        "        \"\"\"Convert CursiveTransformer weights to HookedCursiveTransformer format.\"\"\"\n",
        "        new_state_dict = {}\n",
        "\n",
        "        # Embeddings\n",
        "        new_state_dict[\"embed.W_E\"] = state_dict[\"transformer.wte.weight\"]\n",
        "        new_state_dict[\"pos_embed.W_pos\"] = state_dict[\"transformer.wpe.weight\"]\n",
        "        new_state_dict[\"embed_c.W_E\"] = state_dict[\"transformer.wce.weight\"]\n",
        "        new_state_dict[\"pos_embed_c.W_pos\"] = state_dict[\"transformer.wcpe.weight\"]\n",
        "\n",
        "        for l in range(cfg.n_layers):\n",
        "            # Layer Norms\n",
        "            new_state_dict[f'blocks.{l}.ln1.w'] = state_dict[f'transformer.h.{l}.ln_1.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln1.b'] = state_dict[f'transformer.h.{l}.ln_1.bias']\n",
        "            new_state_dict[f'blocks.{l}.ln2.w'] = state_dict[f'transformer.h.{l}.ln_2.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln2.b'] = state_dict[f'transformer.h.{l}.ln_2.bias']\n",
        "            new_state_dict[f'blocks.{l}.ln3.w'] = state_dict[f'transformer.h.{l}.ln_3.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln3.b'] = state_dict[f'transformer.h.{l}.ln_3.bias']\n",
        "\n",
        "            # Self-Attention\n",
        "            W_qkv = state_dict[f'transformer.h.{l}.attn.c_attn.weight']\n",
        "            b_qkv = state_dict[f'transformer.h.{l}.attn.c_attn.bias']\n",
        "            W_q, W_k, W_v = W_qkv.t().chunk(3, dim=1)\n",
        "            b_q, b_k, b_v = b_qkv.chunk(3, dim=0)\n",
        "\n",
        "            new_state_dict[f'blocks.{l}.attn.W_Q'] = W_q.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.W_K'] = W_k.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.W_V'] = W_v.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_Q'] = b_q.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_K'] = b_k.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_V'] = b_v.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_o = state_dict[f'transformer.h.{l}.attn.c_proj.weight']\n",
        "            new_state_dict[f'blocks.{l}.attn.W_O'] = W_o.t().reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_O'] = state_dict[f'transformer.h.{l}.attn.c_proj.bias']\n",
        "\n",
        "            # Cross-Attention\n",
        "            W_q = state_dict[f'transformer.h.{l}.cross_attn.c_attn_q.weight']\n",
        "            b_q = state_dict[f'transformer.h.{l}.cross_attn.c_attn_q.bias']\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_Q'] = W_q.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_Q'] = b_q.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_kv = state_dict[f'transformer.h.{l}.cross_attn.c_attn_kv.weight']\n",
        "            b_kv = state_dict[f'transformer.h.{l}.cross_attn.c_attn_kv.bias']\n",
        "            W_k, W_v = W_kv.t().chunk(2, dim=1)\n",
        "            b_k, b_v = b_kv.chunk(2, dim=0)\n",
        "\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_K'] = W_k.t().reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_V'] = W_v.t().reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_K'] = b_k.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_V'] = b_v.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_o = state_dict[f'transformer.h.{l}.cross_attn.c_proj.weight']\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_O'] = W_o.t().reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_O'] = state_dict[f'transformer.h.{l}.cross_attn.c_proj.bias']\n",
        "\n",
        "            # MLP\n",
        "            new_state_dict[f'blocks.{l}.mlp.W_in'] = state_dict[f'transformer.h.{l}.mlp.c_fc.weight'].t()\n",
        "            new_state_dict[f'blocks.{l}.mlp.b_in'] = state_dict[f'transformer.h.{l}.mlp.c_fc.bias']\n",
        "            new_state_dict[f'blocks.{l}.mlp.W_out'] = state_dict[f'transformer.h.{l}.mlp.c_proj.weight'].t()\n",
        "            new_state_dict[f'blocks.{l}.mlp.b_out'] = state_dict[f'transformer.h.{l}.mlp.c_proj.bias']\n",
        "\n",
        "        # Final layer norm and unembedding\n",
        "        new_state_dict[\"ln_final.w\"] = state_dict[\"transformer.ln_f.weight\"]\n",
        "        new_state_dict[\"ln_final.b\"] = state_dict[\"transformer.ln_f.bias\"]\n",
        "        new_state_dict[\"unembed.W_U\"] = state_dict[\"lm_head.weight\"].t()\n",
        "        new_state_dict[\"unembed.b_U\"] = state_dict.get(\"lm_head.bias\", torch.zeros(cfg.d_vocab))\n",
        "\n",
        "        return new_state_dict\n",
        "\n",
        "    def loss_fn(self, logits, targets):\n",
        "        return F.cross_entropy(logits[:, :-1, :].reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-1)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: Union[Dict, HookedTransformerConfig], block_index):\n",
        "        super().__init__()\n",
        "        self.cfg = HookedCursiveTransformerConfig.unwrap(cfg)\n",
        "\n",
        "        # Determine normalization type\n",
        "        if self.cfg.normalization_type == \"LN\":\n",
        "            normalization_layer = LayerNorm\n",
        "        elif self.cfg.normalization_type == \"LNPre\":\n",
        "            normalization_layer = LayerNormPre\n",
        "        elif self.cfg.normalization_type == \"RMS\":\n",
        "            normalization_layer = RMSNorm\n",
        "        elif self.cfg.normalization_type == \"RMSPre\":\n",
        "            normalization_layer = RMSNormPre\n",
        "        elif self.cfg.normalization_type is None:\n",
        "            normalization_layer = lambda cfg: nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid normalization_type: {self.cfg.normalization_type}\")\n",
        "\n",
        "        # Initialize layers\n",
        "        self.ln1 = normalization_layer(cfg)\n",
        "        self.ln2 = normalization_layer(cfg)\n",
        "        self.ln3 = normalization_layer(cfg)\n",
        "\n",
        "        attention_class = Attention\n",
        "        cross_attention_class = CrossAttention\n",
        "        self.attn = attention_class(self.cfg, \"global\", block_index)\n",
        "        self.cross_attn = cross_attention_class(self.cfg, \"global\", block_index)\n",
        "        self.mlp = MLPFactory.create_mlp(self.cfg)\n",
        "\n",
        "        # Hook points\n",
        "        self.hook_attn_in = HookPoint()\n",
        "        self.hook_cross_attn_in = HookPoint()\n",
        "        self.hook_mlp_in = HookPoint()\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_cross_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid1 = HookPoint()\n",
        "        self.hook_resid_mid2 = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "        context: Float[torch.Tensor, \"batch context_len d_model_c\"],\n",
        "        attention_mask: Optional[Int[torch.Tensor, \"batch 1 seq_len seq_len\"]] = None,\n",
        "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "        resid_pre = self.hook_resid_pre(resid_pre)\n",
        "\n",
        "        # Self-attention\n",
        "        attn_in = self.hook_attn_in(resid_pre)\n",
        "        normalized_attn_in = self.ln1(attn_in)\n",
        "        attn_out = self.attn(\n",
        "            query_input=normalized_attn_in,\n",
        "            key_input=normalized_attn_in,\n",
        "            value_input=normalized_attn_in,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        attn_out = self.hook_attn_out(attn_out)\n",
        "        resid_mid1 = self.hook_resid_mid1(resid_pre + attn_out)\n",
        "\n",
        "        # Cross-attention\n",
        "        cross_attn_in = self.hook_cross_attn_in(resid_mid1)\n",
        "        normalized_cross_attn_in = self.ln2(cross_attn_in)\n",
        "        cross_attn_out = self.cross_attn(\n",
        "            query_input=normalized_cross_attn_in,\n",
        "            key_input=context,\n",
        "            value_input=context,\n",
        "            attention_mask=None,  # No mask for cross-attention\n",
        "        )\n",
        "        cross_attn_out = self.hook_cross_attn_out(cross_attn_out)\n",
        "        resid_mid2 = self.hook_resid_mid2(resid_mid1 + cross_attn_out)\n",
        "\n",
        "        # MLP\n",
        "        mlp_in = self.hook_mlp_in(resid_mid2)\n",
        "        normalized_mlp_in = self.ln3(mlp_in)\n",
        "        mlp_out = self.mlp(normalized_mlp_in)\n",
        "        mlp_out = self.hook_mlp_out(mlp_out)\n",
        "\n",
        "        resid_post = self.hook_resid_post(resid_mid2 + mlp_out)\n",
        "        return resid_post\n",
        "\n",
        "class CrossAttention(Attention):\n",
        "    def __init__(self, cfg, attn_type='global', layer_id=None):\n",
        "        super().__init__(cfg, attn_type, layer_id)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Initialize W_K and W_V for cross-attention with d_model_c\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.empty(\n",
        "                self.cfg.n_heads, self.cfg.d_model_c, self.cfg.d_head, dtype=self.cfg.dtype\n",
        "            )\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.empty(\n",
        "                self.cfg.n_heads, self.cfg.d_model_c, self.cfg.d_head, dtype=self.cfg.dtype\n",
        "            )\n",
        "        )\n",
        "        self.b_K = nn.Parameter(\n",
        "            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)\n",
        "        )\n",
        "        self.b_V = nn.Parameter(\n",
        "            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)\n",
        "        )\n",
        "        # W_Q and W_O are inherited from AbstractAttention\n",
        "\n",
        "        # Override the attention direction\n",
        "        self.cfg.attention_dir = 'bidirectional'\n",
        "\n",
        "    def calculate_qkv_matrices(self, query_input, key_input, value_input):\n",
        "        # query_input: [batch, pos, d_model]\n",
        "        # key_input and value_input: [batch, kv_pos, d_model_c]\n",
        "\n",
        "        # Use W_Q and b_Q from AbstractAttention for queries\n",
        "        q = self.hook_q(simple_attn_linear(query_input, self.W_Q, self.b_Q))\n",
        "        # Use custom W_K and b_K for keys with d_model_c\n",
        "        k = self.hook_k(simple_attn_linear(key_input, self.W_K, self.b_K))\n",
        "        v = self.hook_v(simple_attn_linear(value_input, self.W_V, self.b_V))\n",
        "        return q, k, v\n",
        "\n",
        "\n",
        "class MLPFactory:\n",
        "    @staticmethod\n",
        "    def create_mlp(cfg):\n",
        "        return MLP(cfg)\n",
        "\n",
        "class MLP(CanBeUsedAsMLP):\n",
        "    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):\n",
        "        super().__init__(cfg)\n",
        "        self.select_activation_function()\n",
        "\n",
        "        self.W_in = nn.Parameter(torch.empty(self.cfg.d_model, self.d_mlp, dtype=self.cfg.dtype))\n",
        "        self.b_in = nn.Parameter(torch.zeros(self.d_mlp, dtype=self.cfg.dtype))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.empty(self.d_mlp, self.cfg.d_model, dtype=self.cfg.dtype))\n",
        "        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))\n",
        "\n",
        "        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]\n",
        "        self.hook_post = HookPoint()  # [batch, pos, d_mlp]\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[torch.Tensor, \"batch pos d_model\"]\n",
        "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "        # This is equivalent to (roughly) W_in @ x + b_in. It's important to\n",
        "        # use a fused addmm to ensure it matches the Huggingface implementation\n",
        "        # exactly.\n",
        "        pre_act = self.hook_pre(batch_addmm(self.b_in, self.W_in, x))  # [batch, pos, d_mlp]\n",
        "\n",
        "        if (\n",
        "            self.cfg.is_layer_norm_activation()\n",
        "            and self.hook_mid is not None\n",
        "            and self.ln is not None\n",
        "        ):\n",
        "            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp]\n",
        "            post_act = self.hook_post(self.ln(mid_act))\n",
        "        else:\n",
        "            post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp]\n",
        "        return batch_addmm(self.b_out, self.W_out, post_act)\n",
        "\n",
        "\n",
        "def convert_cursivetransformer_model_config(args):\n",
        "    cfg_dict = {\n",
        "        # Standard parameters\n",
        "        \"d_model\": args.n_embd,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embd * 4,\n",
        "        \"d_head\": args.n_embd // args.n_ctx_head,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": args.vocab_size,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": \"gelu_new\",\n",
        "        \"attn_only\": False,\n",
        "        \"final_rms\": False,\n",
        "        \"attention_dir\": \"causal\",\n",
        "        \"original_architecture\": \"cursivetransformer\",\n",
        "        \"normalization_type\": \"LN\",\n",
        "        \"init_weights\": False,\n",
        "        \"device\": args.device,\n",
        "        # Additional parameters for cross-attention\n",
        "        \"d_model_c\": args.n_embd2,\n",
        "        \"context_block_size\": args.context_block_size,\n",
        "        \"context_vocab_size\": args.context_vocab_size,\n",
        "        \"use_cross_attention\": True,\n",
        "        # W&B specific parameters\n",
        "        \"wandb_entity\": args.wandb_entity,\n",
        "        \"wandb_project\": args.wandb_project,\n",
        "        \"load_from_run_id\": args.load_from_run_id,\n",
        "    }\n",
        "    return HookedCursiveTransformerConfig.from_dict(cfg_dict)"
      ],
      "metadata": {
        "id": "IRwfr66QBHg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "model = HookedCursiveTransformer.from_pretrained(\"cursivetransformer\", cfg)"
      ],
      "metadata": {
        "id": "tA5DOgaDFbtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add batch dimension to your inputs\n",
        "x, c, y = test_dataset[0]\n",
        "x = x.unsqueeze(0) # Shape: [1, 1000]\n",
        "c = c.unsqueeze(0) # Shape: [1, 50]\n",
        "y = y.unsqueeze(0) # Shape: [1, 1000]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits, cache = model.run_with_cache(x, c, return_type=\"both\")\n",
        "\n",
        "# Choose the layer and head to visualize\n",
        "layer = 0  # Change to the desired layer index\n",
        "head = 2  # Change to the desired head index\n",
        "\n",
        "# Access attention patterns for the specified layer\n",
        "attn_patterns = cache[f'blocks.{layer}.attn.hook_pattern']  # Shape: [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "# Extract the attention pattern for the specified head and sample\n",
        "attn = attn_patterns[0, head].cpu().numpy()  # Shape: [seq_len, seq_len]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn, cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title(f'Self-Attention Pattern for Layer {layer}, Head {head}')\n",
        "plt.xlabel('Key Positions')\n",
        "plt.ylabel('Query Positions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QAuXU8fafyiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}