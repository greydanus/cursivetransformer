{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMxSoC4SS+jsQicNyPJ5OCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HookedCursiveTransformer"
      ],
      "metadata": {
        "id": "9N5SAaDHtG0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jOa1OkwOABq"
      },
      "outputs": [],
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "\n",
        "# Import TransformerLens modules\n",
        "\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Union\n",
        "\n",
        "from transformer_lens import HookedTransformer, ActivationCache\n",
        "from transformer_lens.components import Attention, MLP\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import einops\n",
        "from einops import rearrange, einsum\n",
        "\n",
        "# Move input to the correct device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "g9Mm8fSiwc-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HookedCursiveTransformerConfig(HookedTransformerConfig):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Extract custom arguments\n",
        "        self.d_model_c = kwargs.pop('d_model_c', None)\n",
        "        self.n_ctx_c = kwargs.pop('n_ctx_c', None)\n",
        "        self.d_vocab_c = kwargs.pop('d_vocab_c', None)\n",
        "        self.use_cross_attention = kwargs.pop('use_cross_attention', True)\n",
        "\n",
        "        # Now, call the superclass constructor with the remaining kwargs\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Set default values if necessary\n",
        "        if self.d_model_c is None:\n",
        "            self.d_model_c = self.d_model\n",
        "        if self.n_ctx_c is None:\n",
        "            self.n_ctx_c = self.n_ctx\n",
        "        if self.d_vocab_c is None:\n",
        "            self.d_vocab_c = self.d_vocab\n",
        "\n",
        "# - [ ] TODO: Change this to inherit from AbstractAttention: https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/components/abstract_attention.py\n",
        "class HookedAttention(nn.Module):\n",
        "    def __init__(self, cfg, is_cross_attention=False, layer_idx=None):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.is_cross_attention = is_cross_attention\n",
        "        self.layer_idx = layer_idx\n",
        "        self.W_Q = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
        "        if is_cross_attention:\n",
        "            self.W_K = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model_c, cfg.d_head))\n",
        "            self.W_V = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model_c, cfg.d_head))\n",
        "        else:\n",
        "            self.W_K = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
        "            self.W_V = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
        "        self.W_O = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
        "        # Biases\n",
        "        self.b_Q = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_K = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_V = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_O = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "        # Register hooks if needed\n",
        "\n",
        "    def forward(self, x, context=None, **kwargs):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        if self.is_cross_attention:\n",
        "            if context is None:\n",
        "                raise ValueError(\"Context must be provided for cross-attention\")\n",
        "            k_input = context\n",
        "            v_input = context\n",
        "        else:\n",
        "            k_input = x\n",
        "            v_input = x\n",
        "\n",
        "        # Compute queries, keys, values\n",
        "        q = einsum(x, self.W_Q, 'b t d, h d e -> b h t e') + self.b_Q  # [batch, n_heads, seq_len, d_head]\n",
        "        k = einsum(k_input, self.W_K, 'b s d, h d e -> b h s e') + self.b_K  # [batch, n_heads, seq_len_k, d_head]\n",
        "        v = einsum(v_input, self.W_V, 'b s d, h d e -> b h s e') + self.b_V  # [batch, n_heads, seq_len_v, d_head]\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = einsum(q, k, 'b h t e, b h s e -> b h t s') / math.sqrt(self.cfg.d_head)\n",
        "\n",
        "        # Apply causal mask if needed (for self-attention)\n",
        "        if not self.is_cross_attention and self.cfg.attn_only:\n",
        "            attn_scores = self.apply_causal_mask(attn_scores)\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attn_probs = nn.functional.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Save attention patterns for analysis\n",
        "        if kwargs.get('cache', None) is not None:\n",
        "            cache = kwargs['cache']\n",
        "            hook_name = f'blocks.{self.layer_idx}.{\"cross_attn\" if self.is_cross_attention else \"attn\"}.hook_pattern'\n",
        "            cache[hook_name] = attn_probs.detach()\n",
        "\n",
        "        # Compute attention output\n",
        "        attn_output = einsum(attn_probs, v, 'b h t s, b h s e -> b h t e')\n",
        "        attn_output = einsum(attn_output, self.W_O, 'b h t e, h e d -> b t d') + self.b_O  # [batch, seq_len, d_model]\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class CursiveTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg, layer_idx):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Layer norms\n",
        "        self.ln1 = nn.LayerNorm(cfg.d_model)\n",
        "        self.ln2 = nn.LayerNorm(cfg.d_model)\n",
        "        self.ln3 = nn.LayerNorm(cfg.d_model)\n",
        "\n",
        "        # Self-attention using Attention\n",
        "        self.attn = HookedAttention(cfg, layer_idx=layer_idx)\n",
        "\n",
        "        # Cross-attention using modified HookedAttention\n",
        "        self.cross_attn = HookedAttention(cfg, is_cross_attention=True, layer_idx=layer_idx)\n",
        "\n",
        "        # MLP\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, x, c, **kwargs):\n",
        "        # Self-attention\n",
        "        x = x + self.attn(self.ln1(x), **kwargs)\n",
        "\n",
        "        # Cross-attention\n",
        "        x = x + self.cross_attn(self.ln2(x), context=c, **kwargs)\n",
        "\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.ln3(x), **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "class HookedCursiveTransformer(HookedTransformer):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Embedding layers for the context input\n",
        "        self.embed_c = nn.Embedding(cfg.d_vocab_c, cfg.d_model_c)\n",
        "        self.pos_embed_c = nn.Embedding(cfg.n_ctx_c, cfg.d_model_c)\n",
        "\n",
        "        # Override the transformer blocks with custom blocks that include cross-attention\n",
        "        self.blocks = nn.ModuleList([\n",
        "            CursiveTransformerBlock(self.cfg, layer_idx)\n",
        "            for layer_idx in range(self.cfg.n_layers)\n",
        "        ])\n",
        "\n",
        "        # Update the final layer norm to match your model\n",
        "        self.ln_final = nn.LayerNorm(self.cfg.d_model)\n",
        "\n",
        "    def forward(self, x, c, return_type=\"logits\", **kwargs):\n",
        "        # Embedding and positional encoding for x\n",
        "        x_tokens = self.embed(x)  # [batch, seq_len, d_model]\n",
        "        x_positions = self.pos_embed(torch.arange(x_tokens.size(1), device=x_tokens.device))  # [seq_len, d_model]\n",
        "        x = x_tokens + x_positions\n",
        "\n",
        "        # Embedding and positional encoding for c\n",
        "        c_tokens = self.embed_c(c)  # [batch, context_len, d_model_c]\n",
        "        c_positions = self.pos_embed_c(torch.arange(c_tokens.size(1), device=c_tokens.device))  # [context_len, d_model_c]\n",
        "        c = c_tokens + c_positions\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c, **kwargs)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.unembed(x)\n",
        "\n",
        "        if return_type == \"logits\":\n",
        "            return logits\n",
        "        elif return_type == \"loss\":\n",
        "            targets = kwargs.get('targets')\n",
        "            if targets is None:\n",
        "                raise ValueError(\"Targets must be provided when return_type is 'loss'\")\n",
        "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return loss\n",
        "        elif return_type == \"both\":\n",
        "            return logits, kwargs.get('cache', None)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid return_type: {return_type}\")\n",
        "\n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "        # Implement causal masking logic here if needed\n",
        "        return attn_scores"
      ],
      "metadata": {
        "id": "wgEl2dfWwcpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "NrzTSEMksyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_cursivetransformer_model_config(args):\n",
        "    cfg_dict = {\n",
        "        # Standard parameters\n",
        "        \"d_model\": args.n_embd,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embd * 4,\n",
        "        \"d_head\": args.n_embd // args.n_ctx_head,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": args.vocab_size,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": \"gelu_new\",\n",
        "        \"attn_only\": False,\n",
        "        \"final_rms\": False,\n",
        "        \"original_architecture\": \"cursivetransformer\",\n",
        "        \"normalization_type\": \"LN\",\n",
        "        \"init_weights\": False,\n",
        "        \"device\": args.device,\n",
        "        # Additional parameters for cross-attention\n",
        "        \"d_model_c\": args.n_embd2,\n",
        "        \"n_ctx_c\": args.context_block_size,\n",
        "        \"d_vocab_c\": args.context_vocab_size,\n",
        "        \"use_cross_attention\": True,\n",
        "    }\n",
        "    cfg = HookedCursiveTransformerConfig.from_dict(cfg_dict)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def fill_missing_keys(model, state_dict):\n",
        "    \"\"\"Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.\n",
        "\n",
        "    This function is assumed to be run before weights are initialized.\n",
        "\n",
        "    Args:\n",
        "        state_dict (dict): State dict from a pretrained model\n",
        "\n",
        "    Returns:\n",
        "        dict: State dict with missing keys filled in\n",
        "    \"\"\"\n",
        "    # Get the default state dict\n",
        "    default_state_dict = model.state_dict()\n",
        "    # Get the keys that are missing from the pretrained model\n",
        "    missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())\n",
        "    # Fill in the missing keys with the default initialization\n",
        "    for key in missing_keys:\n",
        "        if \"hf_model\" in key:\n",
        "            # Skip keys that are from the HuggingFace model, if loading from HF.\n",
        "            continue\n",
        "        if \"W_\" in key:\n",
        "            logging.warning(\n",
        "                \"Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}\".format(\n",
        "                    key\n",
        "                )\n",
        "            )\n",
        "        state_dict[key] = default_state_dict[key]\n",
        "    return state_dict\n",
        "\n",
        "def convert_cursivetransformer_weights(cursivetransformer, cfg):\n",
        "    state_dict = {}\n",
        "    # Embeddings\n",
        "    state_dict[\"embed.W_E\"] = cursivetransformer.transformer.wte.weight\n",
        "    state_dict[\"pos_embed.W_pos\"] = cursivetransformer.transformer.wpe.weight\n",
        "    state_dict[\"embed_c.weight\"] = cursivetransformer.transformer.wce.weight\n",
        "    state_dict[\"pos_embed_c.weight\"] = cursivetransformer.transformer.wcpe.weight\n",
        "\n",
        "    for l in range(cfg.n_layers):\n",
        "        block = cursivetransformer.transformer.h[l]\n",
        "\n",
        "        # Layer Norms\n",
        "        state_dict[f'blocks.{l}.ln1.weight'] = block.ln_1.weight\n",
        "        state_dict[f'blocks.{l}.ln1.bias'] = block.ln_1.bias\n",
        "        state_dict[f'blocks.{l}.ln2.weight'] = block.ln_2.weight\n",
        "        state_dict[f'blocks.{l}.ln2.bias'] = block.ln_2.bias\n",
        "        state_dict[f'blocks.{l}.ln3.weight'] = block.ln_3.weight\n",
        "        state_dict[f'blocks.{l}.ln3.bias'] = block.ln_3.bias\n",
        "\n",
        "        # Self-Attention Weights\n",
        "        self_attn = block.attn\n",
        "        W_self = self_attn.c_attn.weight  # Shape: [3 * d_model, d_model]\n",
        "        b_self = self_attn.c_attn.bias    # Shape: [3 * d_model]\n",
        "\n",
        "        # Split weights and biases\n",
        "        W_Q_self, W_K_self, W_V_self = torch.chunk(W_self, 3, dim=0)\n",
        "        b_Q_self, b_K_self, b_V_self = torch.chunk(b_self, 3, dim=0)\n",
        "\n",
        "        # Reshape and assign\n",
        "        W_Q_self = W_Q_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_K_self = W_K_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_V_self = W_V_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.W_Q'] = W_Q_self\n",
        "        state_dict[f'blocks.{l}.attn.W_K'] = W_K_self\n",
        "        state_dict[f'blocks.{l}.attn.W_V'] = W_V_self\n",
        "        state_dict[f'blocks.{l}.attn.b_Q'] = b_Q_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.b_K'] = b_K_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.b_V'] = b_V_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "        # Self-Attention Output Projection\n",
        "        W_O_self = self_attn.c_proj.weight  # Shape: [d_model, d_model]\n",
        "        b_O_self = self_attn.c_proj.bias    # Shape: [d_model]\n",
        "        W_O_self = W_O_self.T.reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "        state_dict[f'blocks.{l}.attn.W_O'] = W_O_self\n",
        "        state_dict[f'blocks.{l}.attn.b_O'] = b_O_self\n",
        "\n",
        "        # Cross-Attention Weights\n",
        "        cross_attn = block.cross_attn\n",
        "        W_Q_cross = cross_attn.c_attn_q.weight  # Shape: [d_model, d_model]\n",
        "        b_Q_cross = cross_attn.c_attn_q.bias    # Shape: [d_model]\n",
        "        W_KV_cross = cross_attn.c_attn_kv.weight  # Shape: [2 * d_model_c, d_model_c]\n",
        "        b_KV_cross = cross_attn.c_attn_kv.bias    # Shape: [2 * d_model_c]\n",
        "\n",
        "        # Split KV weights and biases\n",
        "        W_K_cross, W_V_cross = torch.chunk(W_KV_cross, 2, dim=0)\n",
        "        b_K_cross, b_V_cross = torch.chunk(b_KV_cross, 2, dim=0)\n",
        "\n",
        "        # Reshape and assign\n",
        "        W_Q_cross = W_Q_cross.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_K_cross = W_K_cross.T.reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "        W_V_cross = W_V_cross.T.reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_Q'] = W_Q_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_K'] = W_K_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_V'] = W_V_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_Q'] = b_Q_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_K'] = b_K_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_V'] = b_V_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "        # Cross-Attention Output Projection\n",
        "        W_O_cross = cross_attn.c_proj.weight  # Shape: [d_model, d_model]\n",
        "        b_O_cross = cross_attn.c_proj.bias    # Shape: [d_model]\n",
        "        W_O_cross = W_O_cross.T.reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_O'] = W_O_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_O'] = b_O_cross\n",
        "\n",
        "        # MLP Weights\n",
        "        mlp = block.mlp\n",
        "        W_in = mlp.c_fc.weight  # Shape: [4 * d_model, d_model]\n",
        "        b_in = mlp.c_fc.bias    # Shape: [4 * d_model]\n",
        "        W_out = mlp.c_proj.weight  # Shape: [d_model, 4 * d_model]\n",
        "        b_out = mlp.c_proj.bias    # Shape: [d_model]\n",
        "\n",
        "        # Transpose and assign\n",
        "        state_dict[f'blocks.{l}.mlp.W_in'] = W_in.T\n",
        "        state_dict[f'blocks.{l}.mlp.b_in'] = b_in\n",
        "        state_dict[f'blocks.{l}.mlp.W_out'] = W_out.T\n",
        "        state_dict[f'blocks.{l}.mlp.b_out'] = b_out\n",
        "\n",
        "    # Unembedding Weights\n",
        "    state_dict[\"unembed.W_U\"] = cursivetransformer.lm_head.weight.T\n",
        "    if cursivetransformer.lm_head.bias is not None:\n",
        "        state_dict[\"unembed.b_U\"] = cursivetransformer.lm_head.bias\n",
        "    else:\n",
        "        state_dict[\"unembed.b_U\"] = torch.zeros(cfg.d_vocab_out)\n",
        "\n",
        "    # Final Layer Norm\n",
        "    state_dict[\"ln_final.weight\"] = cursivetransformer.transformer.ln_f.weight\n",
        "    state_dict[\"ln_final.bias\"] = cursivetransformer.transformer.ln_f.bias\n",
        "\n",
        "    return state_dict"
      ],
      "metadata": {
        "id": "qUx3I0jenYhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sandbox"
      ],
      "metadata": {
        "id": "7iB8ma0Fs0pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the configuration has the correct maximum sequence lengths\n",
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "state_dict = convert_cursivetransformer_weights(model, cfg)\n",
        "hooked_model = HookedCursiveTransformer(cfg)\n",
        "hooked_model.load_state_dict(state_dict)\n",
        "hooked_model.to(device)\n",
        "\n",
        "# Add batch dimension to your inputs\n",
        "x, c, y = test_dataset[0]\n",
        "x = x.unsqueeze(0).to(device)  # Shape: [1, 1000]\n",
        "c = c.unsqueeze(0).to(device)  # Shape: [1, 50]\n",
        "y = y.unsqueeze(0).to(device) # Shape: [1, 1000]\n",
        "\n",
        "print(x.shape)  # torch.Size([1, 1000])\n",
        "print(c.shape)  # torch.Size([1, 50])\n",
        "print(y.shape)  # torch.Size([1, 1000])\n",
        "\n",
        "# Run the model with cache\n",
        "hooked_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, cache = hooked_model(x, c, return_type=\"both\")\n",
        "\n",
        "# Choose the layer and head to visualize\n",
        "layer = 0  # Change to the desired layer index\n",
        "head = 0   # Change to the desired head index\n",
        "\n",
        "# Access attention patterns for the specified layer\n",
        "attn_patterns = cache[f'blocks.{layer}.attn.hook_pattern']  # Shape: [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "# Extract the attention pattern for the specified head and sample\n",
        "attn = attn_patterns[0, head].cpu().numpy()  # Shape: [seq_len, seq_len]\n",
        "\n",
        "# Visualize the attention pattern\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn, cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title(f'Self-Attention Pattern for Layer {layer}, Head {head}')\n",
        "plt.xlabel('Key Positions')\n",
        "plt.ylabel('Query Positions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ftYaXgofqArq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9FDMKiN15SIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}