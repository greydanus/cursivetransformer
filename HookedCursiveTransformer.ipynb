{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs5yTqTdW7vtX/N8zDe1wv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/zwimpee/cursivetransformer/issues/26"
      ],
      "metadata": {
        "id": "F1SLPMG7UJcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5jOa1OkwOABq",
        "outputId": "61a293ba-feaf-4f60-8af7-571dfba68202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.34.2)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.44.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading transformer_lens-2.7.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, pyarrow, fancy-einsum, docker-pycreds, dill, beartype, multiprocess, jaxtyping, gitdb, gitpython, wandb, datasets, transformer_lens\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 datasets-3.0.1 dill-0.3.8 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.11 gitpython-3.1.43 jaxtyping-0.2.34 multiprocess-0.70.16 pyarrow-17.0.0 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 transformer_lens-2.7.0 typeguard-2.13.3 wandb-0.18.2 xxhash-3.5.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m292.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.11-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.11-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.11 ruff-0.6.8 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2415, done.\u001b[K\n",
            "remote: Counting objects: 100% (550/550), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 2415 (delta 454), reused 448 (delta 385), pack-reused 1865 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2415/2415), 32.18 MiB | 26.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1368/1368), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 1)) (2.4.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r cursivetransformer/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r cursivetransformer/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "\n",
        "# Import TransformerLens modules\n",
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will find this post particularly useful: https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch\n",
        "As well as this notebook: https://colab.research.google.com/drive/1bZkkJd8pAVnSN23svyszZ3f4WrnYKN_3?usp=sharing"
      ],
      "metadata": {
        "id": "mrGWMzmwQtoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii-WF1Vhc7Qa",
        "outputId": "b25c3201-8591-4c6a-a4a2-dd4a182903f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load dataset file from /content/cursivetransformer/data/bigbank.json.zip\n",
            "Succeeded in loading the bigbank dataset; contains 1900 items.\n",
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Functions"
      ],
      "metadata": {
        "id": "7U_6MxmEZuQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - [ ] TODO: Update this for our config!\n",
        "def convert_cursivetransformer_model_config(official_model_name: str, args, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads the config for a model trained by us (ZachWimpee & SamGreydanus), converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    AutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.\n",
        "    \"\"\"\n",
        "    official_model_name = get_official_model_name(official_model_name)\n",
        "    cfg_arch = \"cursivetransformer\"\n",
        "    cfg_dict = {\n",
        "        \"d_model\": args.n_embed,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embed,\n",
        "        \"d_head\": args.n_embed,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": ,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": ,\n",
        "        \"attn_only\": ,\n",
        "        \"final_rms\": ,\n",
        "        \"original_architecture\": cfg_arch,\n",
        "    }\n",
        "    if \"normalization\" in cfg_json:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization\"]\n",
        "    else:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization_type\"]\n",
        "    if \"shortformer_pos\" in cfg_json:\n",
        "        cfg_dict[\"positional_embedding_type\"] = (\n",
        "            \"shortformer\" if cfg_json[\"shortformer_pos\"] else \"standard\"\n",
        "        )\n",
        "    else:\n",
        "        cfg_dict[\"positional_embedding_type\"] = \"standard\"\n",
        "    return cfg_dict\n",
        "\n",
        "def convert_cursivetransformer_weights(cursivetransformer, cfg: HookedTransformerConfig):\n",
        "    state_dict = {}\n",
        "\n",
        "    state_dict[\"embed.W_E\"] = cursivetransformer.transformer.wte.weight\n",
        "    state_dict[\"pos_embed.W_pos\"] = cursivetransformer.transformer.wpe.weight\n",
        "\n",
        "    for l in range(cfg.n_layers):\n",
        "        state_dict[f\"blocks.{l}.ln1.w\"] = cursivetransformer.transformer.h[l].ln_1.weight\n",
        "        state_dict[f\"blocks.{l}.ln1.b\"] = cursivetransformer.transformer.h[l].ln_1.bias\n",
        "\n",
        "        # In GPT-2, q,k,v are produced by one big linear map, whose output is\n",
        "        # concat([q, k, v])\n",
        "        W = cursivetransformer.transformer.h[l].attn.c_attn.weight\n",
        "        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
        "        W_Q = einops.rearrange(W_Q, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "        W_K = einops.rearrange(W_K, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "        W_V = einops.rearrange(W_V, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "\n",
        "        state_dict[f\"blocks.{l}.attn.W_Q\"] = W_Q\n",
        "        state_dict[f\"blocks.{l}.attn.W_K\"] = W_K\n",
        "        state_dict[f\"blocks.{l}.attn.W_V\"] = W_V\n",
        "\n",
        "        qkv_bias = cursivetransformer.transformer.h[l].attn.c_attn.bias\n",
        "        qkv_bias = einops.rearrange(\n",
        "            qkv_bias,\n",
        "            \"(qkv index head)->qkv index head\",\n",
        "            qkv=3,\n",
        "            index=cfg.n_heads,\n",
        "            head=cfg.d_head,\n",
        "        )\n",
        "        state_dict[f\"blocks.{l}.attn.b_Q\"] = qkv_bias[0]\n",
        "        state_dict[f\"blocks.{l}.attn.b_K\"] = qkv_bias[1]\n",
        "        state_dict[f\"blocks.{l}.attn.b_V\"] = qkv_bias[2]\n",
        "\n",
        "        W_O = cursivetransformer.transformer.h[l].attn.c_proj.weight\n",
        "        W_O = einops.rearrange(W_O, \"(i h) m->i h m\", i=cfg.n_heads)\n",
        "        state_dict[f\"blocks.{l}.attn.W_O\"] = W_O\n",
        "        state_dict[f\"blocks.{l}.attn.b_O\"] = cursivetransformer.transformer.h[l].attn.c_proj.bias\n",
        "\n",
        "        state_dict[f\"blocks.{l}.ln2.w\"] = cursivetransformer.transformer.h[l].ln_2.weight\n",
        "        state_dict[f\"blocks.{l}.ln2.b\"] = cursivetransformer.transformer.h[l].ln_2.bias\n",
        "\n",
        "        W_in = cursivetransformer.transformer.h[l].mlp.c_fc.weight\n",
        "        state_dict[f\"blocks.{l}.mlp.W_in\"] = W_in\n",
        "        state_dict[f\"blocks.{l}.mlp.b_in\"] = cursivetransformer.transformer.h[l].mlp.c_fc.bias\n",
        "\n",
        "        W_out = cursivetransformer.transformer.h[l].mlp.c_proj.weight\n",
        "        state_dict[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
        "        state_dict[f\"blocks.{l}.mlp.b_out\"] = cursivetransformer.transformer.h[l].mlp.c_proj.bias\n",
        "    state_dict[\"unembed.W_U\"] = cursivetransformer.lm_head.weight.T\n",
        "\n",
        "    state_dict[\"ln_final.w\"] = cursivetransformer.transformer.ln_f.weight\n",
        "    state_dict[\"ln_final.b\"] = cursivetransformer.transformer.ln_f.bias\n",
        "    return state_dict"
      ],
      "metadata": {
        "id": "FUibmGELZbLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerLens/transformer_lens/loading_from_pretrained.py\n",
        "- https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/loading_from_pretrained.py"
      ],
      "metadata": {
        "id": "qKwM9sCrUeLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "9712nvgrak9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Loading Pretrained Models Utilities.\n",
        "\n",
        "This module contains functions for loading pretrained models from the Hugging Face Hub.\n",
        "\"\"\"\n",
        "\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Union\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import HfApi\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    BertForPreTraining,\n",
        "    T5ForConditionalGeneration,\n",
        ")\n",
        "\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "from transformer_lens.pretrained.weight_conversions import (\n",
        "    convert_bert_weights,\n",
        "    convert_bloom_weights,\n",
        "    convert_coder_weights,\n",
        "    convert_gemma_weights,\n",
        "    convert_gpt2_weights,\n",
        "    convert_gptj_weights,\n",
        "    convert_llama_weights,\n",
        "    convert_mingpt_weights,\n",
        "    convert_mistral_weights,\n",
        "    convert_mixtral_weights,\n",
        "    convert_neel_solu_old_weights,\n",
        "    convert_neo_weights,\n",
        "    convert_neox_weights,\n",
        "    convert_opt_weights,\n",
        "    convert_phi3_weights,\n",
        "    convert_phi_weights,\n",
        "    convert_qwen2_weights,\n",
        "    convert_qwen_weights,\n",
        "    convert_t5_weights,\n",
        ")\n",
        "\n",
        "OFFICIAL_MODEL_NAMES = [\n",
        "    \"gpt2\",\n",
        "    \"gpt2-medium\",\n",
        "    \"gpt2-large\",\n",
        "    \"gpt2-xl\",\n",
        "    \"distilgpt2\",\n",
        "    \"facebook/opt-125m\",\n",
        "    \"facebook/opt-1.3b\",\n",
        "    \"facebook/opt-2.7b\",\n",
        "    \"facebook/opt-6.7b\",\n",
        "    \"facebook/opt-13b\",\n",
        "    \"facebook/opt-30b\",\n",
        "    \"facebook/opt-66b\",\n",
        "    \"EleutherAI/gpt-neo-125M\",\n",
        "    \"EleutherAI/gpt-neo-1.3B\",\n",
        "    \"EleutherAI/gpt-neo-2.7B\",\n",
        "    \"EleutherAI/gpt-j-6B\",\n",
        "    \"EleutherAI/gpt-neox-20b\",\n",
        "    \"stanford-crfm/alias-gpt2-small-x21\",\n",
        "    \"stanford-crfm/battlestar-gpt2-small-x49\",\n",
        "    \"stanford-crfm/caprica-gpt2-small-x81\",\n",
        "    \"stanford-crfm/darkmatter-gpt2-small-x343\",\n",
        "    \"stanford-crfm/expanse-gpt2-small-x777\",\n",
        "    \"stanford-crfm/arwen-gpt2-medium-x21\",\n",
        "    \"stanford-crfm/beren-gpt2-medium-x49\",\n",
        "    \"stanford-crfm/celebrimbor-gpt2-medium-x81\",\n",
        "    \"stanford-crfm/durin-gpt2-medium-x343\",\n",
        "    \"stanford-crfm/eowyn-gpt2-medium-x777\",\n",
        "    \"EleutherAI/pythia-14m\",\n",
        "    \"EleutherAI/pythia-31m\",\n",
        "    \"EleutherAI/pythia-70m\",\n",
        "    \"EleutherAI/pythia-160m\",\n",
        "    \"EleutherAI/pythia-410m\",\n",
        "    \"EleutherAI/pythia-1b\",\n",
        "    \"EleutherAI/pythia-1.4b\",\n",
        "    \"EleutherAI/pythia-2.8b\",\n",
        "    \"EleutherAI/pythia-6.9b\",\n",
        "    \"EleutherAI/pythia-12b\",\n",
        "    \"EleutherAI/pythia-70m-deduped\",\n",
        "    \"EleutherAI/pythia-160m-deduped\",\n",
        "    \"EleutherAI/pythia-410m-deduped\",\n",
        "    \"EleutherAI/pythia-1b-deduped\",\n",
        "    \"EleutherAI/pythia-1.4b-deduped\",\n",
        "    \"EleutherAI/pythia-2.8b-deduped\",\n",
        "    \"EleutherAI/pythia-6.9b-deduped\",\n",
        "    \"EleutherAI/pythia-12b-deduped\",\n",
        "    \"EleutherAI/pythia-70m-v0\",\n",
        "    \"EleutherAI/pythia-160m-v0\",\n",
        "    \"EleutherAI/pythia-410m-v0\",\n",
        "    \"EleutherAI/pythia-1b-v0\",\n",
        "    \"EleutherAI/pythia-1.4b-v0\",\n",
        "    \"EleutherAI/pythia-2.8b-v0\",\n",
        "    \"EleutherAI/pythia-6.9b-v0\",\n",
        "    \"EleutherAI/pythia-12b-v0\",\n",
        "    \"EleutherAI/pythia-70m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-160m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-410m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-1b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-1.4b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-2.8b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-6.9b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-12b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-160m-seed1\",\n",
        "    \"EleutherAI/pythia-160m-seed2\",\n",
        "    \"EleutherAI/pythia-160m-seed3\",\n",
        "    \"NeelNanda/SoLU_1L_v9_old\",\n",
        "    \"NeelNanda/SoLU_2L_v10_old\",\n",
        "    \"NeelNanda/SoLU_4L_v11_old\",\n",
        "    \"NeelNanda/SoLU_6L_v13_old\",\n",
        "    \"NeelNanda/SoLU_8L_v21_old\",\n",
        "    \"NeelNanda/SoLU_10L_v22_old\",\n",
        "    \"NeelNanda/SoLU_12L_v23_old\",\n",
        "    \"NeelNanda/SoLU_1L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_2L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_3L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_4L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_6L768W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_8L1024W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_10L1280W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_12L1536W_C4_Code\",\n",
        "    \"NeelNanda/GELU_1L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_2L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_3L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_4L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_1L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_2L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_3L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_4L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\",\n",
        "    \"NeelNanda/SoLU_1L512W_Wiki_Finetune\",\n",
        "    \"NeelNanda/SoLU_4L512W_Wiki_Finetune\",\n",
        "    \"ArthurConmy/redwood_attn_2l\",\n",
        "    \"llama-7b-hf\",\n",
        "    \"llama-13b-hf\",\n",
        "    \"llama-30b-hf\",\n",
        "    \"llama-65b-hf\",\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    \"meta-llama/Llama-2-13b-hf\",\n",
        "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
        "    \"CodeLlama-7b-hf\",\n",
        "    \"CodeLlama-7b-Python-hf\",\n",
        "    \"CodeLlama-7b-Instruct-hf\",\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3-70B\",\n",
        "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"Baidicoot/Othello-GPT-Transformer-Lens\",\n",
        "    \"bert-base-cased\",\n",
        "    \"roneneldan/TinyStories-1M\",\n",
        "    \"roneneldan/TinyStories-3M\",\n",
        "    \"roneneldan/TinyStories-8M\",\n",
        "    \"roneneldan/TinyStories-28M\",\n",
        "    \"roneneldan/TinyStories-33M\",\n",
        "    \"roneneldan/TinyStories-Instruct-1M\",\n",
        "    \"roneneldan/TinyStories-Instruct-3M\",\n",
        "    \"roneneldan/TinyStories-Instruct-8M\",\n",
        "    \"roneneldan/TinyStories-Instruct-28M\",\n",
        "    \"roneneldan/TinyStories-Instruct-33M\",\n",
        "    \"roneneldan/TinyStories-1Layer-21M\",\n",
        "    \"roneneldan/TinyStories-2Layers-33M\",\n",
        "    \"roneneldan/TinyStories-Instuct-1Layer-21M\",\n",
        "    \"roneneldan/TinyStories-Instruct-2Layers-33M\",\n",
        "    \"stabilityai/stablelm-base-alpha-3b\",\n",
        "    \"stabilityai/stablelm-base-alpha-7b\",\n",
        "    \"stabilityai/stablelm-tuned-alpha-3b\",\n",
        "    \"stabilityai/stablelm-tuned-alpha-7b\",\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "    \"bigscience/bloom-1b1\",\n",
        "    \"bigscience/bloom-1b7\",\n",
        "    \"bigscience/bloom-3b\",\n",
        "    \"bigscience/bloom-7b1\",\n",
        "    \"bigcode/santacoder\",\n",
        "    \"Qwen/Qwen-1_8B\",\n",
        "    \"Qwen/Qwen-7B\",\n",
        "    \"Qwen/Qwen-14B\",\n",
        "    \"Qwen/Qwen-1_8B-Chat\",\n",
        "    \"Qwen/Qwen-7B-Chat\",\n",
        "    \"Qwen/Qwen-14B-Chat\",\n",
        "    \"Qwen/Qwen1.5-0.5B\",\n",
        "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "    \"Qwen/Qwen1.5-1.8B\",\n",
        "    \"Qwen/Qwen1.5-1.8B-Chat\",\n",
        "    \"Qwen/Qwen1.5-4B\",\n",
        "    \"Qwen/Qwen1.5-4B-Chat\",\n",
        "    \"Qwen/Qwen1.5-7B\",\n",
        "    \"Qwen/Qwen1.5-7B-Chat\",\n",
        "    \"Qwen/Qwen1.5-14B\",\n",
        "    \"Qwen/Qwen1.5-14B-Chat\",\n",
        "    \"Qwen/Qwen2-0.5B\",\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    \"Qwen/Qwen2-1.5B\",\n",
        "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"Qwen/Qwen2-7B\",\n",
        "    \"Qwen/Qwen2-7B-Instruct\",\n",
        "    \"microsoft/phi-1\",\n",
        "    \"microsoft/phi-1_5\",\n",
        "    \"microsoft/phi-2\",\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"google/gemma-2b\",\n",
        "    \"google/gemma-7b\",\n",
        "    \"google/gemma-2b-it\",\n",
        "    \"google/gemma-7b-it\",\n",
        "    \"google/gemma-2-2b\",\n",
        "    \"google/gemma-2-2b-it\",\n",
        "    \"google/gemma-2-9b\",\n",
        "    \"google/gemma-2-9b-it\",\n",
        "    \"google/gemma-2-27b\",\n",
        "    \"google/gemma-2-27b-it\",\n",
        "    \"01-ai/Yi-6B\",\n",
        "    \"01-ai/Yi-34B\",\n",
        "    \"01-ai/Yi-6B-Chat\",\n",
        "    \"01-ai/Yi-34B-Chat\",\n",
        "    \"google-t5/t5-small\",\n",
        "    \"google-t5/t5-base\",\n",
        "    \"google-t5/t5-large\",\n",
        "    \"ai-forever/mGPT\",\n",
        "]\n",
        "\"\"\"Official model names for models on HuggingFace.\"\"\"\n",
        "\n",
        "# Model Aliases:\n",
        "MODEL_ALIASES = {\n",
        "    \"NeelNanda/SoLU_1L_v9_old\": [\"solu-1l-pile\", \"solu-1l-old\"],\n",
        "    \"NeelNanda/SoLU_2L_v10_old\": [\"solu-2l-pile\", \"solu-2l-old\"],\n",
        "    \"NeelNanda/SoLU_4L_v11_old\": [\"solu-4l-pile\", \"solu-4l-old\"],\n",
        "    \"NeelNanda/SoLU_6L_v13_old\": [\"solu-6l-pile\", \"solu-6l-old\"],\n",
        "    \"NeelNanda/SoLU_8L_v21_old\": [\"solu-8l-pile\", \"solu-8l-old\"],\n",
        "    \"NeelNanda/SoLU_10L_v22_old\": [\"solu-10l-pile\", \"solu-10l-old\"],\n",
        "    \"NeelNanda/SoLU_12L_v23_old\": [\"solu-12l-pile\", \"solu-12l-old\"],\n",
        "    \"NeelNanda/SoLU_1L512W_C4_Code\": [\"solu-1l\", \"solu-1l-new\", \"solu-1l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_2L512W_C4_Code\": [\"solu-2l\", \"solu-2l-new\", \"solu-2l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_3L512W_C4_Code\": [\"solu-3l\", \"solu-3l-new\", \"solu-3l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_4L512W_C4_Code\": [\"solu-4l\", \"solu-4l-new\", \"solu-4l-c4-code\"],\n",
        "    \"NeelNanda/GELU_1L512W_C4_Code\": [\"gelu-1l\", \"gelu-1l-new\", \"gelu-1l-c4-code\"],\n",
        "    \"NeelNanda/GELU_2L512W_C4_Code\": [\"gelu-2l\", \"gelu-2l-new\", \"gelu-2l-c4-code\"],\n",
        "    \"NeelNanda/GELU_3L512W_C4_Code\": [\"gelu-3l\", \"gelu-3l-new\", \"gelu-3l-c4-code\"],\n",
        "    \"NeelNanda/GELU_4L512W_C4_Code\": [\"gelu-4l\", \"gelu-4l-new\", \"gelu-4l-c4-code\"],\n",
        "    \"NeelNanda/Attn_Only_1L512W_C4_Code\": [\n",
        "        \"attn-only-1l\",\n",
        "        \"attn-only-1l-new\",\n",
        "        \"attn-only-1l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_2L512W_C4_Code\": [\n",
        "        \"attn-only-2l\",\n",
        "        \"attn-only-2l-new\",\n",
        "        \"attn-only-2l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_3L512W_C4_Code\": [\n",
        "        \"attn-only-3l\",\n",
        "        \"attn-only-3l-new\",\n",
        "        \"attn-only-3l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_4L512W_C4_Code\": [\n",
        "        \"attn-only-4l\",\n",
        "        \"attn-only-4l-new\",\n",
        "        \"attn-only-4l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_6L768W_C4_Code\": [\"solu-6l\", \"solu-6l-new\", \"solu-6l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_8L1024W_C4_Code\": [\"solu-8l\", \"solu-8l-new\", \"solu-8l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_10L1280W_C4_Code\": [\"solu-10l\", \"solu-10l-new\", \"solu-10l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_12L1536W_C4_Code\": [\"solu-12l\", \"solu-12l-new\", \"solu-12l-c4-code\"],\n",
        "    \"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\": [\n",
        "        \"attn-only-2l-demo\",\n",
        "        \"attn-only-2l-shortformer-6b-big-lr\",\n",
        "        \"attn-only-2l-induction-demo\",\n",
        "        \"attn-only-demo\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_1L512W_Wiki_Finetune\": [\n",
        "        \"solu-1l-wiki\",\n",
        "        \"solu-1l-wiki-finetune\",\n",
        "        \"solu-1l-finetune\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_4L512W_Wiki_Finetune\": [\n",
        "        \"solu-4l-wiki\",\n",
        "        \"solu-4l-wiki-finetune\",\n",
        "        \"solu-4l-finetune\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-14m\": [\n",
        "        \"pythia-14m\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-31m\": [\n",
        "        \"pythia-31m\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m\": [\n",
        "        \"pythia-70m\",\n",
        "        \"pythia\",\n",
        "        \"EleutherAI/pythia-19m\",\n",
        "        \"pythia-19m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m\": [\n",
        "        \"pythia-160m\",\n",
        "        \"EleutherAI/pythia-125m\",\n",
        "        \"pythia-125m\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m\": [\n",
        "        \"pythia-410m\",\n",
        "        \"EleutherAI/pythia-350m\",\n",
        "        \"pythia-350m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b\": [\n",
        "        \"pythia-1b\",\n",
        "        \"EleutherAI/pythia-800m\",\n",
        "        \"pythia-800m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b\": [\n",
        "        \"pythia-1.4b\",\n",
        "        \"EleutherAI/pythia-1.3b\",\n",
        "        \"pythia-1.3b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b\": [\n",
        "        \"pythia-2.8b\",\n",
        "        \"EleutherAI/pythia-2.7b\",\n",
        "        \"pythia-2.7b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b\": [\n",
        "        \"pythia-6.9b\",\n",
        "        \"EleutherAI/pythia-6.7b\",\n",
        "        \"pythia-6.7b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b\": [\n",
        "        \"pythia-12b\",\n",
        "        \"EleutherAI/pythia-13b\",\n",
        "        \"pythia-13b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-deduped\": [\n",
        "        \"pythia-70m-deduped\",\n",
        "        \"EleutherAI/pythia-19m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-19m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-deduped\": [\n",
        "        \"pythia-160m-deduped\",\n",
        "        \"EleutherAI/pythia-125m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-125m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-deduped\": [\n",
        "        \"pythia-410m-deduped\",\n",
        "        \"EleutherAI/pythia-350m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-350m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-deduped\": [\n",
        "        \"pythia-1b-deduped\",\n",
        "        \"EleutherAI/pythia-800m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-800m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-deduped\": [\n",
        "        \"pythia-1.4b-deduped\",\n",
        "        \"EleutherAI/pythia-1.3b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-1.3b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-deduped\": [\n",
        "        \"pythia-2.8b-deduped\",\n",
        "        \"EleutherAI/pythia-2.7b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-2.7b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-deduped\": [\n",
        "        \"pythia-6.9b-deduped\",\n",
        "        \"EleutherAI/pythia-6.7b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-6.7b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-deduped\": [\n",
        "        \"pythia-12b-deduped\",\n",
        "        \"EleutherAI/pythia-13b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-13b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-v0\": [\n",
        "        \"pythia-70m-v0\",\n",
        "        \"pythia-v0\",\n",
        "        \"EleutherAI/pythia-19m-v0\",\n",
        "        \"pythia-19m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-v0\": [\n",
        "        \"pythia-160m-v0\",\n",
        "        \"EleutherAI/pythia-125m-v0\",\n",
        "        \"pythia-125m-v0\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-v0\": [\n",
        "        \"pythia-410m-v0\",\n",
        "        \"EleutherAI/pythia-350m-v0\",\n",
        "        \"pythia-350m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-v0\": [\n",
        "        \"pythia-1b-v0\",\n",
        "        \"EleutherAI/pythia-800m-v0\",\n",
        "        \"pythia-800m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-v0\": [\n",
        "        \"pythia-1.4b-v0\",\n",
        "        \"EleutherAI/pythia-1.3b-v0\",\n",
        "        \"pythia-1.3b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-v0\": [\n",
        "        \"pythia-2.8b-v0\",\n",
        "        \"EleutherAI/pythia-2.7b-v0\",\n",
        "        \"pythia-2.7b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-v0\": [\n",
        "        \"pythia-6.9b-v0\",\n",
        "        \"EleutherAI/pythia-6.7b-v0\",\n",
        "        \"pythia-6.7b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-v0\": [\n",
        "        \"pythia-12b-v0\",\n",
        "        \"EleutherAI/pythia-13b-v0\",\n",
        "        \"pythia-13b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-deduped-v0\": [\n",
        "        \"pythia-70m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-19m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-19m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-deduped-v0\": [\n",
        "        \"pythia-160m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-125m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-125m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-deduped-v0\": [\n",
        "        \"pythia-410m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-350m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-350m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-deduped-v0\": [\n",
        "        \"pythia-1b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-800m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-800m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-deduped-v0\": [\n",
        "        \"pythia-1.4b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-1.3b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-1.3b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-deduped-v0\": [\n",
        "        \"pythia-2.8b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-2.7b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-2.7b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-deduped-v0\": [\n",
        "        \"pythia-6.9b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-6.7b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-6.7b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-deduped-v0\": [\n",
        "        \"pythia-12b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-13b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-13b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed1\": [\n",
        "        \"pythia-160m-seed1\",\n",
        "        \"EleutherAI/pythia-125m-seed1\",\n",
        "        \"pythia-125m-seed1\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed2\": [\n",
        "        \"pythia-160m-seed2\",\n",
        "        \"EleutherAI/pythia-125m-seed2\",\n",
        "        \"pythia-125m-seed2\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed3\": [\n",
        "        \"pythia-160m-seed3\",\n",
        "        \"EleutherAI/pythia-125m-seed3\",\n",
        "        \"pythia-125m-seed3\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"gpt2\": [\"gpt2-small\"],\n",
        "    \"distilgpt2\": [\"distillgpt2\", \"distill-gpt2\", \"distil-gpt2\", \"gpt2-xs\"],\n",
        "    \"facebook/opt-125m\": [\"opt-125m\", \"opt-small\", \"opt\"],\n",
        "    \"facebook/opt-1.3b\": [\"opt-1.3b\", \"opt-medium\"],\n",
        "    \"facebook/opt-2.7b\": [\"opt-2.7b\", \"opt-large\"],\n",
        "    \"facebook/opt-6.7b\": [\"opt-6.7b\", \"opt-xl\"],\n",
        "    \"facebook/opt-13b\": [\"opt-13b\", \"opt-xxl\"],\n",
        "    \"facebook/opt-30b\": [\"opt-30b\", \"opt-xxxl\"],\n",
        "    \"facebook/opt-66b\": [\"opt-66b\", \"opt-xxxxl\"],\n",
        "    \"EleutherAI/gpt-neo-125M\": [\"gpt-neo-125M\", \"gpt-neo-small\", \"neo-small\", \"neo\"],\n",
        "    \"EleutherAI/gpt-neo-1.3B\": [\"gpt-neo-1.3B\", \"gpt-neo-medium\", \"neo-medium\"],\n",
        "    \"EleutherAI/gpt-neo-2.7B\": [\"gpt-neo-2.7B\", \"gpt-neo-large\", \"neo-large\"],\n",
        "    \"EleutherAI/gpt-j-6B\": [\"gpt-j-6B\", \"gpt-j\", \"gptj\"],\n",
        "    \"EleutherAI/gpt-neox-20b\": [\"gpt-neox-20b\", \"gpt-neox\", \"neox\"],\n",
        "    \"stanford-crfm/alias-gpt2-small-x21\": [\n",
        "        \"stanford-gpt2-small-a\",\n",
        "        \"alias-gpt2-small-x21\",\n",
        "        \"gpt2-mistral-small-a\",\n",
        "        \"gpt2-stanford-small-a\",\n",
        "    ],\n",
        "    \"stanford-crfm/battlestar-gpt2-small-x49\": [\n",
        "        \"stanford-gpt2-small-b\",\n",
        "        \"battlestar-gpt2-small-x49\",\n",
        "        \"gpt2-mistral-small-b\",\n",
        "        \"gpt2-mistral-small-b\",\n",
        "    ],\n",
        "    \"stanford-crfm/caprica-gpt2-small-x81\": [\n",
        "        \"stanford-gpt2-small-c\",\n",
        "        \"caprica-gpt2-small-x81\",\n",
        "        \"gpt2-mistral-small-c\",\n",
        "        \"gpt2-stanford-small-c\",\n",
        "    ],\n",
        "    \"stanford-crfm/darkmatter-gpt2-small-x343\": [\n",
        "        \"stanford-gpt2-small-d\",\n",
        "        \"darkmatter-gpt2-small-x343\",\n",
        "        \"gpt2-mistral-small-d\",\n",
        "        \"gpt2-mistral-small-d\",\n",
        "    ],\n",
        "    \"stanford-crfm/expanse-gpt2-small-x777\": [\n",
        "        \"stanford-gpt2-small-e\",\n",
        "        \"expanse-gpt2-small-x777\",\n",
        "        \"gpt2-mistral-small-e\",\n",
        "        \"gpt2-mistral-small-e\",\n",
        "    ],\n",
        "    \"stanford-crfm/arwen-gpt2-medium-x21\": [\n",
        "        \"stanford-gpt2-medium-a\",\n",
        "        \"arwen-gpt2-medium-x21\",\n",
        "        \"gpt2-medium-small-a\",\n",
        "        \"gpt2-stanford-medium-a\",\n",
        "    ],\n",
        "    \"stanford-crfm/beren-gpt2-medium-x49\": [\n",
        "        \"stanford-gpt2-medium-b\",\n",
        "        \"beren-gpt2-medium-x49\",\n",
        "        \"gpt2-medium-small-b\",\n",
        "        \"gpt2-stanford-medium-b\",\n",
        "    ],\n",
        "    \"stanford-crfm/celebrimbor-gpt2-medium-x81\": [\n",
        "        \"stanford-gpt2-medium-c\",\n",
        "        \"celebrimbor-gpt2-medium-x81\",\n",
        "        \"gpt2-medium-small-c\",\n",
        "        \"gpt2-medium-small-c\",\n",
        "    ],\n",
        "    \"stanford-crfm/durin-gpt2-medium-x343\": [\n",
        "        \"stanford-gpt2-medium-d\",\n",
        "        \"durin-gpt2-medium-x343\",\n",
        "        \"gpt2-medium-small-d\",\n",
        "        \"gpt2-stanford-medium-d\",\n",
        "    ],\n",
        "    \"stanford-crfm/eowyn-gpt2-medium-x777\": [\n",
        "        \"stanford-gpt2-medium-e\",\n",
        "        \"eowyn-gpt2-medium-x777\",\n",
        "        \"gpt2-medium-small-e\",\n",
        "        \"gpt2-stanford-medium-e\",\n",
        "    ],\n",
        "    \"ArthurConmy/redwood_attn_2l\": [\"redwood_attn_2l\"],\n",
        "    \"llama-7b-hf\": [\"llama-7b\"],\n",
        "    \"llama-13b-hf\": [\"llama-13b\"],\n",
        "    \"llama-30b-hf\": [\"llama-30b\"],\n",
        "    \"llama-65b-hf\": [\"llama-65b\"],\n",
        "    \"meta-llama/Llama-2-7b-hf\": [\"Llama-2-7b\", \"meta-llama/Llama-2-7b-hf\"],\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\": [\n",
        "        \"Llama-2-7b-chat\",\n",
        "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    ],\n",
        "    \"meta-llama/Llama-2-13b-hf\": [\"Llama-2-13b\", \"meta-llama/Llama-2-13b-hf\"],\n",
        "    \"meta-llama/Llama-2-13b-chat-hf\": [\n",
        "        \"Llama-2-13b-chat\",\n",
        "        \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    ],\n",
        "    \"meta-llama/Llama-2-70b-chat-hf\": [\"Llama-2-70b-chat\", \"meta-llama-2-70b-chat-hf\"],\n",
        "    \"CodeLlama-7b-hf\": [\"CodeLlamallama-2-7b\", \"codellama/CodeLlama-7b-hf\"],\n",
        "    \"CodeLlama-7b-Python-hf\": [\n",
        "        \"CodeLlama-7b-python\",\n",
        "        \"codellama/CodeLlama-7b-Python-hf\",\n",
        "    ],\n",
        "    \"CodeLlama-7b-Instruct-hf\": [\n",
        "        \"CodeLlama-7b-instruct\",\n",
        "        \"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "    ],\n",
        "    \"Baidicoot/Othello-GPT-Transformer-Lens\": [\"othello-gpt\"],\n",
        "    \"roneneldan/TinyStories-1M\": [\"tiny-stories-1M\"],\n",
        "    \"roneneldan/TinyStories-3M\": [\"tiny-stories-3M\"],\n",
        "    \"roneneldan/TinyStories-8M\": [\"tiny-stories-8M\"],\n",
        "    \"roneneldan/TinyStories-28M\": [\"tiny-stories-28M\"],\n",
        "    \"roneneldan/TinyStories-33M\": [\"tiny-stories-33M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-1M\": [\"tiny-stories-instruct-1M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-3M\": [\"tiny-stories-instruct-3M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-8M\": [\"tiny-stories-instruct-8M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-28M\": [\"tiny-stories-instruct-28M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-33M\": [\"tiny-stories-instruct-33M\"],\n",
        "    \"roneneldan/TinyStories-1Layer-21M\": [\"tiny-stories-1L-21M\"],\n",
        "    \"roneneldan/TinyStories-2Layers-33M\": [\"tiny-stories-2L-33M\"],\n",
        "    \"roneneldan/TinyStories-Instuct-1Layer-21M\": [\"tiny-stories-instruct-1L-21M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-2Layers-33M\": [\"tiny-stories-instruct-2L-33M\"],\n",
        "    \"stabilityai/stablelm-base-alpha-3b\": [\n",
        "        \"stablelm-base-alpha-3b\",\n",
        "        \"stablelm-base-3b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-base-alpha-7b\": [\n",
        "        \"stablelm-base-alpha-7b\",\n",
        "        \"stablelm-base-7b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-tuned-alpha-3b\": [\n",
        "        \"stablelm-tuned-alpha-3b\",\n",
        "        \"stablelm-tuned-3b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-tuned-alpha-7b\": [\n",
        "        \"stablelm-tuned-alpha-7b\",\n",
        "        \"stablelm-tuned-7b\",\n",
        "    ],\n",
        "    \"mistralai/Mistral-7B-v0.1\": [\"mistral-7b\"],\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\": [\"mistral-7b-instruct\"],\n",
        "    \"mistralai/Mixtral-8x7B-v0.1\": [\"mixtral\", \"mixtral-8x7b\"],\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": [\n",
        "        \"mixtral-instruct\",\n",
        "        \"mixtral-8x7b-instruct\",\n",
        "    ],\n",
        "    \"bigscience/bloom-560m\": [\"bloom-560m\"],\n",
        "    \"bigscience/bloom-1b1\": [\"bloom-1b1\"],\n",
        "    \"bigscience/bloom-1b7\": [\"bloom-1b7\"],\n",
        "    \"bigscience/bloom-3b\": [\"bloom-3b\"],\n",
        "    \"bigscience/bloom-7b1\": [\"bloom-7b1\"],\n",
        "    \"bigcode/santacoder\": [\"santacoder\"],\n",
        "    \"Qwen/Qwen-1_8B\": [\"qwen-1.8b\"],\n",
        "    \"Qwen/Qwen-7B\": [\"qwen-7b\"],\n",
        "    \"Qwen/Qwen-14B\": [\"qwen-14b\"],\n",
        "    \"Qwen/Qwen-1_8B-Chat\": [\"qwen-1.8b-chat\"],\n",
        "    \"Qwen/Qwen-7B-Chat\": [\"qwen-7b-chat\"],\n",
        "    \"Qwen/Qwen-14B-Chat\": [\"qwen-14b-chat\"],\n",
        "    \"Qwen/Qwen1.5-0.5B\": [\"qwen1.5-0.5b\"],\n",
        "    \"Qwen/Qwen1.5-0.5B-Chat\": [\"qwen1.5-0.5b-chat\"],\n",
        "    \"Qwen/Qwen1.5-1.8B\": [\"qwen1.5-1.8b\"],\n",
        "    \"Qwen/Qwen1.5-1.8B-Chat\": [\"qwen1.5-1.8b-chat\"],\n",
        "    \"Qwen/Qwen1.5-4B\": [\"qwen1.5-4b\"],\n",
        "    \"Qwen/Qwen1.5-4B-Chat\": [\"qwen1.5-4b-chat\"],\n",
        "    \"Qwen/Qwen1.5-7B\": [\"qwen1.5-7b\"],\n",
        "    \"Qwen/Qwen1.5-7B-Chat\": [\"qwen1.5-7b-chat\"],\n",
        "    \"Qwen/Qwen1.5-14B\": [\"qwen1.5-14b\"],\n",
        "    \"Qwen/Qwen1.5-14B-Chat\": [\"qwen1.5-14b-chat\"],\n",
        "    \"microsoft/phi-1\": [\"phi-1\"],\n",
        "    \"microsoft/phi-1_5\": [\"phi-1_5\"],\n",
        "    \"microsoft/phi-2\": [\"phi-2\"],\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\": [\"phi-3\"],\n",
        "    \"google/gemma-2b\": [\"gemma-2b\"],\n",
        "    \"google/gemma-7b\": [\"gemma-7b\"],\n",
        "    \"google/gemma-2b-it\": [\"gemma-2b-it\"],\n",
        "    \"google/gemma-7b-it\": [\"gemma-7b-it\"],\n",
        "    \"google/gemma-2-2b\": [\"gemma-2-2b\"],\n",
        "    \"google/gemma-2-9b\": [\"gemma-2-9b\"],\n",
        "    \"google/gemma-2-27b\": [\"gemma-2-27b\"],\n",
        "    \"google/gemma-2-2b-it\": [\"gemma-2-2b-it\"],\n",
        "    \"google/gemma-2-9b-it\": [\"gemma-2-9b-it\"],\n",
        "    \"google/gemma-2-27b-it\": [\"gemma-2-27b-it\"],\n",
        "    \"01-ai/Yi-6B\": [\"yi-6b\", \"Yi-6B\"],\n",
        "    \"01-ai/Yi-34B\": [\"yi-34b\", \"Yi-34B\"],\n",
        "    \"01-ai/Yi-6B-Chat\": [\"yi-6b-chat\", \"Yi-6B-Chat\"],\n",
        "    \"01-ai/Yi-34B-Chat\": [\"yi-34b-chat\", \"Yi-34B-Chat\"],\n",
        "    \"google-t5/t5-small\": [\"t5-small\"],\n",
        "    \"google-t5/t5-base\": [\"t5-base\"],\n",
        "    \"google-t5/t5-large\": [\"t5-large\"],\n",
        "    \"ai-forever/mGPT\": [\"mGPT\"],\n",
        "}\n",
        "\"\"\"Model aliases for models on HuggingFace.\"\"\"\n",
        "\n",
        "NON_HF_HOSTED_MODEL_NAMES = [\n",
        "    \"cursivetransformer/ct_bigbank_400M\", # NEW\n",
        "    \"llama-7b-hf\",\n",
        "    \"llama-13b-hf\",\n",
        "    \"llama-30b-hf\",\n",
        "    \"llama-65b-hf\",\n",
        "]\n",
        "\"\"\"Official model names for models not hosted on HuggingFace.\"\"\"\n",
        "\n",
        "# Sets a default model alias, by convention the first one in the model alias table, else the official name if it has no aliases\n",
        "DEFAULT_MODEL_ALIASES = [\n",
        "    MODEL_ALIASES[name][0] if name in MODEL_ALIASES else name for name in OFFICIAL_MODEL_NAMES\n",
        "]\n",
        "\n",
        "NEED_REMOTE_CODE_MODELS = (\n",
        "    \"bigcode/santacoder\",\n",
        "    \"Qwen/Qwen-\",\n",
        "    \"microsoft/phi-2\",\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        ")"
      ],
      "metadata": {
        "id": "iaYbeY1Iakym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "9zw3J3tEaspN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_alias_map():\n",
        "    \"\"\"\n",
        "    Converts OFFICIAL_MODEL_NAMES (the list of actual model names on\n",
        "    HuggingFace) and MODEL_ALIASES (a dictionary mapping official model names to\n",
        "    aliases) into a dictionary mapping all aliases to the official model name.\n",
        "    \"\"\"\n",
        "    model_alias_map = {}\n",
        "    for official_model_name in OFFICIAL_MODEL_NAMES:\n",
        "        aliases = MODEL_ALIASES.get(official_model_name, [])\n",
        "        for alias in aliases:\n",
        "            model_alias_map[alias.lower()] = official_model_name\n",
        "        model_alias_map[official_model_name.lower()] = official_model_name\n",
        "    return model_alias_map\n",
        "\n",
        "\n",
        "def get_official_model_name(model_name: str):\n",
        "    \"\"\"\n",
        "    Returns the official model name for a given model name (or alias).\n",
        "    \"\"\"\n",
        "    model_alias_map = make_model_alias_map()\n",
        "    official_model_name = model_alias_map.get(model_name.lower(), None)\n",
        "    if official_model_name is None:\n",
        "        raise ValueError(\n",
        "            f\"{model_name} not found. Valid official model names (excl aliases): {OFFICIAL_MODEL_NAMES}\"\n",
        "        )\n",
        "    return official_model_name\n",
        "\n",
        "\n",
        "def convert_hf_model_config(model_name: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Returns the model config for a HuggingFace model, converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    Takes the official_model_name as an input.\n",
        "    \"\"\"\n",
        "    # In case the user passed in an alias\n",
        "    if (Path(model_name) / \"config.json\").exists():\n",
        "        logging.info(\"Loading model config from local directory\")\n",
        "        official_model_name = model_name\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(model_name)\n",
        "\n",
        "    # Load HuggingFace model config\n",
        "    if \"llama\" in official_model_name.lower():\n",
        "        architecture = \"LlamaForCausalLM\"\n",
        "    elif \"gemma-2\" in official_model_name.lower():\n",
        "        architecture = \"Gemma2ForCausalLM\"\n",
        "    elif \"gemma\" in official_model_name.lower():\n",
        "        architecture = \"GemmaForCausalLM\"\n",
        "    else:\n",
        "        huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "        hf_config = AutoConfig.from_pretrained(\n",
        "            official_model_name,\n",
        "            token=huggingface_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "        architecture = hf_config.architectures[0]\n",
        "\n",
        "    if official_model_name.startswith(\n",
        "        (\"llama-7b\", \"meta-llama/Llama-2-7b\")\n",
        "    ):  # same architecture for LLaMA and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 11008,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 2048 if official_model_name.startswith(\"llama-7b\") else 4096,\n",
        "            \"eps\": 1e-6 if official_model_name.startswith(\"llama-7b\") else 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"CodeLlama-7b\"):  # same architecture CodeLlama and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 11008,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 4096,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 32016,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "            \"rotary_base\": 1000000,\n",
        "        }\n",
        "        if \"python\" in official_model_name.lower():\n",
        "            # The vocab size of python version of CodeLlama-7b is 32000\n",
        "            cfg_dict[\"d_vocab\"] = 32000\n",
        "    elif official_model_name.startswith(\n",
        "        (\"llama-13b\", \"meta-llama/Llama-2-13b\")\n",
        "    ):  # same architecture for LLaMA and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 5120,\n",
        "            \"d_head\": 5120 // 40,\n",
        "            \"n_heads\": 40,\n",
        "            \"d_mlp\": 13824,\n",
        "            \"n_layers\": 40,\n",
        "            \"n_ctx\": 2048 if official_model_name.startswith(\"llama-13b\") else 4096,\n",
        "            \"eps\": 1e-6 if official_model_name.startswith(\"llama-13b\") else 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 5120 // 40,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"llama-30b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 6656,\n",
        "            \"d_head\": 6656 // 52,\n",
        "            \"n_heads\": 52,\n",
        "            \"d_mlp\": 17920,\n",
        "            \"n_layers\": 60,\n",
        "            \"n_ctx\": 2048,\n",
        "            \"eps\": 1e-6,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 6656 // 52,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"llama-65b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 8192 // 64,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 22016,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 2048,\n",
        "            \"eps\": 1e-6,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": 8192 // 64,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-2-70b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 28672,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 4096,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Meta-Llama-3-8B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Meta-Llama-3-70B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 28672,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-1B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 64,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 16,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 64,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-3B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 24,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-1B-Instruct\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 64,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 16,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 64,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-3B-Instruct\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 24,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"GPTNeoForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_heads,\n",
        "            \"n_heads\": hf_config.num_heads,\n",
        "            \"d_mlp\": hf_config.hidden_size * 4,\n",
        "            \"n_layers\": hf_config.num_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"attn_types\": hf_config.attention_layers,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": False,\n",
        "            \"use_local_attn\": True,\n",
        "            \"window_size\": hf_config.window_size,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPT2LMHeadModel\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.n_embd * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_ctx,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": hf_config.scale_attn_by_inverse_layer_idx,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"OPTForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.ffn_dim,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPTJForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": 4 * hf_config.n_embd,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_positions,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": hf_config.rotary_dim,\n",
        "            \"rotary_adjacent_pairs\": True,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPTNeoXForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "        rotary_pct = hf_config.rotary_pct\n",
        "        cfg_dict[\"rotary_dim\"] = round(rotary_pct * cfg_dict[\"d_head\"])\n",
        "    elif architecture == \"BertForMaskedLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"gelu\",\n",
        "            \"attention_dir\": \"bidirectional\",\n",
        "        }\n",
        "    elif architecture == \"MistralForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 2048,  # Capped due to memory issues\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"window_size\": 4096,\n",
        "            \"attn_types\": [\"local\"] * 32,\n",
        "            \"eps\": 1e-05,\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"gated_mlp\": True,\n",
        "            \"use_local_attn\": True,\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "        }\n",
        "    elif architecture == \"MixtralForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"dtype\": torch.bfloat16,\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,  # Capped due to memory issues\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"window_size\": hf_config.sliding_window,  # This is None, as no sliding window was used\n",
        "            \"attn_types\": [\"global\"] * 32,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"n_key_value_heads\": hf_config.num_key_value_heads,\n",
        "            \"gated_mlp\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"num_experts\": hf_config.num_local_experts,\n",
        "            \"experts_per_token\": hf_config.num_experts_per_tok,\n",
        "        }\n",
        "    elif architecture == \"BloomForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.hidden_size * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": 2048,  # Capped due to HF Tokenizer Constraints\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"gelu_fast\",\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"normalization_type\": \"LN\",\n",
        "            \"post_embedding_ln\": True,\n",
        "            \"positional_embedding_type\": \"alibi\",\n",
        "        }\n",
        "    elif architecture == \"GPT2LMHeadCustomModel\":\n",
        "        # santacoder\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.n_embd * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_positions,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"trust_remote_code\": \"santacoder\"\n",
        "            in official_model_name,  # Only santacoder needs trust_remote_code\n",
        "            \"scale_attn_by_inverse_layer_idx\": hf_config.scale_attn_by_inverse_layer_idx,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"LlamaForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"n_key_value_heads\": (\n",
        "                hf_config.num_key_value_heads\n",
        "                if hf_config.num_key_value_heads != hf_config.num_attention_heads\n",
        "                else None\n",
        "            ),\n",
        "            # This is done because the current implementation of GQA will use Grouped-Query Attention if\n",
        "            # n_key_value_heads is not None, but hf_config.num_key_value_heads is sometimes specified as\n",
        "            # the same as hf_config.num_attention_heads, in which case GQA should not be used.\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"QWenLMHeadModel\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size // 2,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"use_attn_scale\": hf_config.scale_attn_weights,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": hf_config.kv_channels,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"tokenizer_prepends_bos\": True,\n",
        "            \"trust_remote_code\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"Qwen2ForCausalLM\":\n",
        "        # Note that Qwen1.5 models have architecture type Qwen2ForCausalLM.\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"n_key_value_heads\": hf_config.num_key_value_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"tokenizer_prepends_bos\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"PhiForCausalLM\":\n",
        "        # Architecture for microsoft/phi models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"LN\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "        }\n",
        "        partial_rotary_factor = hf_config.partial_rotary_factor\n",
        "        cfg_dict[\"rotary_dim\"] = round(partial_rotary_factor * cfg_dict[\"d_head\"])\n",
        "    elif architecture == \"Phi3ForCausalLM\":\n",
        "        # Architecture for microsoft/phi3 models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"gated_mlp\": True,\n",
        "            \"parallel_attn_mlp\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "        }\n",
        "\n",
        "    elif official_model_name.startswith(\"google/gemma-2b\"):\n",
        "        # Architecture for Gemma 2b and Gemma 2b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 8,\n",
        "            \"d_mlp\": 16384,\n",
        "            \"n_layers\": 18,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_new\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"rotary_dim\": 256,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 1,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-7b\"):\n",
        "        # Architecture for Gemma 7b and Gemma 7b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 16,\n",
        "            \"d_mlp\": 24576,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_new\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"rotary_dim\": 256,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 16,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-2b\"):\n",
        "        # Architecture for Gemma-2 2b and Gemma-2 2b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2304,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 8,\n",
        "            \"d_mlp\": 9216,\n",
        "            \"n_layers\": 26,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 4,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 21,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-9b\"):\n",
        "        # Architecture for Gemma-2 9b and Gemma-2 9b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3584,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 16,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 42,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 21,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-27b\"):\n",
        "        # Architecture for Gemma-2 27b and Gemma-2 27b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4608,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 36864,\n",
        "            \"n_layers\": 46,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"attn_scale\": 12.0,\n",
        "            \"n_key_value_heads\": 16,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 23,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif architecture == \"T5ForConditionalGeneration\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.d_model,\n",
        "            \"d_head\": hf_config.d_kv,\n",
        "            \"n_heads\": hf_config.num_heads,\n",
        "            \"d_mlp\": hf_config.d_ff,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"n_layers\": hf_config.num_layers,\n",
        "            \"n_ctx\": hf_config.max_length,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"act_fn\": hf_config.feed_forward_proj,\n",
        "            \"positional_embedding_type\": \"relative_positional_bias\",\n",
        "            \"relative_attention_max_distance\": hf_config.relative_attention_max_distance,\n",
        "            \"relative_attention_num_buckets\": hf_config.relative_attention_num_buckets,\n",
        "            \"decoder_start_token_id\": hf_config.decoder_start_token_id,\n",
        "            \"attention_dir\": \"bidirectional\",\n",
        "            \"use_attn_scale\": False,\n",
        "            \"tie_word_embeddings\": hf_config.tie_word_embeddings,\n",
        "        }\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{architecture} is not currently supported.\")\n",
        "    # All of these models use LayerNorm\n",
        "    cfg_dict[\"original_architecture\"] = architecture\n",
        "    # The name such that AutoTokenizer.from_pretrained works\n",
        "    cfg_dict[\"tokenizer_name\"] = official_model_name\n",
        "    if kwargs.get(\"trust_remote_code\", False):\n",
        "        cfg_dict[\"trust_remote_code\"] = True\n",
        "    return cfg_dict\n",
        "\n",
        "\n",
        "def convert_neel_model_config(official_model_name: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads the config for a model trained by me (NeelNanda), converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    AutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.\n",
        "    \"\"\"\n",
        "    official_model_name = get_official_model_name(official_model_name)\n",
        "    cfg_json: dict = utils.download_file_from_hf(official_model_name, \"config.json\", **kwargs)\n",
        "    cfg_arch = cfg_json.get(\n",
        "        \"architecture\", \"neel\" if \"_old\" not in official_model_name else \"neel-solu-old\"\n",
        "    )\n",
        "    cfg_dict = {\n",
        "        \"d_model\": cfg_json[\"d_model\"],\n",
        "        \"n_layers\": cfg_json[\"n_layers\"],\n",
        "        \"d_mlp\": cfg_json[\"d_mlp\"],\n",
        "        \"d_head\": cfg_json[\"d_head\"],\n",
        "        \"n_heads\": cfg_json[\"n_heads\"],\n",
        "        \"n_ctx\": cfg_json[\"n_ctx\"],\n",
        "        \"d_vocab\": cfg_json[\"d_vocab\"],\n",
        "        \"tokenizer_name\": cfg_json.get(\"tokenizer_name\", None),\n",
        "        \"act_fn\": cfg_json[\"act_fn\"],\n",
        "        \"attn_only\": cfg_json[\"attn_only\"],\n",
        "        \"final_rms\": cfg_json.get(\"final_rms\", False),\n",
        "        \"original_architecture\": cfg_arch,\n",
        "    }\n",
        "    if \"normalization\" in cfg_json:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization\"]\n",
        "    else:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization_type\"]\n",
        "    if \"shortformer_pos\" in cfg_json:\n",
        "        cfg_dict[\"positional_embedding_type\"] = (\n",
        "            \"shortformer\" if cfg_json[\"shortformer_pos\"] else \"standard\"\n",
        "        )\n",
        "    else:\n",
        "        cfg_dict[\"positional_embedding_type\"] = \"standard\"\n",
        "    return cfg_dict\n",
        "\n",
        "def get_pretrained_model_config(\n",
        "    model_name: str,\n",
        "    hf_cfg: Optional[dict] = None,\n",
        "    checkpoint_index: Optional[int] = None,\n",
        "    checkpoint_value: Optional[int] = None,\n",
        "    fold_ln: bool = False,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    n_devices: int = 1,\n",
        "    default_prepend_bos: bool = True,\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    first_n_layers: Optional[int] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"Returns the pretrained model config as an HookedTransformerConfig object.\n",
        "\n",
        "    There are two types of pretrained models: HuggingFace models (where\n",
        "    AutoModel and AutoConfig work), and models trained by me (NeelNanda) which\n",
        "    aren't as integrated with HuggingFace infrastructure.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model. This can be either the official\n",
        "            HuggingFace model name, or the name of a model trained by me\n",
        "            (NeelNanda).\n",
        "        hf_cfg (dict, optional): Config of a loaded pretrained HF model,\n",
        "            converted to a dictionary.\n",
        "        checkpoint_index (int, optional): If loading from a\n",
        "            checkpoint, the index of the checkpoint to load. Defaults to None.\n",
        "        checkpoint_value (int, optional): If loading from a checkpoint, the\n",
        "        value of\n",
        "            the checkpoint to load, ie the step or token number (each model has\n",
        "            checkpoints labelled with exactly one of these). Defaults to None.\n",
        "        fold_ln (bool, optional): Whether to fold the layer norm into the\n",
        "            subsequent linear layers (see HookedTransformer.fold_layer_norm for\n",
        "            details). Defaults to False.\n",
        "        device (str, optional): The device to load the model onto. By\n",
        "            default will load to CUDA if available, else CPU.\n",
        "        n_devices (int, optional): The number of devices to split the model across. Defaults to 1.\n",
        "        default_prepend_bos (bool, optional): Default behavior of whether to prepend the BOS token when the\n",
        "            methods of HookedTransformer process input text to tokenize (only when input is a string).\n",
        "            Defaults to True - even for models not explicitly trained with this, heads often use the\n",
        "            first position as a resting position and accordingly lose information from the first token,\n",
        "            so this empirically seems to give better results. To change the default behavior to False, pass in\n",
        "            default_prepend_bos=False. Note that you can also locally override the default behavior by passing\n",
        "            in prepend_bos=True/False when you call a method that processes the input string.\n",
        "        dtype (torch.dtype, optional): The dtype to load the TransformerLens model in.\n",
        "        kwargs: Other optional arguments passed to HuggingFace's from_pretrained.\n",
        "            Also given to other HuggingFace functions when compatible.\n",
        "\n",
        "    \"\"\"\n",
        "    if Path(model_name).exists():\n",
        "        # If the model_name is a path, it's a local model\n",
        "        cfg_dict = convert_hf_model_config(model_name, **kwargs)\n",
        "        official_model_name = model_name\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(model_name)\n",
        "    if (\n",
        "        official_model_name.startswith(\"NeelNanda\")\n",
        "        or official_model_name.startswith(\"ArthurConmy\")\n",
        "        or official_model_name.startswith(\"Baidicoot\")\n",
        "    ):\n",
        "        cfg_dict = convert_neel_model_config(official_model_name, **kwargs)\n",
        "    else:\n",
        "        if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(\n",
        "            \"trust_remote_code\", False\n",
        "        ):\n",
        "            logging.warning(\n",
        "                f\"Loading model {official_model_name} requires setting trust_remote_code=True\"\n",
        "            )\n",
        "            kwargs[\"trust_remote_code\"] = True\n",
        "        cfg_dict = convert_hf_model_config(official_model_name, **kwargs)\n",
        "    # Processing common to both model types\n",
        "    # Remove any prefix, saying the organization who made a model.\n",
        "    cfg_dict[\"model_name\"] = official_model_name.split(\"/\")[-1]\n",
        "    # Don't need to initialize weights, we're loading from pretrained\n",
        "    cfg_dict[\"init_weights\"] = False\n",
        "\n",
        "    if (\n",
        "        \"positional_embedding_type\" in cfg_dict\n",
        "        and cfg_dict[\"positional_embedding_type\"] == \"shortformer\"\n",
        "        and fold_ln\n",
        "    ):\n",
        "        logging.warning(\n",
        "            \"You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_ln=False instead.\"\n",
        "        )\n",
        "        fold_ln = False\n",
        "\n",
        "    if device is not None:\n",
        "        cfg_dict[\"device\"] = device\n",
        "\n",
        "    cfg_dict[\"dtype\"] = dtype\n",
        "\n",
        "    if fold_ln:\n",
        "        if cfg_dict[\"normalization_type\"] in [\"LN\", \"LNPre\"]:\n",
        "            cfg_dict[\"normalization_type\"] = \"LNPre\"\n",
        "        elif cfg_dict[\"normalization_type\"] in [\"RMS\", \"RMSPre\"]:\n",
        "            cfg_dict[\"normalization_type\"] = \"RMSPre\"\n",
        "        else:\n",
        "            logging.warning(\"Cannot fold in layer norm, normalization_type is not LN.\")\n",
        "\n",
        "    if checkpoint_index is not None or checkpoint_value is not None:\n",
        "        checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(\n",
        "            official_model_name,\n",
        "            **kwargs,\n",
        "        )\n",
        "        cfg_dict[\"from_checkpoint\"] = True\n",
        "        cfg_dict[\"checkpoint_label_type\"] = checkpoint_label_type\n",
        "        if checkpoint_index is not None:\n",
        "            cfg_dict[\"checkpoint_index\"] = checkpoint_index\n",
        "            cfg_dict[\"checkpoint_value\"] = checkpoint_labels[checkpoint_index]\n",
        "        elif checkpoint_value is not None:\n",
        "            assert (\n",
        "                checkpoint_value in checkpoint_labels\n",
        "            ), f\"Checkpoint value {checkpoint_value} is not in list of available checkpoints\"\n",
        "            cfg_dict[\"checkpoint_value\"] = checkpoint_value\n",
        "            cfg_dict[\"checkpoint_index\"] = checkpoint_labels.index(checkpoint_value)\n",
        "    else:\n",
        "        cfg_dict[\"from_checkpoint\"] = False\n",
        "\n",
        "    cfg_dict[\"device\"] = device\n",
        "    cfg_dict[\"n_devices\"] = n_devices\n",
        "    cfg_dict[\"default_prepend_bos\"] = default_prepend_bos\n",
        "    if hf_cfg is not None:\n",
        "        cfg_dict[\"load_in_4bit\"] = hf_cfg.get(\"quantization_config\", {}).get(\"load_in_4bit\", False)\n",
        "    if first_n_layers is not None:\n",
        "        cfg_dict[\"n_layers\"] = first_n_layers\n",
        "\n",
        "    cfg = HookedTransformerConfig.from_dict(cfg_dict)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def get_num_params_of_pretrained(model_name):\n",
        "    \"\"\"\n",
        "    Returns the number of parameters of a pretrained model, used to filter to only run code for sufficiently small models.\n",
        "    \"\"\"\n",
        "    cfg = get_pretrained_model_config(model_name)\n",
        "    return cfg.n_params\n",
        "\n",
        "\n",
        "# %% Load checkpointed model state dicts\n",
        "# The steps for which there are checkpoints in the stanford crfm models\n",
        "STANFORD_CRFM_CHECKPOINTS = (\n",
        "    list(range(0, 100, 10))\n",
        "    + list(range(100, 2000, 50))\n",
        "    + list(range(2000, 20000, 100))\n",
        "    + list(range(20000, 400000 + 1, 1000))\n",
        ")\n",
        "\n",
        "# Linearly spaced checkpoints for Pythia models, taken every 1000 steps.\n",
        "# Batch size 2,097,152 tokens, so checkpoints every 2.1B tokens\n",
        "PYTHIA_CHECKPOINTS = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512] + list(\n",
        "    range(1000, 143000 + 1, 1000)\n",
        ")\n",
        "# Pythia V1 has log-spaced early checkpoints (see line above), but V0 doesn't\n",
        "PYTHIA_V0_CHECKPOINTS = list(range(1000, 143000 + 1, 1000))\n",
        "\n",
        "\n",
        "def get_checkpoint_labels(model_name: str, **kwargs):\n",
        "    \"\"\"Returns the checkpoint labels for a given model, and the label_type\n",
        "    (step or token). Raises an error for models that are not checkpointed.\"\"\"\n",
        "    official_model_name = get_official_model_name(model_name)\n",
        "    if official_model_name.startswith(\"stanford-crfm/\"):\n",
        "        return STANFORD_CRFM_CHECKPOINTS, \"step\"\n",
        "    elif official_model_name.startswith(\"EleutherAI/pythia\"):\n",
        "        if \"v0\" in official_model_name:\n",
        "            return PYTHIA_V0_CHECKPOINTS, \"step\"\n",
        "        else:\n",
        "            logging.warning(\n",
        "                \"Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\"\n",
        "            )\n",
        "            return PYTHIA_CHECKPOINTS, \"step\"\n",
        "    elif official_model_name.startswith(\"NeelNanda/\"):\n",
        "        api = HfApi()\n",
        "        files_list = api.list_repo_files(\n",
        "            official_model_name,\n",
        "            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),\n",
        "        )\n",
        "        labels = []\n",
        "        for file_name in files_list:\n",
        "            match = re.match(r\"checkpoints/.*_(\\d*)\\.pth\", file_name)\n",
        "            if match:\n",
        "                labels.append(int(match.group(1)))\n",
        "        if labels[-1] > 1e9:\n",
        "            label_type = \"token\"\n",
        "        else:\n",
        "            label_type = \"step\"\n",
        "        return labels, label_type\n",
        "    elif ( # NEW\n",
        "        official_model_name.startswith(\"zwimpee\") # NEW\n",
        "        or official_model_name.startswith(\"sam.greydanus\") # NEW\n",
        "        or official_model_name.startswith(\"cursivetransformer\") # NEW\n",
        "    ): # NEW\n",
        "        # NEW\n",
        "        # - [ ] TODO: Implement this!\n",
        "    else:\n",
        "        raise ValueError(f\"Model {official_model_name} is not checkpointed.\")\n",
        "\n",
        "\n",
        "# %% Loading state dicts\n",
        "def get_pretrained_state_dict(\n",
        "    official_model_name: str,\n",
        "    cfg: HookedTransformerConfig,\n",
        "    hf_model=None,\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    **kwargs,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Loads in the model weights for a pretrained model, and processes them to\n",
        "    have the HookedTransformer parameter names and shapes. Supports checkpointed\n",
        "    models (and expects the checkpoint info to be stored in the config object)\n",
        "\n",
        "    hf_model: Optionally, a HuggingFace model object. If provided, we will use\n",
        "        these weights rather than reloading the model.\n",
        "    dtype: The dtype to load the HuggingFace model in.\n",
        "    kwargs: Other optional arguments passed to HuggingFace's from_pretrained.\n",
        "        Also given to other HuggingFace functions when compatible.\n",
        "    \"\"\"\n",
        "    if \"torch_dtype\" in kwargs:\n",
        "        dtype = kwargs[\"torch_dtype\"]\n",
        "        del kwargs[\"torch_dtype\"]\n",
        "    if Path(official_model_name).exists():\n",
        "        official_model_name = str(Path(official_model_name).resolve())\n",
        "        logging.info(f\"Loading model from local path {official_model_name}\")\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(official_model_name)\n",
        "    if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(\n",
        "        \"trust_remote_code\", False\n",
        "    ):\n",
        "        logging.warning(\n",
        "            f\"Loading model {official_model_name} state dict requires setting trust_remote_code=True\"\n",
        "        )\n",
        "        kwargs[\"trust_remote_code\"] = True\n",
        "    if (\n",
        "        official_model_name.startswith(\"NeelNanda\")\n",
        "        or official_model_name.startswith(\"ArthurConmy\")\n",
        "        or official_model_name.startswith(\"Baidicoot\")\n",
        "        or official_model_name.startswith(\"cursivetransformer\") # NEW\n",
        "    ):\n",
        "        api = HfApi()\n",
        "        repo_files = api.list_repo_files(\n",
        "            official_model_name,\n",
        "            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),\n",
        "        )\n",
        "        if cfg.from_checkpoint:\n",
        "            file_name = list(\n",
        "                filter(lambda x: x.endswith(f\"{cfg.checkpoint_value}.pth\"), repo_files)\n",
        "            )[0]\n",
        "        else:\n",
        "            file_name = list(filter(lambda x: x.endswith(\"final.pth\"), repo_files))[0]\n",
        "        state_dict = utils.download_file_from_hf(official_model_name, file_name, **kwargs)\n",
        "\n",
        "        # Convert to dtype\n",
        "        state_dict = {k: v.to(dtype) for k, v in state_dict.items()}\n",
        "\n",
        "        if cfg.original_architecture == \"neel-solu-old\":\n",
        "            state_dict = convert_neel_solu_old_weights(state_dict, cfg)\n",
        "        elif cfg.original_architecture == \"mingpt\":\n",
        "            state_dict = convert_mingpt_weights(state_dict, cfg)\n",
        "        elif cfg.original_architecture == \"cursivetransformer\": # NEW\n",
        "            state_dict = convert_cursivetransformer_weights(state_dict, cfg)\n",
        "        return state_dict\n",
        "    else:\n",
        "        if cfg.from_checkpoint:\n",
        "            huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "            if official_model_name.startswith(\"stanford-crfm\"):\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    revision=f\"checkpoint-{cfg.checkpoint_value}\",\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            elif official_model_name.startswith(\"EleutherAI/pythia\"):\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    revision=f\"step{cfg.checkpoint_value}\",\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Checkpoints for model {official_model_name} are not supported\")\n",
        "        elif hf_model is None:\n",
        "            huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "            if official_model_name in NON_HF_HOSTED_MODEL_NAMES:\n",
        "                raise NotImplementedError(\"Model not hosted on HuggingFace, must pass in hf_model\")\n",
        "            elif \"bert\" in official_model_name:\n",
        "                hf_model = BertForPreTraining.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            elif \"t5\" in official_model_name:\n",
        "                hf_model = T5ForConditionalGeneration.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            else:\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "\n",
        "            # Load model weights, and fold in layer norm weights\n",
        "\n",
        "        for param in hf_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if cfg.original_architecture == \"GPT2LMHeadModel\":\n",
        "            state_dict = convert_gpt2_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTNeoForCausalLM\":\n",
        "            state_dict = convert_neo_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"OPTForCausalLM\":\n",
        "            state_dict = convert_opt_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTJForCausalLM\":\n",
        "            state_dict = convert_gptj_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTNeoXForCausalLM\":\n",
        "            state_dict = convert_neox_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"LlamaForCausalLM\":\n",
        "            state_dict = convert_llama_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"BertForMaskedLM\":\n",
        "            state_dict = convert_bert_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"T5ForConditionalGeneration\":\n",
        "            state_dict = convert_t5_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"MistralForCausalLM\":\n",
        "            state_dict = convert_mistral_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"MixtralForCausalLM\":\n",
        "            state_dict = convert_mixtral_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"BloomForCausalLM\":\n",
        "            state_dict = convert_bloom_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPT2LMHeadCustomModel\":\n",
        "            state_dict = convert_coder_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"QWenLMHeadModel\":\n",
        "            state_dict = convert_qwen_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Qwen2ForCausalLM\":\n",
        "            state_dict = convert_qwen2_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"PhiForCausalLM\":\n",
        "            state_dict = convert_phi_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Phi3ForCausalLM\":\n",
        "            state_dict = convert_phi3_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GemmaForCausalLM\":\n",
        "            state_dict = convert_gemma_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Gemma2ForCausalLM\":\n",
        "            state_dict = convert_gemma_weights(hf_model, cfg)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Loading weights from the architecture is not currently supported: {cfg.original_architecture}, generated from model name {cfg.model_name}. Feel free to open an issue on GitHub to request this feature.\"\n",
        "            )\n",
        "\n",
        "        return state_dict\n",
        "\n",
        "\n",
        "def fill_missing_keys(model, state_dict):\n",
        "    \"\"\"Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.\n",
        "\n",
        "    This function is assumed to be run before weights are initialized.\n",
        "\n",
        "    Args:\n",
        "        state_dict (dict): State dict from a pretrained model\n",
        "\n",
        "    Returns:\n",
        "        dict: State dict with missing keys filled in\n",
        "    \"\"\"\n",
        "    # Get the default state dict\n",
        "    default_state_dict = model.state_dict()\n",
        "    # Get the keys that are missing from the pretrained model\n",
        "    missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())\n",
        "    # Fill in the missing keys with the default initialization\n",
        "    for key in missing_keys:\n",
        "        if \"hf_model\" in key:\n",
        "            # Skip keys that are from the HuggingFace model, if loading from HF.\n",
        "            continue\n",
        "        if \"W_\" in key:\n",
        "            logging.warning(\n",
        "                \"Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}\".format(\n",
        "                    key\n",
        "                )\n",
        "            )\n",
        "        state_dict[key] = default_state_dict[key]\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    d_model: int = 768\n",
        "    debug: bool = True\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024\n",
        "    d_head: int = 64\n",
        "    d_mlp: int = 3072\n",
        "    n_heads: int = 12\n",
        "    n_layers: int = 12\n",
        "\n",
        "\n",
        "# Returns the configuration parameters of the model as a basic Config dataclass\n",
        "def get_basic_config(model_name: str, **kwargs) -> Config:\n",
        "    return Config(\n",
        "        **{\n",
        "            k: v\n",
        "            for k, v in get_pretrained_model_config(model_name, **kwargs).to_dict().items()\n",
        "            if k\n",
        "            in [\n",
        "                \"d_model\",\n",
        "                \"debug\",\n",
        "                \"layer_norm_eps\",\n",
        "                \"d_vocab\",\n",
        "                \"init_range\",\n",
        "                \"n_ctx\",\n",
        "                \"d_head\",\n",
        "                \"d_mlp\",\n",
        "                \"n_heads\",\n",
        "                \"n_layers\",\n",
        "            ]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "KJxG7qFMQsVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}