{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOiiu3Xxlkj0CM/WTiZioFu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HookedCursiveTransformer"
      ],
      "metadata": {
        "id": "9N5SAaDHtG0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "6hl6g0b0fz2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import types\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from jaxtyping import Float, Int\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# import circuitsvis as cv\n",
        "from IPython.display import display\n",
        "\n",
        "from typing import Callable, Dict, Optional, Union\n",
        "\n",
        "from cursivetransformer.model import (\n",
        "    get_all_args,\n",
        "    get_checkpoint,\n",
        "    get_latest_checkpoint_artifact\n",
        ")\n",
        "from cursivetransformer.data import (\n",
        "    create_datasets,\n",
        "    offsets_to_strokes\n",
        ")\n",
        "from cursivetransformer.sample import (\n",
        "    generate,\n",
        "    generate_n_words,\n",
        "    plot_strokes\n",
        ")\n",
        "from transformer_lens import (\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig\n",
        ")\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.utils import (\n",
        "    get_act_name,\n",
        "    repeat_along_head_dimension\n",
        ")\n",
        "from transformer_lens.utilities.attention import simple_attn_linear\n",
        "from transformer_lens.components import (\n",
        "    Attention,\n",
        "    GroupedQueryAttention,\n",
        "    LayerNorm,\n",
        "    LayerNormPre,\n",
        "    RMSNorm,\n",
        "    RMSNormPre,\n",
        "    Embed,\n",
        "    Unembed\n",
        ")\n",
        "from transformer_lens.components.mlps.can_be_used_as_mlp import CanBeUsedAsMLP\n",
        "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCacheEntry\n",
        "\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, Tuple, Union, cast\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing_extensions import Literal, get_args\n",
        "\n",
        "from transformer_lens.ActivationCache import ActivationCache\n",
        "from transformer_lens.HookedTransformer import HookedTransformer\n",
        "from transformer_lens.utils import is_lower_triangular, is_square"
      ],
      "metadata": {
        "id": "fZ-PLgwnfu9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()"
      ],
      "metadata": {
        "id": "JyoojEqSFrGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HookedCursiveTransformerConfig(HookedTransformerConfig):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Extract custom arguments\n",
        "        self.d_model_c = kwargs.pop('d_model_c', None)\n",
        "        self.context_block_size = kwargs.pop('context_block_size', None)\n",
        "        self.context_vocab_size = kwargs.pop('context_vocab_size', None)\n",
        "        # self.use_cross_attention = kwargs.pop('use_cross_attention', True)\n",
        "\n",
        "        # W&B specific parameters\n",
        "        self.wandb_entity = kwargs.pop('wandb_entity', None)\n",
        "        self.wandb_project = kwargs.pop('wandb_project', None)\n",
        "        self.load_from_run_id = kwargs.pop('load_from_run_id', None)\n",
        "\n",
        "        # Now, call the superclass constructor with the remaining kwargs\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict):\n",
        "        return cls(**config_dict)\n",
        "\n",
        "class HookedCursiveTransformer(HookedTransformer):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embed = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
        "        self.pos_embed = nn.Embedding(cfg.n_ctx, cfg.d_model)\n",
        "        self.embed_c = nn.Embedding(cfg.context_vocab_size, cfg.d_model_c)\n",
        "        self.pos_embed_c = nn.Embedding(cfg.context_block_size, cfg.d_model_c)\n",
        "\n",
        "        # Projection layer if d_model_c != d_model\n",
        "        if cfg.d_model_c != cfg.d_model:\n",
        "            self.context_proj = nn.Linear(cfg.d_model_c, cfg.d_model)\n",
        "        else:\n",
        "            self.context_proj = nn.Identity()\n",
        "\n",
        "        # Blocks\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg, block_index=_) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        # Final layers\n",
        "        self.ln_final = nn.LayerNorm(cfg.d_model)\n",
        "        self.unembed = nn.Linear(cfg.d_model, cfg.d_vocab, bias=True)\n",
        "\n",
        "        # Hook points\n",
        "        self.hook_embed = HookPoint()\n",
        "        self.hook_pos_embed = HookPoint()\n",
        "        self.hook_embed_c = HookPoint()\n",
        "        self.hook_pos_embed_c = HookPoint()\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def forward(self, tokens, context, return_type=\"logits\"):\n",
        "        B, T = tokens.shape\n",
        "        B_c, T_c = context.shape\n",
        "\n",
        "        token_embed = self.hook_embed(self.embed(tokens))\n",
        "        pos_embed = self.hook_pos_embed(self.pos_embed(torch.arange(T, device=tokens.device)))\n",
        "        x = token_embed + pos_embed\n",
        "\n",
        "        context_embed = self.hook_embed_c(self.embed_c(context))\n",
        "        context_pos_embed = self.hook_pos_embed_c(self.pos_embed_c(torch.arange(T_c, device=context.device)))\n",
        "        c = context_embed + context_pos_embed\n",
        "        c = self.context_proj(c)\n",
        "\n",
        "        # Generate causal mask\n",
        "        # causal_mask = torch.tril(torch.ones((T, T), device=tokens.device)).unsqueeze(0).unsqueeze(0).unsqueeze(1)\n",
        "        # causal_mask = causal_mask.to(dtype=torch.bool)\n",
        "\n",
        "        # Debug: Print mask shape and a sample\n",
        "        # print(\"Causal mask shape:\", causal_mask.shape)\n",
        "        # print(\"Causal mask sample:\\n\", causal_mask[0, 0, :5, :5])\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.unembed(x)\n",
        "\n",
        "        if return_type == \"logits\":\n",
        "            return logits\n",
        "        elif return_type == \"loss\":\n",
        "            return self.loss_fn(logits, tokens[:, 1:])\n",
        "        elif return_type == \"both\":\n",
        "            return logits, self.loss_fn(logits, tokens[:, 1:])\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid return_type {return_type}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        model_name: str,\n",
        "        cfg,\n",
        "        tokenizer=None,\n",
        "        **from_pretrained_kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a pretrained CursiveTransformer model into the HookedCursiveTransformer format.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name or path of the pretrained model.\n",
        "            cfg: The configuration object for the model.\n",
        "            tokenizer: The tokenizer to use (optional).\n",
        "            **from_pretrained_kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            HookedCursiveTransformer: The loaded model.\n",
        "        \"\"\"\n",
        "        print(f\"Loading pretrained model {model_name}\")\n",
        "\n",
        "        # Initialize the HookedCursiveTransformer with the given config\n",
        "        model = cls(cfg)\n",
        "\n",
        "        # Load the state dict from the wandb artifact\n",
        "        state_dict = cls.load_state_dict_from_wandb(cfg)\n",
        "\n",
        "        # Convert the state dict to match HookedCursiveTransformer format\n",
        "        converted_state_dict = cls.convert_cursivetransformer_weights(state_dict, cfg)\n",
        "\n",
        "        # Load the converted state dict into the model\n",
        "        model.load_state_dict(converted_state_dict, strict=False)\n",
        "\n",
        "        if tokenizer is not None:\n",
        "            model.tokenizer = tokenizer\n",
        "\n",
        "        print(f\"Successfully loaded pretrained model {model_name}\")\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def load_state_dict_from_wandb(args):\n",
        "        artifact = get_latest_checkpoint_artifact(args)\n",
        "        artifact_dir = artifact.download()\n",
        "        checkpoint = torch.load(os.path.join(artifact_dir, \"best_checkpoint.pt\"), weights_only=True)\n",
        "        return checkpoint['model_state_dict']\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_cursivetransformer_weights(state_dict, cfg):\n",
        "        \"\"\"Convert CursiveTransformer weights to HookedCursiveTransformer format.\"\"\"\n",
        "        new_state_dict = {}\n",
        "\n",
        "        # Embeddings\n",
        "        new_state_dict[\"embed.W_E\"] = state_dict[\"transformer.wte.weight\"]\n",
        "        new_state_dict[\"pos_embed.W_pos\"] = state_dict[\"transformer.wpe.weight\"]\n",
        "        new_state_dict[\"embed_c.W_E\"] = state_dict[\"transformer.wce.weight\"]\n",
        "        new_state_dict[\"pos_embed_c.W_pos\"] = state_dict[\"transformer.wcpe.weight\"]\n",
        "\n",
        "        for l in range(cfg.n_layers):\n",
        "            # Layer Norms\n",
        "            new_state_dict[f'blocks.{l}.ln1.w'] = state_dict[f'transformer.h.{l}.ln_1.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln1.b'] = state_dict[f'transformer.h.{l}.ln_1.bias']\n",
        "            new_state_dict[f'blocks.{l}.ln2.w'] = state_dict[f'transformer.h.{l}.ln_2.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln2.b'] = state_dict[f'transformer.h.{l}.ln_2.bias']\n",
        "            new_state_dict[f'blocks.{l}.ln3.w'] = state_dict[f'transformer.h.{l}.ln_3.weight']\n",
        "            new_state_dict[f'blocks.{l}.ln3.b'] = state_dict[f'transformer.h.{l}.ln_3.bias']\n",
        "\n",
        "            # Self-Attention\n",
        "            W_qkv = state_dict[f'transformer.h.{l}.attn.c_attn.weight']\n",
        "            b_qkv = state_dict[f'transformer.h.{l}.attn.c_attn.bias']\n",
        "            W_q, W_k, W_v = W_qkv.t().chunk(3, dim=1)\n",
        "            b_q, b_k, b_v = b_qkv.chunk(3, dim=0)\n",
        "\n",
        "            new_state_dict[f'blocks.{l}.attn.W_Q'] = W_q.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.W_K'] = W_k.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.W_V'] = W_v.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_Q'] = b_q.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_K'] = b_k.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_V'] = b_v.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_o = state_dict[f'transformer.h.{l}.attn.c_proj.weight']\n",
        "            new_state_dict[f'blocks.{l}.attn.W_O'] = W_o.t().reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "            new_state_dict[f'blocks.{l}.attn.b_O'] = state_dict[f'transformer.h.{l}.attn.c_proj.bias']\n",
        "\n",
        "            # Cross-Attention\n",
        "            W_q = state_dict[f'transformer.h.{l}.cross_attn.c_attn_q.weight']\n",
        "            b_q = state_dict[f'transformer.h.{l}.cross_attn.c_attn_q.bias']\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_Q'] = W_q.t().reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_Q'] = b_q.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_kv = state_dict[f'transformer.h.{l}.cross_attn.c_attn_kv.weight']\n",
        "            b_kv = state_dict[f'transformer.h.{l}.cross_attn.c_attn_kv.bias']\n",
        "            W_k, W_v = W_kv.t().chunk(2, dim=1)\n",
        "            b_k, b_v = b_kv.chunk(2, dim=0)\n",
        "\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_K'] = W_k.t().reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_V'] = W_v.t().reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_K'] = b_k.reshape(cfg.n_heads, cfg.d_head)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_V'] = b_v.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "            W_o = state_dict[f'transformer.h.{l}.cross_attn.c_proj.weight']\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.W_O'] = W_o.t().reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "            new_state_dict[f'blocks.{l}.cross_attn.b_O'] = state_dict[f'transformer.h.{l}.cross_attn.c_proj.bias']\n",
        "\n",
        "            # MLP\n",
        "            new_state_dict[f'blocks.{l}.mlp.W_in'] = state_dict[f'transformer.h.{l}.mlp.c_fc.weight'].t()\n",
        "            new_state_dict[f'blocks.{l}.mlp.b_in'] = state_dict[f'transformer.h.{l}.mlp.c_fc.bias']\n",
        "            new_state_dict[f'blocks.{l}.mlp.W_out'] = state_dict[f'transformer.h.{l}.mlp.c_proj.weight'].t()\n",
        "            new_state_dict[f'blocks.{l}.mlp.b_out'] = state_dict[f'transformer.h.{l}.mlp.c_proj.bias']\n",
        "\n",
        "        # Final layer norm and unembedding\n",
        "        new_state_dict[\"ln_final.w\"] = state_dict[\"transformer.ln_f.weight\"]\n",
        "        new_state_dict[\"ln_final.b\"] = state_dict[\"transformer.ln_f.bias\"]\n",
        "        new_state_dict[\"unembed.W_U\"] = state_dict[\"lm_head.weight\"].t()\n",
        "        new_state_dict[\"unembed.b_U\"] = state_dict.get(\"lm_head.bias\", torch.zeros(cfg.d_vocab))\n",
        "\n",
        "        return new_state_dict\n",
        "\n",
        "    def loss_fn(self, logits, targets):\n",
        "        return F.cross_entropy(logits[:, :-1, :].reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-1)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: Union[Dict, HookedTransformerConfig], block_index):\n",
        "        super().__init__()\n",
        "        self.cfg = HookedCursiveTransformerConfig.unwrap(cfg)\n",
        "\n",
        "        self.cfg_c = copy.deepcopy(cfg)\n",
        "        self.cfg_c.attention_dir = 'bidirectional'\n",
        "\n",
        "        # Determine normalization type\n",
        "        if self.cfg.normalization_type == \"LN\":\n",
        "            normalization_layer = LayerNorm\n",
        "        elif self.cfg.normalization_type == \"LNPre\":\n",
        "            normalization_layer = LayerNormPre\n",
        "        elif self.cfg.normalization_type == \"RMS\":\n",
        "            normalization_layer = RMSNorm\n",
        "        elif self.cfg.normalization_type == \"RMSPre\":\n",
        "            normalization_layer = RMSNormPre\n",
        "        elif self.cfg.normalization_type is None:\n",
        "            normalization_layer = lambda cfg: nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid normalization_type: {self.cfg.normalization_type}\")\n",
        "\n",
        "        # Initialize layers\n",
        "        self.ln1 = normalization_layer(cfg)\n",
        "        self.ln2 = normalization_layer(cfg)\n",
        "        self.ln3 = normalization_layer(cfg)\n",
        "\n",
        "        attention_class = Attention\n",
        "        cross_attention_class = CrossAttention\n",
        "        self.attn = attention_class(self.cfg, \"global\", block_index)\n",
        "        self.cross_attn = cross_attention_class(self.cfg_c, \"global\", block_index)\n",
        "        self.mlp = MLPFactory.create_mlp(self.cfg)\n",
        "\n",
        "        # Hook points\n",
        "        self.hook_attn_in = HookPoint()\n",
        "        self.hook_cross_attn_in = HookPoint()\n",
        "        self.hook_mlp_in = HookPoint()\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_cross_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid1 = HookPoint()\n",
        "        self.hook_resid_mid2 = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
        "        context: Float[torch.Tensor, \"batch context_len d_model_c\"],\n",
        "        attention_mask: Optional[Int[torch.Tensor, \"batch 1 seq_len seq_len\"]] = None,\n",
        "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "        resid_pre = self.hook_resid_pre(resid_pre)\n",
        "\n",
        "        # Self-attention\n",
        "        attn_in = self.hook_attn_in(resid_pre)\n",
        "        normalized_attn_in = self.ln1(attn_in)\n",
        "        attn_out = self.attn(\n",
        "            query_input=normalized_attn_in,\n",
        "            key_input=normalized_attn_in,\n",
        "            value_input=normalized_attn_in,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        attn_out = self.hook_attn_out(attn_out)\n",
        "        resid_mid1 = self.hook_resid_mid1(resid_pre + attn_out)\n",
        "\n",
        "        # Cross-attention\n",
        "        cross_attn_in = self.hook_cross_attn_in(resid_mid1)\n",
        "        normalized_cross_attn_in = self.ln2(cross_attn_in)\n",
        "        cross_attn_out = self.cross_attn(\n",
        "            query_input=normalized_cross_attn_in,\n",
        "            key_input=context,\n",
        "            value_input=context,\n",
        "            attention_mask=None,  # No mask for cross-attention\n",
        "        )\n",
        "        cross_attn_out = self.hook_cross_attn_out(cross_attn_out)\n",
        "        resid_mid2 = self.hook_resid_mid2(resid_mid1 + cross_attn_out)\n",
        "\n",
        "        # MLP\n",
        "        mlp_in = self.hook_mlp_in(resid_mid2)\n",
        "        normalized_mlp_in = self.ln3(mlp_in)\n",
        "        mlp_out = self.mlp(normalized_mlp_in)\n",
        "        mlp_out = self.hook_mlp_out(mlp_out)\n",
        "\n",
        "        resid_post = self.hook_resid_post(resid_mid2 + mlp_out)\n",
        "        return resid_post\n",
        "\n",
        "class CrossAttention(Attention):\n",
        "    def __init__(self, cfg, attn_type='global', layer_id=None):\n",
        "        super().__init__(cfg, attn_type, layer_id)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Initialize W_K and W_V for cross-attention with d_model_c\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.empty(\n",
        "                self.cfg.n_heads, self.cfg.d_model_c, self.cfg.d_head, dtype=self.cfg.dtype\n",
        "            )\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.empty(\n",
        "                self.cfg.n_heads, self.cfg.d_model_c, self.cfg.d_head, dtype=self.cfg.dtype\n",
        "            )\n",
        "        )\n",
        "        self.b_K = nn.Parameter(\n",
        "            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)\n",
        "        )\n",
        "        self.b_V = nn.Parameter(\n",
        "            torch.zeros(self.cfg.n_heads, self.cfg.d_head, dtype=self.cfg.dtype)\n",
        "        )\n",
        "        # W_Q and W_O are inherited from AbstractAttention\n",
        "\n",
        "        # Override the attention direction\n",
        "        # self.cfg.attention_dir = 'bidirectional'\n",
        "\n",
        "    def calculate_qkv_matrices(self, query_input, key_input, value_input):\n",
        "        # query_input: [batch, pos, d_model]\n",
        "        # key_input and value_input: [batch, kv_pos, d_model_c]\n",
        "\n",
        "        # Use W_Q and b_Q from AbstractAttention for queries\n",
        "        q = self.hook_q(simple_attn_linear(query_input, self.W_Q, self.b_Q))\n",
        "        # Use custom W_K and b_K for keys with d_model_c\n",
        "        k = self.hook_k(simple_attn_linear(key_input, self.W_K, self.b_K))\n",
        "        v = self.hook_v(simple_attn_linear(value_input, self.W_V, self.b_V))\n",
        "        return q, k, v\n",
        "\n",
        "\n",
        "class MLPFactory:\n",
        "    @staticmethod\n",
        "    def create_mlp(cfg):\n",
        "        return MLP(cfg)\n",
        "\n",
        "class MLP(CanBeUsedAsMLP):\n",
        "    def __init__(self, cfg: Union[Dict, HookedTransformerConfig]):\n",
        "        super().__init__(cfg)\n",
        "        self.select_activation_function()\n",
        "\n",
        "        self.W_in = nn.Parameter(torch.empty(self.cfg.d_model, self.d_mlp, dtype=self.cfg.dtype))\n",
        "        self.b_in = nn.Parameter(torch.zeros(self.d_mlp, dtype=self.cfg.dtype))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.empty(self.d_mlp, self.cfg.d_model, dtype=self.cfg.dtype))\n",
        "        self.b_out = nn.Parameter(torch.zeros(self.cfg.d_model, dtype=self.cfg.dtype))\n",
        "\n",
        "        self.hook_pre = HookPoint()  # [batch, pos, d_mlp]\n",
        "        self.hook_post = HookPoint()  # [batch, pos, d_mlp]\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[torch.Tensor, \"batch pos d_model\"]\n",
        "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
        "        # This is equivalent to (roughly) W_in @ x + b_in. It's important to\n",
        "        # use a fused addmm to ensure it matches the Huggingface implementation\n",
        "        # exactly.\n",
        "        pre_act = self.hook_pre(batch_addmm(self.b_in, self.W_in, x))  # [batch, pos, d_mlp]\n",
        "\n",
        "        if (\n",
        "            self.cfg.is_layer_norm_activation()\n",
        "            and self.hook_mid is not None\n",
        "            and self.ln is not None\n",
        "        ):\n",
        "            mid_act = self.hook_mid(self.act_fn(pre_act))  # [batch, pos, d_mlp]\n",
        "            post_act = self.hook_post(self.ln(mid_act))\n",
        "        else:\n",
        "            post_act = self.hook_post(self.act_fn(pre_act))  # [batch, pos, d_mlp]\n",
        "        return batch_addmm(self.b_out, self.W_out, post_act)\n",
        "\n",
        "\n",
        "def convert_cursivetransformer_model_config(args):\n",
        "    cfg_dict = {\n",
        "        # Standard parameters\n",
        "        \"d_model\": args.n_embd,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embd * 4,\n",
        "        \"d_head\": args.n_embd // args.n_ctx_head,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": args.vocab_size,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": \"gelu_new\",\n",
        "        \"attn_only\": False,\n",
        "        \"final_rms\": False,\n",
        "        \"attention_dir\": \"causal\",\n",
        "        \"original_architecture\": \"cursivetransformer\",\n",
        "        \"normalization_type\": \"LN\",\n",
        "        \"init_weights\": False,\n",
        "        \"device\": args.device,\n",
        "        # Additional parameters for cross-attention\n",
        "        \"d_model_c\": args.n_embd2,\n",
        "        \"context_block_size\": args.context_block_size,\n",
        "        \"context_vocab_size\": args.context_vocab_size,\n",
        "        # \"use_cross_attention\": True,\n",
        "        # W&B specific parameters\n",
        "        \"wandb_entity\": args.wandb_entity,\n",
        "        \"wandb_project\": args.wandb_project,\n",
        "        \"load_from_run_id\": args.load_from_run_id,\n",
        "    }\n",
        "    return HookedCursiveTransformerConfig.from_dict(cfg_dict)\n",
        "\n",
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "model = HookedCursiveTransformer.from_pretrained(\"cursivetransformer\", cfg)"
      ],
      "metadata": {
        "id": "IRwfr66QBHg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add batch dimension to your inputs\n",
        "x, c, y = test_dataset[0]\n",
        "x = x.unsqueeze(0) # Shape: [1, 1000]\n",
        "c = c.unsqueeze(0) # Shape: [1, 50]\n",
        "y = y.unsqueeze(0) # Shape: [1, 1000]\n",
        "\n",
        "\n",
        "# - [ ] TODO: Add stroke plots to top-level figure\n",
        "def visualize_attention(model, x, c, layer_range=None, head_range=None, attn_type='self'):\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache(x, c, return_type=\"both\")\n",
        "\n",
        "    if layer_range is None:\n",
        "        layer_range = range(model.cfg.n_layers)\n",
        "    if head_range is None:\n",
        "        head_range = range(model.cfg.n_heads)\n",
        "\n",
        "    n_layers = len(layer_range)\n",
        "    n_heads = len(head_range)\n",
        "\n",
        "    fig, axes = plt.subplots(n_layers, n_heads, figsize=(4*n_heads, 4*n_layers), squeeze=False)\n",
        "\n",
        "    for i, layer in enumerate(layer_range):\n",
        "        for j, head in enumerate(head_range):\n",
        "            if attn_type == 'self':\n",
        "                attn_patterns = cache[f'blocks.{layer}.attn.hook_pattern']\n",
        "            elif attn_type == 'cross':\n",
        "                attn_patterns = cache[f'blocks.{layer}.cross_attn.hook_pattern']\n",
        "            else:\n",
        "                raise ValueError(\"attn_type must be 'self' or 'cross'\")\n",
        "\n",
        "            attn = attn_patterns[0, head].cpu().numpy()\n",
        "            im = axes[i, j].imshow(attn, cmap='viridis', aspect='auto')\n",
        "            axes[i, j].set_title(f'Layer {layer}, Head {head}')\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "_ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(x)),\"\")\n",
        "_ = visualize_attention(model, x, c, layer_range=range(4), head_range=range(4), attn_type='self')"
      ],
      "metadata": {
        "id": "QAuXU8fafyiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head Detector"
      ],
      "metadata": {
        "id": "CdW6bQ2LE3DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HeadName = Literal[\"stroke_transition_head\", \"character_continuation_head\"]\n",
        "HEAD_NAMES = cast(List[HeadName], get_args(HeadName))\n",
        "ErrorMeasure = Literal[\"abs\", \"mul\"]\n",
        "\n",
        "LayerHeadTuple = Tuple[int, int]\n",
        "LayerToHead = Dict[int, List[int]]\n",
        "\n",
        "SEQ_LEN_ERR = \"The sequence must be non-empty and must fit within the model's context window.\"\n",
        "DET_PAT_NOT_SQUARE_ERR = \"The detection pattern must be a lower triangular matrix of shape (sequence_length, sequence_length); sequence_length=%d; got detection pattern of shape %s\"\n",
        "\n",
        "def get_stroke_transition_head_detection_pattern(\n",
        "    tokens: torch.Tensor,  # [batch (1) x pos]\n",
        ") -> torch.Tensor:\n",
        "    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])\n",
        "    detection_pattern[1:, :-1] = torch.eye(tokens.shape[-1] - 1)\n",
        "    return torch.tril(detection_pattern)\n",
        "\n",
        "def get_character_continuation_head_detection_pattern(\n",
        "    tokens: torch.Tensor,  # [batch (1) x pos]\n",
        "    char_boundaries: List[int],  # List of positions where characters end\n",
        ") -> torch.Tensor:\n",
        "    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])\n",
        "    for i in range(len(char_boundaries) - 1):\n",
        "        start, end = char_boundaries[i], char_boundaries[i + 1]\n",
        "        detection_pattern[start:end, start:end] = 1\n",
        "    return torch.tril(detection_pattern)\n",
        "\n",
        "def compute_stroke_similarity_score(\n",
        "    attention_pattern: torch.Tensor,\n",
        "    detection_pattern: torch.Tensor,\n",
        "    *,\n",
        "    exclude_bos: bool,\n",
        "    exclude_current_token: bool,\n",
        "    error_measure: ErrorMeasure,\n",
        ") -> float:\n",
        "    \"\"\"Compute similarity score between attention and detection patterns.\"\"\"\n",
        "    # Ensure both tensors are on the same device\n",
        "    detection_pattern = detection_pattern.to(attention_pattern.device)\n",
        "\n",
        "    abs_diff = (attention_pattern - detection_pattern).abs()\n",
        "\n",
        "    if exclude_bos:\n",
        "        abs_diff[:, 0] = 0\n",
        "    if exclude_current_token:\n",
        "        abs_diff.fill_diagonal_(0)\n",
        "\n",
        "    if error_measure == \"mul\":\n",
        "        score = (attention_pattern * detection_pattern).sum() / attention_pattern.sum()\n",
        "        return score.item()\n",
        "\n",
        "    size = len(abs_diff)\n",
        "    return 1 - round((abs_diff.mean() * size).item(), 3)\n",
        "\n",
        "def detect_head(\n",
        "    model: HookedTransformer,\n",
        "    test_dataset,\n",
        "    index: int,  # Index of the example from the dataset\n",
        "    detection_pattern: Union[torch.Tensor, HeadName],\n",
        "    heads: Optional[Union[List[LayerHeadTuple], LayerToHead]] = None,\n",
        "    cache: Optional[ActivationCache] = None,\n",
        "    *,\n",
        "    exclude_bos: bool = False,\n",
        "    exclude_current_token: bool = False,\n",
        "    error_measure: ErrorMeasure = \"mul\",\n",
        "    char_boundaries: Optional[List[int]] = None,\n",
        ") -> torch.Tensor:\n",
        "    # Load the example from the test dataset\n",
        "    x, c, y = test_dataset[index]\n",
        "    x, c, y = x.unsqueeze(0), c.unsqueeze(0), y.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    tokens = x  # Use tokens (which are encoded strokes)\n",
        "\n",
        "    assert error_measure in get_args(ErrorMeasure), f\"Invalid error_measure={error_measure}; valid values are {get_args(ErrorMeasure)}\"\n",
        "\n",
        "    # Generate the detection pattern if provided as a string\n",
        "    if isinstance(detection_pattern, str):\n",
        "        if detection_pattern == \"stroke_transition_head\":\n",
        "            detection_pattern = get_stroke_transition_head_detection_pattern(tokens)\n",
        "        elif detection_pattern == \"character_continuation_head\" and char_boundaries:\n",
        "            detection_pattern = get_character_continuation_head_detection_pattern(tokens, char_boundaries)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown detection pattern: {detection_pattern}\")\n",
        "        detection_pattern = detection_pattern.to(model.cfg.device)  # Ensure the detection pattern is on the same device\n",
        "\n",
        "    seq_len = tokens.shape[-1]\n",
        "    assert 1 <= seq_len <= model.cfg.n_ctx, SEQ_LEN_ERR\n",
        "    assert is_lower_triangular(detection_pattern) and seq_len == detection_pattern.shape[0], DET_PAT_NOT_SQUARE_ERR % (seq_len, detection_pattern.shape)\n",
        "\n",
        "    if cache is None:\n",
        "        _, cache = model.run_with_cache(x, c, return_type=\"both\")\n",
        "\n",
        "    if heads is None:\n",
        "        layer2heads = {layer_i: list(range(model.cfg.n_heads)) for layer_i in range(model.cfg.n_layers)}\n",
        "    elif isinstance(heads, list):\n",
        "        layer2heads = defaultdict(list)\n",
        "        for layer, head in heads:\n",
        "            layer2heads[layer].append(head)\n",
        "    else:\n",
        "        layer2heads = heads\n",
        "\n",
        "    matches = -torch.ones(model.cfg.n_layers, model.cfg.n_heads, dtype=model.cfg.dtype)\n",
        "\n",
        "    for layer, layer_heads in layer2heads.items():\n",
        "        # Handle dimensions of attention patterns correctly\n",
        "        layer_attention_patterns = cache[\"pattern\", layer, \"attn\"]  # [batch, n_heads, q_pos, k_pos]\n",
        "\n",
        "        if layer_attention_patterns.dim() < 4:  # Sanity check to ensure the right number of dimensions\n",
        "            raise ValueError(f\"Expected attention patterns with 4 dimensions, but got {layer_attention_patterns.dim()}\")\n",
        "\n",
        "        for head in layer_heads:\n",
        "            # Ensure the head index is within bounds\n",
        "            if head >= layer_attention_patterns.shape[1]:\n",
        "                print(f\"Skipping head {head} in layer {layer} (out of bounds)\")\n",
        "                continue\n",
        "\n",
        "            head_attention_pattern = layer_attention_patterns[0, head, :, :]  # Handle batch index correctly\n",
        "\n",
        "            # Compute the similarity score\n",
        "            head_score = compute_stroke_similarity_score(\n",
        "                head_attention_pattern,\n",
        "                detection_pattern=detection_pattern,\n",
        "                exclude_bos=exclude_bos,\n",
        "                exclude_current_token=exclude_current_token,\n",
        "                error_measure=error_measure,\n",
        "            )\n",
        "            matches[layer, head] = head_score\n",
        "    return matches\n",
        "\n",
        "# Example usage\n",
        "# index = 0  # Pick an example from the test dataset\n",
        "for index in range(100):\n",
        "    x, c, y = test_dataset[index]\n",
        "    x, c, y = x.unsqueeze(0), c.unsqueeze(0), y.unsqueeze(0)  # Add batch dimension\n",
        "    detection_pattern = \"stroke_transition_head\"\n",
        "    matches = detect_head(model, test_dataset, index, detection_pattern)\n",
        "    # print(matches)\n",
        "\n",
        "    # Set a threshold for significance\n",
        "    threshold = 0.005\n",
        "\n",
        "    # Find heads that exceed the threshold\n",
        "    significant_heads = torch.nonzero(matches > threshold)\n",
        "\n",
        "    print(f\"Significant heads (layer, head) with scores above {threshold}:\")\n",
        "    for layer, head in significant_heads:\n",
        "        print(f\"Layer {layer.item()}, Head {head.item()} - Score: {matches[layer, head].item()}\")\n",
        "\n",
        "    # Visualize only the significant heads\n",
        "    def visualize_significant_attention(model, x, c, significant_heads, attn_type='self'):\n",
        "        if len(significant_heads) == 0:\n",
        "            print(\"No significant heads found for the given threshold.\")\n",
        "            return\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, cache = model.run_with_cache(x, c, return_type=\"both\")\n",
        "\n",
        "        fig, axes = plt.subplots(len(significant_heads), 1, figsize=(8, 4 * len(significant_heads)), squeeze=False)\n",
        "\n",
        "        for i, (layer, head) in enumerate(significant_heads):\n",
        "            attn_patterns = cache[f'blocks.{layer.item()}.attn.hook_pattern'] if attn_type == 'self' else cache[f'blocks.{layer.item()}.cross_attn.hook_pattern']\n",
        "            attn = attn_patterns[0, head.item()].cpu().numpy()\n",
        "\n",
        "            im = axes[i, 0].imshow(attn, cmap='viridis', aspect='auto')\n",
        "            axes[i, 0].set_title(f'Layer {layer.item()}, Head {head.item()}')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Call the function with the detected significant heads\n",
        "    _ = plot_strokes(offsets_to_strokes(test_dataset.decode_stroke(x)),\"\")\n",
        "    visualize_significant_attention(model, x, c, significant_heads)"
      ],
      "metadata": {
        "id": "rd8JmmJ3CTQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9U2xPuJCK9wN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}