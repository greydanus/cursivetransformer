{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN0lJSIY5QrFNMioHTABn2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HookedCursiveTransformer"
      ],
      "metadata": {
        "id": "9N5SAaDHtG0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hl6g0b0fz2l",
        "outputId": "1e1a1b82-889f-496c-a86f-ab4e67b57cd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.34.2)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (3.0.1)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.34)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.44.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.18.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2480, done.\u001b[K\n",
            "remote: Counting objects: 100% (621/621), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 2480 (delta 504), reused 453 (delta 390), pack-reused 1859 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2480/2480), 32.38 MiB | 9.25 MiB/s, done.\n",
            "Resolving deltas: 100% (1410/1410), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 1)) (2.4.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r cursivetransformer/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r cursivetransformer/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzwimpee\u001b[0m (\u001b[33mllm-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "\n",
        "# Import TransformerLens modules\n",
        "\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Union\n",
        "\n",
        "from transformer_lens import HookedTransformer, ActivationCache\n",
        "from transformer_lens.components import AbstractAttention, Attention, LayerNorm, Embed, PosEmbed\n",
        "from transformer_lens.components.mlps.mlp import MLP\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import einops\n",
        "from einops import rearrange, einsum"
      ],
      "metadata": {
        "id": "fZ-PLgwnfu9d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move input to the correct device\n",
        "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v1MYP9hfuxv",
        "outputId": "7a2861c5-174e-4bb3-cf90-124d13ff00ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n",
            "For a dataset of 95 examples we can generate 3183545 combinations of 4 examples.\n",
            "Generating 3000 4-word examples.\n",
            "Number of examples in the train dataset: 497000\n",
            "Number of examples in the test dataset: 3000\n",
            "Max token sequence length: 1000\n",
            "Number of unique characters in the ascii vocabulary: 71\n",
            "Ascii vocabulary:\n",
            "\t\" enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"'(BCQLMWYU,ZF!DXV?KPGJ\"\n",
            "Split up the dataset into 497000 training examples and 3000 test examples\n",
            "Number of Transformer parameters: 368064\n",
            "Model #params: 397184\n",
            "Loaded model from local path: best_checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5jOa1OkwOABq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZncqIrlKejHj",
        "outputId": "40193732-45cf-4d2b-99c5-26788605a30e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(455, 64)\n",
              "    (wpe): Embedding(1000, 64)\n",
              "    (wce): Embedding(72, 64)\n",
              "    (wcpe): Embedding(50, 64)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=64, out_features=192, bias=True)\n",
              "          (c_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (cross_attn): CrossAttention(\n",
              "          (c_attn_q): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (c_attn_kv): Linear(in_features=64, out_features=128, bias=True)\n",
              "          (c_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (ln_3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModuleDict(\n",
              "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (act): NewGELU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=64, out_features=455, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "g9Mm8fSiwc-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HookedCursiveTransformerConfig(HookedTransformerConfig):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Extract custom arguments\n",
        "        d_model_c = kwargs.pop('d_model_c', None)\n",
        "        n_ctx_c = kwargs.pop('n_ctx_c', None)\n",
        "        d_vocab_c = kwargs.pop('d_vocab_c', None)\n",
        "        use_cross_attention = kwargs.pop('use_cross_attention', True)\n",
        "\n",
        "        # Now, call the superclass constructor with the remaining kwargs\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Set custom attributes after superclass initialization\n",
        "        self.d_model_c = d_model_c if d_model_c is not None else self.d_model\n",
        "        self.n_ctx_c = n_ctx_c if n_ctx_c is not None else self.n_ctx\n",
        "        self.d_vocab_c = d_vocab_c if d_vocab_c is not None else self.d_vocab\n",
        "        self.use_cross_attention = use_cross_attention\n",
        "\n",
        "\n",
        "class CrossAttention(AbstractAttention):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "\n",
        "        self.W_Q = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
        "        self.W_K = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model_c, cfg.d_head))\n",
        "        self.W_V = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model_c, cfg.d_head))\n",
        "        self.W_O = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
        "\n",
        "        self.b_Q = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_K = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_V = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
        "        self.b_O = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "\n",
        "        # Buffers (not parameters)\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5))\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones((cfg.n_ctx, cfg.n_ctx_c), dtype=torch.bool)).view(1, 1, cfg.n_ctx, cfg.n_ctx_c))\n",
        "\n",
        "    def forward(self, query_input, key_input, value_input, **kwargs):\n",
        "        # Compute queries\n",
        "        q = einsum('batch pos d_model, n_heads d_model d_head -> batch n_heads pos d_head', query_input, self.W_Q) + self.b_Q\n",
        "\n",
        "        # Compute keys and values from context\n",
        "        k = einsum('batch pos d_model_c, n_heads d_model_c d_head -> batch n_heads pos d_head', key_input, self.W_K) + self.b_K\n",
        "        v = einsum('batch pos d_model_c, n_heads d_model_c d_head -> batch n_heads pos d_head', value_input, self.W_V) + self.b_V\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = einsum('batch n_heads pos_q d_head, batch n_heads pos_k d_head -> batch n_heads pos_q pos_k', q, k)\n",
        "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
        "\n",
        "        # Apply softmax\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Compute output\n",
        "        z = einsum('batch n_heads pos_q pos_k, batch n_heads pos_k d_head -> batch n_heads pos_q d_head', attn_probs, v)\n",
        "        z = einsum('batch n_heads pos_q d_head, n_heads d_head d_model -> batch pos_q d_model', z, self.W_O) + self.b_O\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "class CursiveTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg, layer_idx):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        # Layer norms\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.ln3 = LayerNorm(cfg)\n",
        "\n",
        "        # Self-attention using Attention\n",
        "        self.attn = Attention(cfg)\n",
        "\n",
        "        # Cross-attention using the implemented CrossAttention\n",
        "        self.cross_attn = CrossAttention(cfg)\n",
        "\n",
        "        # MLP\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, x, c, **kwargs):\n",
        "        # Self-attention\n",
        "        x_ln = self.ln1(x)\n",
        "        x = x + self.attn(x_ln, x_ln, x_ln, **kwargs)\n",
        "\n",
        "        # Cross-attention\n",
        "        x_ln = self.ln2(x)\n",
        "        x = x + self.cross_attn(x_ln, c, c, **kwargs)\n",
        "\n",
        "        # MLP\n",
        "        x = x + self.mlp(self.ln3(x), **kwargs)\n",
        "\n",
        "        return x\n",
        "\n",
        "class HookedCursiveTransformer(HookedTransformer):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # Create a modified config for context embeddings\n",
        "        import copy\n",
        "        cfg_c = copy.deepcopy(cfg)\n",
        "        cfg_c.d_vocab = cfg.d_vocab_c\n",
        "        cfg_c.d_model = cfg.d_model_c\n",
        "        cfg_c.n_ctx = cfg.n_ctx_c\n",
        "\n",
        "        # Use transformer_lens components with the modified config\n",
        "        self.embed_c = Embed(cfg_c)\n",
        "        self.pos_embed_c = PosEmbed(cfg_c)\n",
        "\n",
        "        # Override the transformer blocks with custom blocks that include cross-attention\n",
        "        self.blocks = nn.ModuleList([\n",
        "            CursiveTransformerBlock(self.cfg, layer_idx)\n",
        "            for layer_idx in range(self.cfg.n_layers)\n",
        "        ])\n",
        "\n",
        "        # Use transformer_lens's LayerNorm\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "\n",
        "    def forward(self, x, c, return_type=\"logits\", **kwargs):\n",
        "        # Embedding and positional encoding for x (main sequence)\n",
        "        x_tokens = self.embed(x)  # [batch, seq_len, d_model]\n",
        "        x_positions = self.pos_embed(torch.arange(x_tokens.size(1), device=x_tokens.device))  # [seq_len, d_model]\n",
        "        x = x_tokens + x_positions\n",
        "\n",
        "        # Embedding and positional encoding for c (context)\n",
        "        c_tokens = self.embed_c(c)  # [batch, context_len, d_model_c]\n",
        "        c_positions = self.pos_embed_c(torch.arange(c_tokens.size(1), device=c_tokens.device))  # [context_len, d_model_c]\n",
        "        c = c_tokens + c_positions\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c, **kwargs)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.unembed(x)\n",
        "\n",
        "        # Handle return types as before\n",
        "        if return_type == \"logits\":\n",
        "            return logits\n",
        "        elif return_type == \"loss\":\n",
        "            targets = kwargs.get('targets')\n",
        "            if targets is None:\n",
        "                raise ValueError(\"Targets must be provided when return_type is 'loss'\")\n",
        "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return loss\n",
        "        elif return_type == \"both\":\n",
        "            return logits, kwargs.get('cache', None)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid return_type: {return_type}\")\n"
      ],
      "metadata": {
        "id": "wgEl2dfWwcpl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "NrzTSEMksyTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_cursivetransformer_model_config(args):\n",
        "    cfg_dict = {\n",
        "        # Standard parameters\n",
        "        \"d_model\": args.n_embd,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embd * 4,\n",
        "        \"d_head\": args.n_embd // args.n_ctx_head,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": args.vocab_size,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": \"gelu_new\",\n",
        "        \"attn_only\": False,\n",
        "        \"final_rms\": False,\n",
        "        \"original_architecture\": \"cursivetransformer\",\n",
        "        \"normalization_type\": \"LN\",\n",
        "        \"init_weights\": False,\n",
        "        \"device\": args.device,\n",
        "        # Additional parameters for cross-attention\n",
        "        \"d_model_c\": args.n_embd2,\n",
        "        \"n_ctx_c\": args.context_block_size,\n",
        "        \"d_vocab_c\": args.context_vocab_size,\n",
        "        \"use_cross_attention\": True,\n",
        "    }\n",
        "    cfg = HookedCursiveTransformerConfig.from_dict(cfg_dict)\n",
        "    return cfg\n",
        "\n",
        "# - [ ] TODO: Determine if this is actually needed for us, I suspect it might be vestigial\n",
        "def fill_missing_keys(model, state_dict):\n",
        "    \"\"\"Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.\n",
        "\n",
        "    This function is assumed to be run before weights are initialized.\n",
        "\n",
        "    Args:\n",
        "        state_dict (dict): State dict from a pretrained model\n",
        "\n",
        "    Returns:\n",
        "        dict: State dict with missing keys filled in\n",
        "    \"\"\"\n",
        "    # Get the default state dict\n",
        "    default_state_dict = model.state_dict()\n",
        "    # Get the keys that are missing from the pretrained model\n",
        "    missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())\n",
        "    # Fill in the missing keys with the default initialization\n",
        "    for key in missing_keys:\n",
        "        if \"hf_model\" in key:\n",
        "            # Skip keys that are from the HuggingFace model, if loading from HF.\n",
        "            continue\n",
        "        if \"W_\" in key:\n",
        "            logging.warning(\n",
        "                \"Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}\".format(\n",
        "                    key\n",
        "                )\n",
        "            )\n",
        "        state_dict[key] = default_state_dict[key]\n",
        "    return state_dict\n",
        "\n",
        "def convert_cursivetransformer_weights(cursivetransformer, cfg):\n",
        "    state_dict = {}\n",
        "    # Embeddings\n",
        "    state_dict[\"embed.W_E\"] = cursivetransformer.transformer.wte.weight\n",
        "    state_dict[\"pos_embed.W_pos\"] = cursivetransformer.transformer.wpe.weight\n",
        "    state_dict[\"embed_c.W_E\"] = cursivetransformer.transformer.wce.weight\n",
        "    state_dict[\"pos_embed_c.W_pos\"] = cursivetransformer.transformer.wcpe.weight\n",
        "\n",
        "    for l in range(cfg.n_layers):\n",
        "        block = cursivetransformer.transformer.h[l]\n",
        "\n",
        "        # Layer Norms (adjusted parameter names)\n",
        "        state_dict[f'blocks.{l}.ln1.w'] = block.ln_1.weight\n",
        "        state_dict[f'blocks.{l}.ln1.b'] = block.ln_1.bias\n",
        "        state_dict[f'blocks.{l}.ln2.w'] = block.ln_2.weight\n",
        "        state_dict[f'blocks.{l}.ln2.b'] = block.ln_2.bias\n",
        "        state_dict[f'blocks.{l}.ln3.w'] = block.ln_3.weight\n",
        "        state_dict[f'blocks.{l}.ln3.b'] = block.ln_3.bias\n",
        "\n",
        "        # Self-Attention Weights\n",
        "        self_attn = block.attn\n",
        "        W_self = self_attn.c_attn.weight  # Shape: [3 * d_model, d_model]\n",
        "        b_self = self_attn.c_attn.bias    # Shape: [3 * d_model]\n",
        "\n",
        "        # Split weights and biases\n",
        "        W_Q_self, W_K_self, W_V_self = torch.chunk(W_self, 3, dim=0)\n",
        "        b_Q_self, b_K_self, b_V_self = torch.chunk(b_self, 3, dim=0)\n",
        "\n",
        "        # Reshape and assign\n",
        "        W_Q_self = W_Q_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_K_self = W_K_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_V_self = W_V_self.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.W_Q'] = W_Q_self\n",
        "        state_dict[f'blocks.{l}.attn.W_K'] = W_K_self\n",
        "        state_dict[f'blocks.{l}.attn.W_V'] = W_V_self\n",
        "        state_dict[f'blocks.{l}.attn.b_Q'] = b_Q_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.b_K'] = b_K_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.attn.b_V'] = b_V_self.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "        # Self-Attention Output Projection\n",
        "        W_O_self = self_attn.c_proj.weight  # Shape: [d_model, d_model]\n",
        "        b_O_self = self_attn.c_proj.bias    # Shape: [d_model]\n",
        "        W_O_self = W_O_self.T.reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "        state_dict[f'blocks.{l}.attn.W_O'] = W_O_self\n",
        "        state_dict[f'blocks.{l}.attn.b_O'] = b_O_self\n",
        "\n",
        "        # Cross-Attention Weights\n",
        "        cross_attn = block.cross_attn\n",
        "        W_Q_cross = cross_attn.c_attn_q.weight  # Shape: [d_model, d_model]\n",
        "        b_Q_cross = cross_attn.c_attn_q.bias    # Shape: [d_model]\n",
        "        W_KV_cross = cross_attn.c_attn_kv.weight  # Shape: [2 * d_model_c, d_model_c]\n",
        "        b_KV_cross = cross_attn.c_attn_kv.bias    # Shape: [2 * d_model_c]\n",
        "\n",
        "        # Split KV weights and biases\n",
        "        W_K_cross, W_V_cross = torch.chunk(W_KV_cross, 2, dim=0)\n",
        "        b_K_cross, b_V_cross = torch.chunk(b_KV_cross, 2, dim=0)\n",
        "\n",
        "        # Reshape and assign\n",
        "        W_Q_cross = W_Q_cross.T.reshape(cfg.n_heads, cfg.d_model, cfg.d_head)\n",
        "        W_K_cross = W_K_cross.T.reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "        W_V_cross = W_V_cross.T.reshape(cfg.n_heads, cfg.d_model_c, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_Q'] = W_Q_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_K'] = W_K_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_V'] = W_V_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_Q'] = b_Q_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_K'] = b_K_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_V'] = b_V_cross.reshape(cfg.n_heads, cfg.d_head)\n",
        "\n",
        "        # Cross-Attention Output Projection\n",
        "        W_O_cross = cross_attn.c_proj.weight  # Shape: [d_model, d_model]\n",
        "        b_O_cross = cross_attn.c_proj.bias    # Shape: [d_model]\n",
        "        W_O_cross = W_O_cross.T.reshape(cfg.n_heads, cfg.d_head, cfg.d_model)\n",
        "        state_dict[f'blocks.{l}.cross_attn.W_O'] = W_O_cross\n",
        "        state_dict[f'blocks.{l}.cross_attn.b_O'] = b_O_cross\n",
        "\n",
        "        # MLP Weights\n",
        "        mlp = block.mlp\n",
        "        W_in = mlp.c_fc.weight  # Shape: [4 * d_model, d_model]\n",
        "        b_in = mlp.c_fc.bias    # Shape: [4 * d_model]\n",
        "        W_out = mlp.c_proj.weight  # Shape: [d_model, 4 * d_model]\n",
        "        b_out = mlp.c_proj.bias    # Shape: [d_model]\n",
        "\n",
        "        # Transpose and assign\n",
        "        state_dict[f'blocks.{l}.mlp.W_in'] = W_in.T\n",
        "        state_dict[f'blocks.{l}.mlp.b_in'] = b_in\n",
        "        state_dict[f'blocks.{l}.mlp.W_out'] = W_out.T\n",
        "        state_dict[f'blocks.{l}.mlp.b_out'] = b_out\n",
        "\n",
        "    # Unembedding Weights\n",
        "    state_dict[\"unembed.W_U\"] = cursivetransformer.lm_head.weight.T\n",
        "    if cursivetransformer.lm_head.bias is not None:\n",
        "        state_dict[\"unembed.b_U\"] = cursivetransformer.lm_head.bias\n",
        "    else:\n",
        "        state_dict[\"unembed.b_U\"] = torch.zeros(cfg.d_vocab_out)\n",
        "\n",
        "    # Final Layer Norm (adjusted parameter names)\n",
        "    state_dict[\"ln_final.w\"] = cursivetransformer.transformer.ln_f.weight\n",
        "    state_dict[\"ln_final.b\"] = cursivetransformer.transformer.ln_f.bias\n",
        "\n",
        "    return state_dict"
      ],
      "metadata": {
        "id": "qUx3I0jenYhU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sandbox"
      ],
      "metadata": {
        "id": "7iB8ma0Fs0pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the configuration has the correct maximum sequence lengths\n",
        "cfg = convert_cursivetransformer_model_config(args)\n",
        "state_dict = convert_cursivetransformer_weights(model, cfg)\n",
        "hooked_model = HookedCursiveTransformer(cfg)\n",
        "hooked_model.load_state_dict(state_dict, strict=False)\n",
        "hooked_model.to(device)\n",
        "\n",
        "# Add batch dimension to your inputs\n",
        "x, c, y = test_dataset[0]\n",
        "x = x.unsqueeze(0).to(device)  # Shape: [1, 1000]\n",
        "c = c.unsqueeze(0).to(device)  # Shape: [1, 50]\n",
        "y = y.unsqueeze(0).to(device) # Shape: [1, 1000]\n",
        "\n",
        "print(x.shape)  # torch.Size([1, 1000])\n",
        "print(c.shape)  # torch.Size([1, 50])\n",
        "print(y.shape)  # torch.Size([1, 1000])\n",
        "\n",
        "# Run the model with cache\n",
        "hooked_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, cache = hooked_model(x, c, return_type=\"both\")\n",
        "\n",
        "# Choose the layer and head to visualize\n",
        "layer = 0  # Change to the desired layer index\n",
        "head = 0   # Change to the desired head index\n",
        "\n",
        "# Access attention patterns for the specified layer\n",
        "attn_patterns = cache[f'blocks.{layer}.attn.hook_pattern']  # Shape: [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "# Extract the attention pattern for the specified head and sample\n",
        "attn = attn_patterns[0, head].cpu().numpy()  # Shape: [seq_len, seq_len]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn, cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title(f'Self-Attention Pattern for Layer {layer}, Head {head}')\n",
        "plt.xlabel('Key Positions')\n",
        "plt.ylabel('Query Positions')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftYaXgofqArq",
        "outputId": "b89f360a-bf41-4b87-f264-63e373fbd157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving model to device:  cpu\n",
            "torch.Size([1, 1000])\n",
            "torch.Size([1, 50])\n",
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit cache to specific layer and head\n",
        "layer = 0  # Layer you're interested in\n",
        "head = 0   # Head you're interested in\n",
        "\n",
        "def cache_filter(name):\n",
        "    return name == f'blocks.{layer}.attn.hook_pattern'\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    logits, cache = hooked_model.run_with_cache(\n",
        "        x, c,\n",
        "        return_type=\"logits\",\n",
        "        names_filter=cache_filter\n",
        "    )\n",
        "\n",
        "# Proceed to extract and visualize the attention patterns\n",
        "attn_patterns = cache[f'blocks.{layer}.attn.hook_pattern']  # [batch_size, n_heads, seq_len_q, seq_len_k]\n",
        "attn = attn_patterns[0, head].cpu().numpy()  # [seq_len_q, seq_len_k]\n",
        "\n",
        "# Visualization code remains the same\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn, cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title(f'Self-Attention Pattern for Layer {layer}, Head {head}')\n",
        "plt.xlabel('Key Positions')\n",
        "plt.ylabel('Query Positions')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9FDMKiN15SIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1uO8Va6CtE0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}