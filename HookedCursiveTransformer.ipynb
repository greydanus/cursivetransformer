{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMtvqsp9CL+E+HuS7shEBMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/HookedCursiveTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/zwimpee/cursivetransformer/issues/26"
      ],
      "metadata": {
        "id": "F1SLPMG7UJcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will find this post particularly useful: https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch\n",
        "As well as this notebook: https://colab.research.google.com/drive/1bZkkJd8pAVnSN23svyszZ3f4WrnYKN_3?usp=sharing"
      ],
      "metadata": {
        "id": "mrGWMzmwQtoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WPOVT1xvQBOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5jOa1OkwOABq",
        "outputId": "a5466e39-7302-496e-be00-adc321538bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.34.2)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.44.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.5)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading transformer_lens-2.7.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, pyarrow, fancy-einsum, docker-pycreds, dill, beartype, multiprocess, jaxtyping, gitdb, gitpython, wandb, datasets, transformer_lens\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.14.1 better-abc-0.0.3 datasets-3.0.1 dill-0.3.8 docker-pycreds-0.4.0 fancy-einsum-0.0.3 gitdb-4.0.11 gitpython-3.1.43 jaxtyping-0.2.34 multiprocess-0.70.16 pyarrow-17.0.0 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 transformer_lens-2.7.0 typeguard-2.13.3 wandb-0.18.2 xxhash-3.5.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m129.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.11-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.11-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.11 ruff-0.6.8 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2424, done.\u001b[K\n",
            "remote: Counting objects: 100% (559/559), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 2424 (delta 460), reused 448 (delta 385), pack-reused 1865 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2424/2424), 32.19 MiB | 20.64 MiB/s, done.\n",
            "Resolving deltas: 100% (1374/1374), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 1)) (2.4.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 2)) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r cursivetransformer/requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r cursivetransformer/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r cursivetransformer/requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r cursivetransformer/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r cursivetransformer/requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r cursivetransformer/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r cursivetransformer/requirements.txt (line 2)) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load dataset file from /content/cursivetransformer/data/bigbank.json.zip\n",
            "Succeeded in loading the bigbank dataset; contains 1900 items.\n",
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n",
            "For a dataset of 95 examples we can generate 3183545 combinations of 4 examples.\n",
            "Generating 3000 4-word examples.\n",
            "Number of examples in the train dataset: 497000\n",
            "Number of examples in the test dataset: 3000\n",
            "Max token sequence length: 1000\n",
            "Number of unique characters in the ascii vocabulary: 71\n",
            "Ascii vocabulary:\n",
            "\t\" enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"'(BCQLMWYU,ZF!DXV?KPGJ\"\n",
            "Split up the dataset into 497000 training examples and 3000 test examples\n",
            "Number of Transformer parameters: 368064\n",
            "Model #params: 397184\n",
            "Finding latest checkpoint for W&B run id 6le6tujz\n",
            "  model:best_checkpoint:v70\n",
            "  model:best_checkpoint:v71\n",
            "  model:best_checkpoint:v72\n",
            "  model:best_checkpoint:v73\n",
            "  model:best_checkpoint:v74\n",
            "  model:best_checkpoint:v75\n",
            "  model:best_checkpoint:v76\n",
            "  model:best_checkpoint:v77\n",
            "  model:best_checkpoint:v78\n",
            "  model:best_checkpoint:v79\n",
            "  model:best_checkpoint:v80\n",
            "  model:best_checkpoint:v81\n",
            "  model:best_checkpoint:v82\n",
            "  model:best_checkpoint:v83\n",
            "  model:best_checkpoint:v84\n",
            "  model:best_checkpoint:v85\n",
            "  model:best_checkpoint:v86\n",
            "  model:best_checkpoint:v87\n",
            "  model:best_checkpoint:v88\n",
            "  model:best_checkpoint:v89\n",
            "  wandb-history:run-6le6tujz-history:v4\n",
            "Selected:  model:best_checkpoint:v89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "namespace(max_steps=110000, print_every=100, log_every=2500, lr_decay=0.333, step_lr_every=33000, device='cuda', seed=42, n_layer=4, n_embd=64, n_embd2=64, n_ctx_head=4, learning_rate=0.01, weight_decay=0.0001, batch_size=32, train_size=497000, test_size=3000, num_words=4, max_seq_length=1000, augment=True, ablate_cross_attention=False, downsample_mean=0.65, downsample_width=0.1, add_digits=True, alphabet=' enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"\\'(BCQLMWYU,ZF!DXV?KPGJ', dataset_name='bigbank', wandb_project='bigbank_experiments', wandb_entity='sam-greydanus', wandb_run_name='cursivetransformer_dictionary_learning', wandb_api_key=None, load_from_run_id='6le6tujz', sample_only=True, local_checkpoint_path='best_checkpoint.pt', block_size=1000, context_block_size=50, vocab_size=455, context_vocab_size=72)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformer_lens\n",
        "!pip install gradio\n",
        "!pip install wandb\n",
        "!pip install einops\n",
        "!pip install matplotlib\n",
        "!pip install datasets\n",
        "\n",
        "# Clone the cursivetransformer repository and install its requirements\n",
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git\n",
        "!pip install -r cursivetransformer/requirements.txt\n",
        "\n",
        "# Login to Weights & Biases (replace 'your_api_key' with your actual API key)\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')  # Adjust the path if necessary\n",
        "\n",
        "# Import cursivetransformer modules\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, offsets_to_strokes\n",
        "from cursivetransformer.sample import generate, generate_n_words, plot_strokes\n",
        "\n",
        "# Import TransformerLens modules\n",
        "# from transformer_lens import HookedTransformer\n",
        "\n",
        "# Import other necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'  # Replace with your actual run ID\n",
        "args.wandb_entity = 'sam-greydanus'\n",
        "args.dataset_name = 'bigbank'  # Replace with your dataset name\n",
        "args.wandb_run_name = 'cursivetransformer_dictionary_learning'\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "train_dataset, test_dataset = create_datasets(args)\n",
        "\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "\n",
        "model, optimizer, scheduler, step, best_loss = get_checkpoint(args)\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerLens/transformer_lens/loading_from_pretrained.py\n",
        "- https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/loading_from_pretrained.py"
      ],
      "metadata": {
        "id": "qKwM9sCrUeLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "9712nvgrak9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Loading Pretrained Models Utilities.\n",
        "\n",
        "This module contains functions for loading pretrained models from the Hugging Face Hub.\n",
        "\"\"\"\n",
        "\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Union\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import HfApi\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    BertForPreTraining,\n",
        "    T5ForConditionalGeneration,\n",
        ")\n",
        "\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "from transformer_lens.pretrained.weight_conversions import (\n",
        "    convert_bert_weights,\n",
        "    convert_bloom_weights,\n",
        "    convert_coder_weights,\n",
        "    convert_gemma_weights,\n",
        "    convert_gpt2_weights,\n",
        "    convert_gptj_weights,\n",
        "    convert_llama_weights,\n",
        "    convert_mingpt_weights,\n",
        "    convert_mistral_weights,\n",
        "    convert_mixtral_weights,\n",
        "    convert_neel_solu_old_weights,\n",
        "    convert_neo_weights,\n",
        "    convert_neox_weights,\n",
        "    convert_opt_weights,\n",
        "    convert_phi3_weights,\n",
        "    convert_phi_weights,\n",
        "    convert_qwen2_weights,\n",
        "    convert_qwen_weights,\n",
        "    convert_t5_weights,\n",
        ")\n",
        "\n",
        "OFFICIAL_MODEL_NAMES = [\n",
        "    \"gpt2\",\n",
        "    \"gpt2-medium\",\n",
        "    \"gpt2-large\",\n",
        "    \"gpt2-xl\",\n",
        "    \"distilgpt2\",\n",
        "    \"facebook/opt-125m\",\n",
        "    \"facebook/opt-1.3b\",\n",
        "    \"facebook/opt-2.7b\",\n",
        "    \"facebook/opt-6.7b\",\n",
        "    \"facebook/opt-13b\",\n",
        "    \"facebook/opt-30b\",\n",
        "    \"facebook/opt-66b\",\n",
        "    \"EleutherAI/gpt-neo-125M\",\n",
        "    \"EleutherAI/gpt-neo-1.3B\",\n",
        "    \"EleutherAI/gpt-neo-2.7B\",\n",
        "    \"EleutherAI/gpt-j-6B\",\n",
        "    \"EleutherAI/gpt-neox-20b\",\n",
        "    \"stanford-crfm/alias-gpt2-small-x21\",\n",
        "    \"stanford-crfm/battlestar-gpt2-small-x49\",\n",
        "    \"stanford-crfm/caprica-gpt2-small-x81\",\n",
        "    \"stanford-crfm/darkmatter-gpt2-small-x343\",\n",
        "    \"stanford-crfm/expanse-gpt2-small-x777\",\n",
        "    \"stanford-crfm/arwen-gpt2-medium-x21\",\n",
        "    \"stanford-crfm/beren-gpt2-medium-x49\",\n",
        "    \"stanford-crfm/celebrimbor-gpt2-medium-x81\",\n",
        "    \"stanford-crfm/durin-gpt2-medium-x343\",\n",
        "    \"stanford-crfm/eowyn-gpt2-medium-x777\",\n",
        "    \"EleutherAI/pythia-14m\",\n",
        "    \"EleutherAI/pythia-31m\",\n",
        "    \"EleutherAI/pythia-70m\",\n",
        "    \"EleutherAI/pythia-160m\",\n",
        "    \"EleutherAI/pythia-410m\",\n",
        "    \"EleutherAI/pythia-1b\",\n",
        "    \"EleutherAI/pythia-1.4b\",\n",
        "    \"EleutherAI/pythia-2.8b\",\n",
        "    \"EleutherAI/pythia-6.9b\",\n",
        "    \"EleutherAI/pythia-12b\",\n",
        "    \"EleutherAI/pythia-70m-deduped\",\n",
        "    \"EleutherAI/pythia-160m-deduped\",\n",
        "    \"EleutherAI/pythia-410m-deduped\",\n",
        "    \"EleutherAI/pythia-1b-deduped\",\n",
        "    \"EleutherAI/pythia-1.4b-deduped\",\n",
        "    \"EleutherAI/pythia-2.8b-deduped\",\n",
        "    \"EleutherAI/pythia-6.9b-deduped\",\n",
        "    \"EleutherAI/pythia-12b-deduped\",\n",
        "    \"EleutherAI/pythia-70m-v0\",\n",
        "    \"EleutherAI/pythia-160m-v0\",\n",
        "    \"EleutherAI/pythia-410m-v0\",\n",
        "    \"EleutherAI/pythia-1b-v0\",\n",
        "    \"EleutherAI/pythia-1.4b-v0\",\n",
        "    \"EleutherAI/pythia-2.8b-v0\",\n",
        "    \"EleutherAI/pythia-6.9b-v0\",\n",
        "    \"EleutherAI/pythia-12b-v0\",\n",
        "    \"EleutherAI/pythia-70m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-160m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-410m-deduped-v0\",\n",
        "    \"EleutherAI/pythia-1b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-1.4b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-2.8b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-6.9b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-12b-deduped-v0\",\n",
        "    \"EleutherAI/pythia-160m-seed1\",\n",
        "    \"EleutherAI/pythia-160m-seed2\",\n",
        "    \"EleutherAI/pythia-160m-seed3\",\n",
        "    \"NeelNanda/SoLU_1L_v9_old\",\n",
        "    \"NeelNanda/SoLU_2L_v10_old\",\n",
        "    \"NeelNanda/SoLU_4L_v11_old\",\n",
        "    \"NeelNanda/SoLU_6L_v13_old\",\n",
        "    \"NeelNanda/SoLU_8L_v21_old\",\n",
        "    \"NeelNanda/SoLU_10L_v22_old\",\n",
        "    \"NeelNanda/SoLU_12L_v23_old\",\n",
        "    \"NeelNanda/SoLU_1L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_2L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_3L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_4L512W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_6L768W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_8L1024W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_10L1280W_C4_Code\",\n",
        "    \"NeelNanda/SoLU_12L1536W_C4_Code\",\n",
        "    \"NeelNanda/GELU_1L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_2L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_3L512W_C4_Code\",\n",
        "    \"NeelNanda/GELU_4L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_1L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_2L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_3L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn_Only_4L512W_C4_Code\",\n",
        "    \"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\",\n",
        "    \"NeelNanda/SoLU_1L512W_Wiki_Finetune\",\n",
        "    \"NeelNanda/SoLU_4L512W_Wiki_Finetune\",\n",
        "    \"ArthurConmy/redwood_attn_2l\",\n",
        "    \"llama-7b-hf\",\n",
        "    \"llama-13b-hf\",\n",
        "    \"llama-30b-hf\",\n",
        "    \"llama-65b-hf\",\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    \"meta-llama/Llama-2-13b-hf\",\n",
        "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
        "    \"CodeLlama-7b-hf\",\n",
        "    \"CodeLlama-7b-Python-hf\",\n",
        "    \"CodeLlama-7b-Instruct-hf\",\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3-70B\",\n",
        "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"Baidicoot/Othello-GPT-Transformer-Lens\",\n",
        "    \"bert-base-cased\",\n",
        "    \"roneneldan/TinyStories-1M\",\n",
        "    \"roneneldan/TinyStories-3M\",\n",
        "    \"roneneldan/TinyStories-8M\",\n",
        "    \"roneneldan/TinyStories-28M\",\n",
        "    \"roneneldan/TinyStories-33M\",\n",
        "    \"roneneldan/TinyStories-Instruct-1M\",\n",
        "    \"roneneldan/TinyStories-Instruct-3M\",\n",
        "    \"roneneldan/TinyStories-Instruct-8M\",\n",
        "    \"roneneldan/TinyStories-Instruct-28M\",\n",
        "    \"roneneldan/TinyStories-Instruct-33M\",\n",
        "    \"roneneldan/TinyStories-1Layer-21M\",\n",
        "    \"roneneldan/TinyStories-2Layers-33M\",\n",
        "    \"roneneldan/TinyStories-Instuct-1Layer-21M\",\n",
        "    \"roneneldan/TinyStories-Instruct-2Layers-33M\",\n",
        "    \"stabilityai/stablelm-base-alpha-3b\",\n",
        "    \"stabilityai/stablelm-base-alpha-7b\",\n",
        "    \"stabilityai/stablelm-tuned-alpha-3b\",\n",
        "    \"stabilityai/stablelm-tuned-alpha-7b\",\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "    \"bigscience/bloom-1b1\",\n",
        "    \"bigscience/bloom-1b7\",\n",
        "    \"bigscience/bloom-3b\",\n",
        "    \"bigscience/bloom-7b1\",\n",
        "    \"bigcode/santacoder\",\n",
        "    \"Qwen/Qwen-1_8B\",\n",
        "    \"Qwen/Qwen-7B\",\n",
        "    \"Qwen/Qwen-14B\",\n",
        "    \"Qwen/Qwen-1_8B-Chat\",\n",
        "    \"Qwen/Qwen-7B-Chat\",\n",
        "    \"Qwen/Qwen-14B-Chat\",\n",
        "    \"Qwen/Qwen1.5-0.5B\",\n",
        "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "    \"Qwen/Qwen1.5-1.8B\",\n",
        "    \"Qwen/Qwen1.5-1.8B-Chat\",\n",
        "    \"Qwen/Qwen1.5-4B\",\n",
        "    \"Qwen/Qwen1.5-4B-Chat\",\n",
        "    \"Qwen/Qwen1.5-7B\",\n",
        "    \"Qwen/Qwen1.5-7B-Chat\",\n",
        "    \"Qwen/Qwen1.5-14B\",\n",
        "    \"Qwen/Qwen1.5-14B-Chat\",\n",
        "    \"Qwen/Qwen2-0.5B\",\n",
        "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    \"Qwen/Qwen2-1.5B\",\n",
        "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"Qwen/Qwen2-7B\",\n",
        "    \"Qwen/Qwen2-7B-Instruct\",\n",
        "    \"microsoft/phi-1\",\n",
        "    \"microsoft/phi-1_5\",\n",
        "    \"microsoft/phi-2\",\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"google/gemma-2b\",\n",
        "    \"google/gemma-7b\",\n",
        "    \"google/gemma-2b-it\",\n",
        "    \"google/gemma-7b-it\",\n",
        "    \"google/gemma-2-2b\",\n",
        "    \"google/gemma-2-2b-it\",\n",
        "    \"google/gemma-2-9b\",\n",
        "    \"google/gemma-2-9b-it\",\n",
        "    \"google/gemma-2-27b\",\n",
        "    \"google/gemma-2-27b-it\",\n",
        "    \"01-ai/Yi-6B\",\n",
        "    \"01-ai/Yi-34B\",\n",
        "    \"01-ai/Yi-6B-Chat\",\n",
        "    \"01-ai/Yi-34B-Chat\",\n",
        "    \"google-t5/t5-small\",\n",
        "    \"google-t5/t5-base\",\n",
        "    \"google-t5/t5-large\",\n",
        "    \"ai-forever/mGPT\",\n",
        "]\n",
        "\"\"\"Official model names for models on HuggingFace.\"\"\"\n",
        "\n",
        "# Model Aliases:\n",
        "MODEL_ALIASES = {\n",
        "    \"NeelNanda/SoLU_1L_v9_old\": [\"solu-1l-pile\", \"solu-1l-old\"],\n",
        "    \"NeelNanda/SoLU_2L_v10_old\": [\"solu-2l-pile\", \"solu-2l-old\"],\n",
        "    \"NeelNanda/SoLU_4L_v11_old\": [\"solu-4l-pile\", \"solu-4l-old\"],\n",
        "    \"NeelNanda/SoLU_6L_v13_old\": [\"solu-6l-pile\", \"solu-6l-old\"],\n",
        "    \"NeelNanda/SoLU_8L_v21_old\": [\"solu-8l-pile\", \"solu-8l-old\"],\n",
        "    \"NeelNanda/SoLU_10L_v22_old\": [\"solu-10l-pile\", \"solu-10l-old\"],\n",
        "    \"NeelNanda/SoLU_12L_v23_old\": [\"solu-12l-pile\", \"solu-12l-old\"],\n",
        "    \"NeelNanda/SoLU_1L512W_C4_Code\": [\"solu-1l\", \"solu-1l-new\", \"solu-1l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_2L512W_C4_Code\": [\"solu-2l\", \"solu-2l-new\", \"solu-2l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_3L512W_C4_Code\": [\"solu-3l\", \"solu-3l-new\", \"solu-3l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_4L512W_C4_Code\": [\"solu-4l\", \"solu-4l-new\", \"solu-4l-c4-code\"],\n",
        "    \"NeelNanda/GELU_1L512W_C4_Code\": [\"gelu-1l\", \"gelu-1l-new\", \"gelu-1l-c4-code\"],\n",
        "    \"NeelNanda/GELU_2L512W_C4_Code\": [\"gelu-2l\", \"gelu-2l-new\", \"gelu-2l-c4-code\"],\n",
        "    \"NeelNanda/GELU_3L512W_C4_Code\": [\"gelu-3l\", \"gelu-3l-new\", \"gelu-3l-c4-code\"],\n",
        "    \"NeelNanda/GELU_4L512W_C4_Code\": [\"gelu-4l\", \"gelu-4l-new\", \"gelu-4l-c4-code\"],\n",
        "    \"NeelNanda/Attn_Only_1L512W_C4_Code\": [\n",
        "        \"attn-only-1l\",\n",
        "        \"attn-only-1l-new\",\n",
        "        \"attn-only-1l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_2L512W_C4_Code\": [\n",
        "        \"attn-only-2l\",\n",
        "        \"attn-only-2l-new\",\n",
        "        \"attn-only-2l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_3L512W_C4_Code\": [\n",
        "        \"attn-only-3l\",\n",
        "        \"attn-only-3l-new\",\n",
        "        \"attn-only-3l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/Attn_Only_4L512W_C4_Code\": [\n",
        "        \"attn-only-4l\",\n",
        "        \"attn-only-4l-new\",\n",
        "        \"attn-only-4l-c4-code\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_6L768W_C4_Code\": [\"solu-6l\", \"solu-6l-new\", \"solu-6l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_8L1024W_C4_Code\": [\"solu-8l\", \"solu-8l-new\", \"solu-8l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_10L1280W_C4_Code\": [\"solu-10l\", \"solu-10l-new\", \"solu-10l-c4-code\"],\n",
        "    \"NeelNanda/SoLU_12L1536W_C4_Code\": [\"solu-12l\", \"solu-12l-new\", \"solu-12l-c4-code\"],\n",
        "    \"NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr\": [\n",
        "        \"attn-only-2l-demo\",\n",
        "        \"attn-only-2l-shortformer-6b-big-lr\",\n",
        "        \"attn-only-2l-induction-demo\",\n",
        "        \"attn-only-demo\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_1L512W_Wiki_Finetune\": [\n",
        "        \"solu-1l-wiki\",\n",
        "        \"solu-1l-wiki-finetune\",\n",
        "        \"solu-1l-finetune\",\n",
        "    ],\n",
        "    \"NeelNanda/SoLU_4L512W_Wiki_Finetune\": [\n",
        "        \"solu-4l-wiki\",\n",
        "        \"solu-4l-wiki-finetune\",\n",
        "        \"solu-4l-finetune\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-14m\": [\n",
        "        \"pythia-14m\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-31m\": [\n",
        "        \"pythia-31m\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m\": [\n",
        "        \"pythia-70m\",\n",
        "        \"pythia\",\n",
        "        \"EleutherAI/pythia-19m\",\n",
        "        \"pythia-19m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m\": [\n",
        "        \"pythia-160m\",\n",
        "        \"EleutherAI/pythia-125m\",\n",
        "        \"pythia-125m\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m\": [\n",
        "        \"pythia-410m\",\n",
        "        \"EleutherAI/pythia-350m\",\n",
        "        \"pythia-350m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b\": [\n",
        "        \"pythia-1b\",\n",
        "        \"EleutherAI/pythia-800m\",\n",
        "        \"pythia-800m\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b\": [\n",
        "        \"pythia-1.4b\",\n",
        "        \"EleutherAI/pythia-1.3b\",\n",
        "        \"pythia-1.3b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b\": [\n",
        "        \"pythia-2.8b\",\n",
        "        \"EleutherAI/pythia-2.7b\",\n",
        "        \"pythia-2.7b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b\": [\n",
        "        \"pythia-6.9b\",\n",
        "        \"EleutherAI/pythia-6.7b\",\n",
        "        \"pythia-6.7b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b\": [\n",
        "        \"pythia-12b\",\n",
        "        \"EleutherAI/pythia-13b\",\n",
        "        \"pythia-13b\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-deduped\": [\n",
        "        \"pythia-70m-deduped\",\n",
        "        \"EleutherAI/pythia-19m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-19m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-deduped\": [\n",
        "        \"pythia-160m-deduped\",\n",
        "        \"EleutherAI/pythia-125m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-125m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-deduped\": [\n",
        "        \"pythia-410m-deduped\",\n",
        "        \"EleutherAI/pythia-350m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-350m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-deduped\": [\n",
        "        \"pythia-1b-deduped\",\n",
        "        \"EleutherAI/pythia-800m-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-800m-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-deduped\": [\n",
        "        \"pythia-1.4b-deduped\",\n",
        "        \"EleutherAI/pythia-1.3b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-1.3b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-deduped\": [\n",
        "        \"pythia-2.8b-deduped\",\n",
        "        \"EleutherAI/pythia-2.7b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-2.7b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-deduped\": [\n",
        "        \"pythia-6.9b-deduped\",\n",
        "        \"EleutherAI/pythia-6.7b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-6.7b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-deduped\": [\n",
        "        \"pythia-12b-deduped\",\n",
        "        \"EleutherAI/pythia-13b-deduped\",  # EleutherAI renamed this model\n",
        "        \"pythia-13b-deduped\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-v0\": [\n",
        "        \"pythia-70m-v0\",\n",
        "        \"pythia-v0\",\n",
        "        \"EleutherAI/pythia-19m-v0\",\n",
        "        \"pythia-19m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-v0\": [\n",
        "        \"pythia-160m-v0\",\n",
        "        \"EleutherAI/pythia-125m-v0\",\n",
        "        \"pythia-125m-v0\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-v0\": [\n",
        "        \"pythia-410m-v0\",\n",
        "        \"EleutherAI/pythia-350m-v0\",\n",
        "        \"pythia-350m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-v0\": [\n",
        "        \"pythia-1b-v0\",\n",
        "        \"EleutherAI/pythia-800m-v0\",\n",
        "        \"pythia-800m-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-v0\": [\n",
        "        \"pythia-1.4b-v0\",\n",
        "        \"EleutherAI/pythia-1.3b-v0\",\n",
        "        \"pythia-1.3b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-v0\": [\n",
        "        \"pythia-2.8b-v0\",\n",
        "        \"EleutherAI/pythia-2.7b-v0\",\n",
        "        \"pythia-2.7b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-v0\": [\n",
        "        \"pythia-6.9b-v0\",\n",
        "        \"EleutherAI/pythia-6.7b-v0\",\n",
        "        \"pythia-6.7b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-v0\": [\n",
        "        \"pythia-12b-v0\",\n",
        "        \"EleutherAI/pythia-13b-v0\",\n",
        "        \"pythia-13b-v0\",  # EleutherAI renamed this model\n",
        "    ],\n",
        "    \"EleutherAI/pythia-70m-deduped-v0\": [\n",
        "        \"pythia-70m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-19m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-19m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-deduped-v0\": [\n",
        "        \"pythia-160m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-125m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-125m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-410m-deduped-v0\": [\n",
        "        \"pythia-410m-deduped-v0\",\n",
        "        \"EleutherAI/pythia-350m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-350m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1b-deduped-v0\": [\n",
        "        \"pythia-1b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-800m-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-800m-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-1.4b-deduped-v0\": [\n",
        "        \"pythia-1.4b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-1.3b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-1.3b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-2.8b-deduped-v0\": [\n",
        "        \"pythia-2.8b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-2.7b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-2.7b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-6.9b-deduped-v0\": [\n",
        "        \"pythia-6.9b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-6.7b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-6.7b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-12b-deduped-v0\": [\n",
        "        \"pythia-12b-deduped-v0\",\n",
        "        \"EleutherAI/pythia-13b-deduped-v0\",  # EleutherAI renamed this model\n",
        "        \"pythia-13b-deduped-v0\",\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed1\": [\n",
        "        \"pythia-160m-seed1\",\n",
        "        \"EleutherAI/pythia-125m-seed1\",\n",
        "        \"pythia-125m-seed1\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed2\": [\n",
        "        \"pythia-160m-seed2\",\n",
        "        \"EleutherAI/pythia-125m-seed2\",\n",
        "        \"pythia-125m-seed2\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"EleutherAI/pythia-160m-seed3\": [\n",
        "        \"pythia-160m-seed3\",\n",
        "        \"EleutherAI/pythia-125m-seed3\",\n",
        "        \"pythia-125m-seed3\",  # EleutherAI renamed this model\"\n",
        "    ],\n",
        "    \"gpt2\": [\"gpt2-small\"],\n",
        "    \"distilgpt2\": [\"distillgpt2\", \"distill-gpt2\", \"distil-gpt2\", \"gpt2-xs\"],\n",
        "    \"facebook/opt-125m\": [\"opt-125m\", \"opt-small\", \"opt\"],\n",
        "    \"facebook/opt-1.3b\": [\"opt-1.3b\", \"opt-medium\"],\n",
        "    \"facebook/opt-2.7b\": [\"opt-2.7b\", \"opt-large\"],\n",
        "    \"facebook/opt-6.7b\": [\"opt-6.7b\", \"opt-xl\"],\n",
        "    \"facebook/opt-13b\": [\"opt-13b\", \"opt-xxl\"],\n",
        "    \"facebook/opt-30b\": [\"opt-30b\", \"opt-xxxl\"],\n",
        "    \"facebook/opt-66b\": [\"opt-66b\", \"opt-xxxxl\"],\n",
        "    \"EleutherAI/gpt-neo-125M\": [\"gpt-neo-125M\", \"gpt-neo-small\", \"neo-small\", \"neo\"],\n",
        "    \"EleutherAI/gpt-neo-1.3B\": [\"gpt-neo-1.3B\", \"gpt-neo-medium\", \"neo-medium\"],\n",
        "    \"EleutherAI/gpt-neo-2.7B\": [\"gpt-neo-2.7B\", \"gpt-neo-large\", \"neo-large\"],\n",
        "    \"EleutherAI/gpt-j-6B\": [\"gpt-j-6B\", \"gpt-j\", \"gptj\"],\n",
        "    \"EleutherAI/gpt-neox-20b\": [\"gpt-neox-20b\", \"gpt-neox\", \"neox\"],\n",
        "    \"stanford-crfm/alias-gpt2-small-x21\": [\n",
        "        \"stanford-gpt2-small-a\",\n",
        "        \"alias-gpt2-small-x21\",\n",
        "        \"gpt2-mistral-small-a\",\n",
        "        \"gpt2-stanford-small-a\",\n",
        "    ],\n",
        "    \"stanford-crfm/battlestar-gpt2-small-x49\": [\n",
        "        \"stanford-gpt2-small-b\",\n",
        "        \"battlestar-gpt2-small-x49\",\n",
        "        \"gpt2-mistral-small-b\",\n",
        "        \"gpt2-mistral-small-b\",\n",
        "    ],\n",
        "    \"stanford-crfm/caprica-gpt2-small-x81\": [\n",
        "        \"stanford-gpt2-small-c\",\n",
        "        \"caprica-gpt2-small-x81\",\n",
        "        \"gpt2-mistral-small-c\",\n",
        "        \"gpt2-stanford-small-c\",\n",
        "    ],\n",
        "    \"stanford-crfm/darkmatter-gpt2-small-x343\": [\n",
        "        \"stanford-gpt2-small-d\",\n",
        "        \"darkmatter-gpt2-small-x343\",\n",
        "        \"gpt2-mistral-small-d\",\n",
        "        \"gpt2-mistral-small-d\",\n",
        "    ],\n",
        "    \"stanford-crfm/expanse-gpt2-small-x777\": [\n",
        "        \"stanford-gpt2-small-e\",\n",
        "        \"expanse-gpt2-small-x777\",\n",
        "        \"gpt2-mistral-small-e\",\n",
        "        \"gpt2-mistral-small-e\",\n",
        "    ],\n",
        "    \"stanford-crfm/arwen-gpt2-medium-x21\": [\n",
        "        \"stanford-gpt2-medium-a\",\n",
        "        \"arwen-gpt2-medium-x21\",\n",
        "        \"gpt2-medium-small-a\",\n",
        "        \"gpt2-stanford-medium-a\",\n",
        "    ],\n",
        "    \"stanford-crfm/beren-gpt2-medium-x49\": [\n",
        "        \"stanford-gpt2-medium-b\",\n",
        "        \"beren-gpt2-medium-x49\",\n",
        "        \"gpt2-medium-small-b\",\n",
        "        \"gpt2-stanford-medium-b\",\n",
        "    ],\n",
        "    \"stanford-crfm/celebrimbor-gpt2-medium-x81\": [\n",
        "        \"stanford-gpt2-medium-c\",\n",
        "        \"celebrimbor-gpt2-medium-x81\",\n",
        "        \"gpt2-medium-small-c\",\n",
        "        \"gpt2-medium-small-c\",\n",
        "    ],\n",
        "    \"stanford-crfm/durin-gpt2-medium-x343\": [\n",
        "        \"stanford-gpt2-medium-d\",\n",
        "        \"durin-gpt2-medium-x343\",\n",
        "        \"gpt2-medium-small-d\",\n",
        "        \"gpt2-stanford-medium-d\",\n",
        "    ],\n",
        "    \"stanford-crfm/eowyn-gpt2-medium-x777\": [\n",
        "        \"stanford-gpt2-medium-e\",\n",
        "        \"eowyn-gpt2-medium-x777\",\n",
        "        \"gpt2-medium-small-e\",\n",
        "        \"gpt2-stanford-medium-e\",\n",
        "    ],\n",
        "    \"ArthurConmy/redwood_attn_2l\": [\"redwood_attn_2l\"],\n",
        "    \"llama-7b-hf\": [\"llama-7b\"],\n",
        "    \"llama-13b-hf\": [\"llama-13b\"],\n",
        "    \"llama-30b-hf\": [\"llama-30b\"],\n",
        "    \"llama-65b-hf\": [\"llama-65b\"],\n",
        "    \"meta-llama/Llama-2-7b-hf\": [\"Llama-2-7b\", \"meta-llama/Llama-2-7b-hf\"],\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\": [\n",
        "        \"Llama-2-7b-chat\",\n",
        "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    ],\n",
        "    \"meta-llama/Llama-2-13b-hf\": [\"Llama-2-13b\", \"meta-llama/Llama-2-13b-hf\"],\n",
        "    \"meta-llama/Llama-2-13b-chat-hf\": [\n",
        "        \"Llama-2-13b-chat\",\n",
        "        \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    ],\n",
        "    \"meta-llama/Llama-2-70b-chat-hf\": [\"Llama-2-70b-chat\", \"meta-llama-2-70b-chat-hf\"],\n",
        "    \"CodeLlama-7b-hf\": [\"CodeLlamallama-2-7b\", \"codellama/CodeLlama-7b-hf\"],\n",
        "    \"CodeLlama-7b-Python-hf\": [\n",
        "        \"CodeLlama-7b-python\",\n",
        "        \"codellama/CodeLlama-7b-Python-hf\",\n",
        "    ],\n",
        "    \"CodeLlama-7b-Instruct-hf\": [\n",
        "        \"CodeLlama-7b-instruct\",\n",
        "        \"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "    ],\n",
        "    \"Baidicoot/Othello-GPT-Transformer-Lens\": [\"othello-gpt\"],\n",
        "    \"roneneldan/TinyStories-1M\": [\"tiny-stories-1M\"],\n",
        "    \"roneneldan/TinyStories-3M\": [\"tiny-stories-3M\"],\n",
        "    \"roneneldan/TinyStories-8M\": [\"tiny-stories-8M\"],\n",
        "    \"roneneldan/TinyStories-28M\": [\"tiny-stories-28M\"],\n",
        "    \"roneneldan/TinyStories-33M\": [\"tiny-stories-33M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-1M\": [\"tiny-stories-instruct-1M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-3M\": [\"tiny-stories-instruct-3M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-8M\": [\"tiny-stories-instruct-8M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-28M\": [\"tiny-stories-instruct-28M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-33M\": [\"tiny-stories-instruct-33M\"],\n",
        "    \"roneneldan/TinyStories-1Layer-21M\": [\"tiny-stories-1L-21M\"],\n",
        "    \"roneneldan/TinyStories-2Layers-33M\": [\"tiny-stories-2L-33M\"],\n",
        "    \"roneneldan/TinyStories-Instuct-1Layer-21M\": [\"tiny-stories-instruct-1L-21M\"],\n",
        "    \"roneneldan/TinyStories-Instruct-2Layers-33M\": [\"tiny-stories-instruct-2L-33M\"],\n",
        "    \"stabilityai/stablelm-base-alpha-3b\": [\n",
        "        \"stablelm-base-alpha-3b\",\n",
        "        \"stablelm-base-3b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-base-alpha-7b\": [\n",
        "        \"stablelm-base-alpha-7b\",\n",
        "        \"stablelm-base-7b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-tuned-alpha-3b\": [\n",
        "        \"stablelm-tuned-alpha-3b\",\n",
        "        \"stablelm-tuned-3b\",\n",
        "    ],\n",
        "    \"stabilityai/stablelm-tuned-alpha-7b\": [\n",
        "        \"stablelm-tuned-alpha-7b\",\n",
        "        \"stablelm-tuned-7b\",\n",
        "    ],\n",
        "    \"mistralai/Mistral-7B-v0.1\": [\"mistral-7b\"],\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\": [\"mistral-7b-instruct\"],\n",
        "    \"mistralai/Mixtral-8x7B-v0.1\": [\"mixtral\", \"mixtral-8x7b\"],\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": [\n",
        "        \"mixtral-instruct\",\n",
        "        \"mixtral-8x7b-instruct\",\n",
        "    ],\n",
        "    \"bigscience/bloom-560m\": [\"bloom-560m\"],\n",
        "    \"bigscience/bloom-1b1\": [\"bloom-1b1\"],\n",
        "    \"bigscience/bloom-1b7\": [\"bloom-1b7\"],\n",
        "    \"bigscience/bloom-3b\": [\"bloom-3b\"],\n",
        "    \"bigscience/bloom-7b1\": [\"bloom-7b1\"],\n",
        "    \"bigcode/santacoder\": [\"santacoder\"],\n",
        "    \"Qwen/Qwen-1_8B\": [\"qwen-1.8b\"],\n",
        "    \"Qwen/Qwen-7B\": [\"qwen-7b\"],\n",
        "    \"Qwen/Qwen-14B\": [\"qwen-14b\"],\n",
        "    \"Qwen/Qwen-1_8B-Chat\": [\"qwen-1.8b-chat\"],\n",
        "    \"Qwen/Qwen-7B-Chat\": [\"qwen-7b-chat\"],\n",
        "    \"Qwen/Qwen-14B-Chat\": [\"qwen-14b-chat\"],\n",
        "    \"Qwen/Qwen1.5-0.5B\": [\"qwen1.5-0.5b\"],\n",
        "    \"Qwen/Qwen1.5-0.5B-Chat\": [\"qwen1.5-0.5b-chat\"],\n",
        "    \"Qwen/Qwen1.5-1.8B\": [\"qwen1.5-1.8b\"],\n",
        "    \"Qwen/Qwen1.5-1.8B-Chat\": [\"qwen1.5-1.8b-chat\"],\n",
        "    \"Qwen/Qwen1.5-4B\": [\"qwen1.5-4b\"],\n",
        "    \"Qwen/Qwen1.5-4B-Chat\": [\"qwen1.5-4b-chat\"],\n",
        "    \"Qwen/Qwen1.5-7B\": [\"qwen1.5-7b\"],\n",
        "    \"Qwen/Qwen1.5-7B-Chat\": [\"qwen1.5-7b-chat\"],\n",
        "    \"Qwen/Qwen1.5-14B\": [\"qwen1.5-14b\"],\n",
        "    \"Qwen/Qwen1.5-14B-Chat\": [\"qwen1.5-14b-chat\"],\n",
        "    \"microsoft/phi-1\": [\"phi-1\"],\n",
        "    \"microsoft/phi-1_5\": [\"phi-1_5\"],\n",
        "    \"microsoft/phi-2\": [\"phi-2\"],\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\": [\"phi-3\"],\n",
        "    \"google/gemma-2b\": [\"gemma-2b\"],\n",
        "    \"google/gemma-7b\": [\"gemma-7b\"],\n",
        "    \"google/gemma-2b-it\": [\"gemma-2b-it\"],\n",
        "    \"google/gemma-7b-it\": [\"gemma-7b-it\"],\n",
        "    \"google/gemma-2-2b\": [\"gemma-2-2b\"],\n",
        "    \"google/gemma-2-9b\": [\"gemma-2-9b\"],\n",
        "    \"google/gemma-2-27b\": [\"gemma-2-27b\"],\n",
        "    \"google/gemma-2-2b-it\": [\"gemma-2-2b-it\"],\n",
        "    \"google/gemma-2-9b-it\": [\"gemma-2-9b-it\"],\n",
        "    \"google/gemma-2-27b-it\": [\"gemma-2-27b-it\"],\n",
        "    \"01-ai/Yi-6B\": [\"yi-6b\", \"Yi-6B\"],\n",
        "    \"01-ai/Yi-34B\": [\"yi-34b\", \"Yi-34B\"],\n",
        "    \"01-ai/Yi-6B-Chat\": [\"yi-6b-chat\", \"Yi-6B-Chat\"],\n",
        "    \"01-ai/Yi-34B-Chat\": [\"yi-34b-chat\", \"Yi-34B-Chat\"],\n",
        "    \"google-t5/t5-small\": [\"t5-small\"],\n",
        "    \"google-t5/t5-base\": [\"t5-base\"],\n",
        "    \"google-t5/t5-large\": [\"t5-large\"],\n",
        "    \"ai-forever/mGPT\": [\"mGPT\"],\n",
        "}\n",
        "\"\"\"Model aliases for models on HuggingFace.\"\"\"\n",
        "\n",
        "NON_HF_HOSTED_MODEL_NAMES = [\n",
        "    \"cursivetransformer\", # NEW\n",
        "    \"llama-7b-hf\",\n",
        "    \"llama-13b-hf\",\n",
        "    \"llama-30b-hf\",\n",
        "    \"llama-65b-hf\",\n",
        "]\n",
        "\"\"\"Official model names for models not hosted on HuggingFace.\"\"\"\n",
        "\n",
        "# Sets a default model alias, by convention the first one in the model alias table, else the official name if it has no aliases\n",
        "DEFAULT_MODEL_ALIASES = [\n",
        "    MODEL_ALIASES[name][0] if name in MODEL_ALIASES else name for name in OFFICIAL_MODEL_NAMES\n",
        "]\n",
        "\n",
        "NEED_REMOTE_CODE_MODELS = (\n",
        "    \"bigcode/santacoder\",\n",
        "    \"Qwen/Qwen-\",\n",
        "    \"microsoft/phi-2\",\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        ")"
      ],
      "metadata": {
        "id": "iaYbeY1Iakym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "9zw3J3tEaspN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_alias_map():\n",
        "    \"\"\"\n",
        "    Converts OFFICIAL_MODEL_NAMES (the list of actual model names on\n",
        "    HuggingFace) and MODEL_ALIASES (a dictionary mapping official model names to\n",
        "    aliases) into a dictionary mapping all aliases to the official model name.\n",
        "    \"\"\"\n",
        "    model_alias_map = {}\n",
        "    for official_model_name in OFFICIAL_MODEL_NAMES:\n",
        "        aliases = MODEL_ALIASES.get(official_model_name, [])\n",
        "        for alias in aliases:\n",
        "            model_alias_map[alias.lower()] = official_model_name\n",
        "        model_alias_map[official_model_name.lower()] = official_model_name\n",
        "    return model_alias_map\n",
        "\n",
        "\n",
        "def get_official_model_name(model_name: str):\n",
        "    \"\"\"\n",
        "    Returns the official model name for a given model name (or alias).\n",
        "    \"\"\"\n",
        "    model_alias_map = make_model_alias_map()\n",
        "    official_model_name = model_alias_map.get(model_name.lower(), None)\n",
        "    if official_model_name is None and not model_name.startswith(\"cursivetransformer\"):\n",
        "        raise ValueError(\n",
        "            f\"{model_name} not found. Valid official model names (excl aliases): {OFFICIAL_MODEL_NAMES}\"\n",
        "        )\n",
        "    else:\n",
        "        official_model_name = model_name\n",
        "    return official_model_name\n",
        "\n",
        "\n",
        "def convert_hf_model_config(model_name: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Returns the model config for a HuggingFace model, converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    Takes the official_model_name as an input.\n",
        "    \"\"\"\n",
        "    # In case the user passed in an alias\n",
        "    if (Path(model_name) / \"config.json\").exists():\n",
        "        logging.info(\"Loading model config from local directory\")\n",
        "        official_model_name = model_name\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(model_name)\n",
        "\n",
        "    # Load HuggingFace model config\n",
        "    if \"llama\" in official_model_name.lower():\n",
        "        architecture = \"LlamaForCausalLM\"\n",
        "    elif \"gemma-2\" in official_model_name.lower():\n",
        "        architecture = \"Gemma2ForCausalLM\"\n",
        "    elif \"gemma\" in official_model_name.lower():\n",
        "        architecture = \"GemmaForCausalLM\"\n",
        "    else:\n",
        "        huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "        hf_config = AutoConfig.from_pretrained(\n",
        "            official_model_name,\n",
        "            token=huggingface_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "        architecture = hf_config.architectures[0]\n",
        "\n",
        "    if official_model_name.startswith(\n",
        "        (\"llama-7b\", \"meta-llama/Llama-2-7b\")\n",
        "    ):  # same architecture for LLaMA and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 11008,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 2048 if official_model_name.startswith(\"llama-7b\") else 4096,\n",
        "            \"eps\": 1e-6 if official_model_name.startswith(\"llama-7b\") else 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"CodeLlama-7b\"):  # same architecture CodeLlama and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 11008,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 4096,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 32016,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "            \"rotary_base\": 1000000,\n",
        "        }\n",
        "        if \"python\" in official_model_name.lower():\n",
        "            # The vocab size of python version of CodeLlama-7b is 32000\n",
        "            cfg_dict[\"d_vocab\"] = 32000\n",
        "    elif official_model_name.startswith(\n",
        "        (\"llama-13b\", \"meta-llama/Llama-2-13b\")\n",
        "    ):  # same architecture for LLaMA and Llama-2\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 5120,\n",
        "            \"d_head\": 5120 // 40,\n",
        "            \"n_heads\": 40,\n",
        "            \"d_mlp\": 13824,\n",
        "            \"n_layers\": 40,\n",
        "            \"n_ctx\": 2048 if official_model_name.startswith(\"llama-13b\") else 4096,\n",
        "            \"eps\": 1e-6 if official_model_name.startswith(\"llama-13b\") else 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 5120 // 40,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"llama-30b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 6656,\n",
        "            \"d_head\": 6656 // 52,\n",
        "            \"n_heads\": 52,\n",
        "            \"d_mlp\": 17920,\n",
        "            \"n_layers\": 60,\n",
        "            \"n_ctx\": 2048,\n",
        "            \"eps\": 1e-6,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 6656 // 52,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"llama-65b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 8192 // 64,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 22016,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 2048,\n",
        "            \"eps\": 1e-6,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": 8192 // 64,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-2-70b\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 28672,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 4096,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Meta-Llama-3-8B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Meta-Llama-3-70B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 8192,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 64,\n",
        "            \"d_mlp\": 28672,\n",
        "            \"n_layers\": 80,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-1B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 64,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 16,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 64,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-3B\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 24,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-1B-Instruct\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 64,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 16,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 64,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif \"Llama-3.2-3B-Instruct\" in official_model_name:\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 24,\n",
        "            \"d_mlp\": 8192,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 2048,  # capped due to memory issues\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": 128256,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": 128,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"GPTNeoForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_heads,\n",
        "            \"n_heads\": hf_config.num_heads,\n",
        "            \"d_mlp\": hf_config.hidden_size * 4,\n",
        "            \"n_layers\": hf_config.num_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"attn_types\": hf_config.attention_layers,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": False,\n",
        "            \"use_local_attn\": True,\n",
        "            \"window_size\": hf_config.window_size,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPT2LMHeadModel\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.n_embd * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_ctx,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": hf_config.scale_attn_by_inverse_layer_idx,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"OPTForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.ffn_dim,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPTJForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": 4 * hf_config.n_embd,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_positions,\n",
        "            \"eps\": 1e-5,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": hf_config.rotary_dim,\n",
        "            \"rotary_adjacent_pairs\": True,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"GPTNeoXForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"scale_attn_by_inverse_layer_idx\": False,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "        rotary_pct = hf_config.rotary_pct\n",
        "        cfg_dict[\"rotary_dim\"] = round(rotary_pct * cfg_dict[\"d_head\"])\n",
        "    elif architecture == \"BertForMaskedLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"gelu\",\n",
        "            \"attention_dir\": \"bidirectional\",\n",
        "        }\n",
        "    elif architecture == \"MistralForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4096,\n",
        "            \"d_head\": 4096 // 32,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 32,\n",
        "            \"n_ctx\": 2048,  # Capped due to memory issues\n",
        "            \"d_vocab\": 32000,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"window_size\": 4096,\n",
        "            \"attn_types\": [\"local\"] * 32,\n",
        "            \"eps\": 1e-05,\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"gated_mlp\": True,\n",
        "            \"use_local_attn\": True,\n",
        "            \"rotary_dim\": 4096 // 32,\n",
        "        }\n",
        "    elif architecture == \"MixtralForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"dtype\": torch.bfloat16,\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,  # Capped due to memory issues\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"window_size\": hf_config.sliding_window,  # This is None, as no sliding window was used\n",
        "            \"attn_types\": [\"global\"] * 32,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"n_key_value_heads\": hf_config.num_key_value_heads,\n",
        "            \"gated_mlp\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"num_experts\": hf_config.num_local_experts,\n",
        "            \"experts_per_token\": hf_config.num_experts_per_tok,\n",
        "        }\n",
        "    elif architecture == \"BloomForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.hidden_size * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": 2048,  # Capped due to HF Tokenizer Constraints\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"gelu_fast\",\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"normalization_type\": \"LN\",\n",
        "            \"post_embedding_ln\": True,\n",
        "            \"positional_embedding_type\": \"alibi\",\n",
        "        }\n",
        "    elif architecture == \"GPT2LMHeadCustomModel\":\n",
        "        # santacoder\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.n_embd,\n",
        "            \"d_head\": hf_config.n_embd // hf_config.n_head,\n",
        "            \"n_heads\": hf_config.n_head,\n",
        "            \"d_mlp\": hf_config.n_embd * 4,\n",
        "            \"n_layers\": hf_config.n_layer,\n",
        "            \"n_ctx\": hf_config.n_positions,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.activation_function,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"use_local_attn\": False,\n",
        "            \"trust_remote_code\": \"santacoder\"\n",
        "            in official_model_name,  # Only santacoder needs trust_remote_code\n",
        "            \"scale_attn_by_inverse_layer_idx\": hf_config.scale_attn_by_inverse_layer_idx,\n",
        "            \"normalization_type\": \"LN\",\n",
        "        }\n",
        "    elif architecture == \"LlamaForCausalLM\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"n_key_value_heads\": (\n",
        "                hf_config.num_key_value_heads\n",
        "                if hf_config.num_key_value_heads != hf_config.num_attention_heads\n",
        "                else None\n",
        "            ),\n",
        "            # This is done because the current implementation of GQA will use Grouped-Query Attention if\n",
        "            # n_key_value_heads is not None, but hf_config.num_key_value_heads is sometimes specified as\n",
        "            # the same as hf_config.num_attention_heads, in which case GQA should not be used.\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"QWenLMHeadModel\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size // 2,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": \"silu\",\n",
        "            \"use_attn_scale\": hf_config.scale_attn_weights,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_dim\": hf_config.kv_channels,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"tokenizer_prepends_bos\": True,\n",
        "            \"trust_remote_code\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"Qwen2ForCausalLM\":\n",
        "        # Note that Qwen1.5 models have architecture type Qwen2ForCausalLM.\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"n_key_value_heads\": hf_config.num_key_value_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": 2048,  # Capped bc the actual ctx length is 30k and the attn mask would be too big\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"rotary_adjacent_pairs\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"tokenizer_prepends_bos\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"gated_mlp\": True,\n",
        "        }\n",
        "    elif architecture == \"PhiForCausalLM\":\n",
        "        # Architecture for microsoft/phi models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.layer_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"LN\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"parallel_attn_mlp\": True,\n",
        "        }\n",
        "        partial_rotary_factor = hf_config.partial_rotary_factor\n",
        "        cfg_dict[\"rotary_dim\"] = round(partial_rotary_factor * cfg_dict[\"d_head\"])\n",
        "    elif architecture == \"Phi3ForCausalLM\":\n",
        "        # Architecture for microsoft/phi3 models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.hidden_size,\n",
        "            \"d_head\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "            \"n_heads\": hf_config.num_attention_heads,\n",
        "            \"d_mlp\": hf_config.intermediate_size,\n",
        "            \"n_layers\": hf_config.num_hidden_layers,\n",
        "            \"n_ctx\": hf_config.max_position_embeddings,\n",
        "            \"eps\": hf_config.rms_norm_eps,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"act_fn\": hf_config.hidden_act,\n",
        "            \"initializer_range\": hf_config.initializer_range,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"rotary_base\": hf_config.rope_theta,\n",
        "            \"use_attn_scale\": True,\n",
        "            \"gated_mlp\": True,\n",
        "            \"parallel_attn_mlp\": False,\n",
        "            \"rotary_dim\": hf_config.hidden_size // hf_config.num_attention_heads,\n",
        "        }\n",
        "\n",
        "    elif official_model_name.startswith(\"google/gemma-2b\"):\n",
        "        # Architecture for Gemma 2b and Gemma 2b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2048,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 8,\n",
        "            \"d_mlp\": 16384,\n",
        "            \"n_layers\": 18,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_new\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"rotary_dim\": 256,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 1,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-7b\"):\n",
        "        # Architecture for Gemma 7b and Gemma 7b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3072,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 16,\n",
        "            \"d_mlp\": 24576,\n",
        "            \"n_layers\": 28,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_new\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"rotary_dim\": 256,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 16,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-2b\"):\n",
        "        # Architecture for Gemma-2 2b and Gemma-2 2b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 2304,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 8,\n",
        "            \"d_mlp\": 9216,\n",
        "            \"n_layers\": 26,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 4,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 21,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-9b\"):\n",
        "        # Architecture for Gemma-2 9b and Gemma-2 9b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 3584,\n",
        "            \"d_head\": 256,\n",
        "            \"n_heads\": 16,\n",
        "            \"d_mlp\": 14336,\n",
        "            \"n_layers\": 42,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"n_key_value_heads\": 8,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 21,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif official_model_name.startswith(\"google/gemma-2-27b\"):\n",
        "        # Architecture for Gemma-2 27b and Gemma-2 27b Instruct models\n",
        "        cfg_dict = {\n",
        "            \"d_model\": 4608,\n",
        "            \"d_head\": 128,\n",
        "            \"n_heads\": 32,\n",
        "            \"d_mlp\": 36864,\n",
        "            \"n_layers\": 46,\n",
        "            \"n_ctx\": 8192,\n",
        "            \"eps\": 1e-06,\n",
        "            \"d_vocab\": 256000,\n",
        "            \"act_fn\": \"gelu_pytorch_tanh\",\n",
        "            \"initializer_range\": 0.02,\n",
        "            \"normalization_type\": \"RMS\",\n",
        "            \"rotary_base\": 10000.0,\n",
        "            \"positional_embedding_type\": \"rotary\",\n",
        "            \"use_attn_scale\": True,\n",
        "            \"attn_scale\": 12.0,\n",
        "            \"n_key_value_heads\": 16,\n",
        "            \"window_size\": 4096,\n",
        "            \"use_local_attn\": True,\n",
        "            \"attn_types\": [\"global\", \"local\"] * 23,  # Alternate global and local attn\n",
        "            \"attn_scores_soft_cap\": 50.0,\n",
        "            \"output_logits_soft_cap\": 30.0,\n",
        "            \"gated_mlp\": True,\n",
        "            \"final_rms\": True,\n",
        "            \"use_normalization_before_and_after\": True,\n",
        "        }\n",
        "    elif architecture == \"T5ForConditionalGeneration\":\n",
        "        cfg_dict = {\n",
        "            \"d_model\": hf_config.d_model,\n",
        "            \"d_head\": hf_config.d_kv,\n",
        "            \"n_heads\": hf_config.num_heads,\n",
        "            \"d_mlp\": hf_config.d_ff,\n",
        "            \"d_vocab\": hf_config.vocab_size,\n",
        "            \"n_layers\": hf_config.num_layers,\n",
        "            \"n_ctx\": hf_config.max_length,\n",
        "            \"eps\": hf_config.layer_norm_epsilon,\n",
        "            \"act_fn\": hf_config.feed_forward_proj,\n",
        "            \"positional_embedding_type\": \"relative_positional_bias\",\n",
        "            \"relative_attention_max_distance\": hf_config.relative_attention_max_distance,\n",
        "            \"relative_attention_num_buckets\": hf_config.relative_attention_num_buckets,\n",
        "            \"decoder_start_token_id\": hf_config.decoder_start_token_id,\n",
        "            \"attention_dir\": \"bidirectional\",\n",
        "            \"use_attn_scale\": False,\n",
        "            \"tie_word_embeddings\": hf_config.tie_word_embeddings,\n",
        "        }\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{architecture} is not currently supported.\")\n",
        "    # All of these models use LayerNorm\n",
        "    cfg_dict[\"original_architecture\"] = architecture\n",
        "    # The name such that AutoTokenizer.from_pretrained works\n",
        "    cfg_dict[\"tokenizer_name\"] = official_model_name\n",
        "    if kwargs.get(\"trust_remote_code\", False):\n",
        "        cfg_dict[\"trust_remote_code\"] = True\n",
        "    return cfg_dict\n",
        "\n",
        "\n",
        "def convert_neel_model_config(official_model_name: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads the config for a model trained by me (NeelNanda), converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    AutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.\n",
        "    \"\"\"\n",
        "    official_model_name = get_official_model_name(official_model_name)\n",
        "    cfg_json: dict = utils.download_file_from_hf(official_model_name, \"config.json\", **kwargs)\n",
        "    cfg_arch = cfg_json.get(\n",
        "        \"architecture\", \"neel\" if \"_old\" not in official_model_name else \"neel-solu-old\"\n",
        "    )\n",
        "    cfg_dict = {\n",
        "        \"d_model\": cfg_json[\"d_model\"],\n",
        "        \"n_layers\": cfg_json[\"n_layers\"],\n",
        "        \"d_mlp\": cfg_json[\"d_mlp\"],\n",
        "        \"d_head\": cfg_json[\"d_head\"],\n",
        "        \"n_heads\": cfg_json[\"n_heads\"],\n",
        "        \"n_ctx\": cfg_json[\"n_ctx\"],\n",
        "        \"d_vocab\": cfg_json[\"d_vocab\"],\n",
        "        \"tokenizer_name\": cfg_json.get(\"tokenizer_name\", None),\n",
        "        \"act_fn\": cfg_json[\"act_fn\"],\n",
        "        \"attn_only\": cfg_json[\"attn_only\"],\n",
        "        \"final_rms\": cfg_json.get(\"final_rms\", False),\n",
        "        \"original_architecture\": cfg_arch,\n",
        "    }\n",
        "    if \"normalization\" in cfg_json:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization\"]\n",
        "    else:\n",
        "        cfg_dict[\"normalization_type\"] = cfg_json[\"normalization_type\"]\n",
        "    if \"shortformer_pos\" in cfg_json:\n",
        "        cfg_dict[\"positional_embedding_type\"] = (\n",
        "            \"shortformer\" if cfg_json[\"shortformer_pos\"] else \"standard\"\n",
        "        )\n",
        "    else:\n",
        "        cfg_dict[\"positional_embedding_type\"] = \"standard\"\n",
        "    return cfg_dict\n",
        "\n",
        "def get_pretrained_model_config(\n",
        "    model_name: str,\n",
        "    hf_cfg: Optional[dict] = None,\n",
        "    checkpoint_index: Optional[int] = None,\n",
        "    checkpoint_value: Optional[int] = None,\n",
        "    fold_ln: bool = False,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    n_devices: int = 1,\n",
        "    default_prepend_bos: bool = True,\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    first_n_layers: Optional[int] = None,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"Returns the pretrained model config as an HookedTransformerConfig object.\n",
        "\n",
        "    There are two types of pretrained models: HuggingFace models (where\n",
        "    AutoModel and AutoConfig work), and models trained by me (NeelNanda) which\n",
        "    aren't as integrated with HuggingFace infrastructure.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model. This can be either the official\n",
        "            HuggingFace model name, or the name of a model trained by me\n",
        "            (NeelNanda).\n",
        "        hf_cfg (dict, optional): Config of a loaded pretrained HF model,\n",
        "            converted to a dictionary.\n",
        "        checkpoint_index (int, optional): If loading from a\n",
        "            checkpoint, the index of the checkpoint to load. Defaults to None.\n",
        "        checkpoint_value (int, optional): If loading from a checkpoint, the\n",
        "        value of\n",
        "            the checkpoint to load, ie the step or token number (each model has\n",
        "            checkpoints labelled with exactly one of these). Defaults to None.\n",
        "        fold_ln (bool, optional): Whether to fold the layer norm into the\n",
        "            subsequent linear layers (see HookedTransformer.fold_layer_norm for\n",
        "            details). Defaults to False.\n",
        "        device (str, optional): The device to load the model onto. By\n",
        "            default will load to CUDA if available, else CPU.\n",
        "        n_devices (int, optional): The number of devices to split the model across. Defaults to 1.\n",
        "        default_prepend_bos (bool, optional): Default behavior of whether to prepend the BOS token when the\n",
        "            methods of HookedTransformer process input text to tokenize (only when input is a string).\n",
        "            Defaults to True - even for models not explicitly trained with this, heads often use the\n",
        "            first position as a resting position and accordingly lose information from the first token,\n",
        "            so this empirically seems to give better results. To change the default behavior to False, pass in\n",
        "            default_prepend_bos=False. Note that you can also locally override the default behavior by passing\n",
        "            in prepend_bos=True/False when you call a method that processes the input string.\n",
        "        dtype (torch.dtype, optional): The dtype to load the TransformerLens model in.\n",
        "        kwargs: Other optional arguments passed to HuggingFace's from_pretrained.\n",
        "            Also given to other HuggingFace functions when compatible.\n",
        "\n",
        "    \"\"\"\n",
        "    if Path(model_name).exists():\n",
        "        # If the model_name is a path, it's a local model\n",
        "        cfg_dict = convert_hf_model_config(model_name, **kwargs)\n",
        "        official_model_name = model_name\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(model_name)\n",
        "    if (\n",
        "        official_model_name.startswith(\"NeelNanda\")\n",
        "        or official_model_name.startswith(\"ArthurConmy\")\n",
        "        or official_model_name.startswith(\"Baidicoot\")\n",
        "    ):\n",
        "        cfg_dict = convert_neel_model_config(official_model_name, **kwargs)\n",
        "    else:\n",
        "        if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(\n",
        "            \"trust_remote_code\", False\n",
        "        ):\n",
        "            logging.warning(\n",
        "                f\"Loading model {official_model_name} requires setting trust_remote_code=True\"\n",
        "            )\n",
        "            kwargs[\"trust_remote_code\"] = True\n",
        "        cfg_dict = convert_hf_model_config(official_model_name, **kwargs)\n",
        "    # Processing common to both model types\n",
        "    # Remove any prefix, saying the organization who made a model.\n",
        "    cfg_dict[\"model_name\"] = official_model_name.split(\"/\")[-1]\n",
        "    # Don't need to initialize weights, we're loading from pretrained\n",
        "    cfg_dict[\"init_weights\"] = False\n",
        "\n",
        "    if (\n",
        "        \"positional_embedding_type\" in cfg_dict\n",
        "        and cfg_dict[\"positional_embedding_type\"] == \"shortformer\"\n",
        "        and fold_ln\n",
        "    ):\n",
        "        logging.warning(\n",
        "            \"You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_ln=False instead.\"\n",
        "        )\n",
        "        fold_ln = False\n",
        "\n",
        "    if device is not None:\n",
        "        cfg_dict[\"device\"] = device\n",
        "\n",
        "    cfg_dict[\"dtype\"] = dtype\n",
        "\n",
        "    if fold_ln:\n",
        "        if cfg_dict[\"normalization_type\"] in [\"LN\", \"LNPre\"]:\n",
        "            cfg_dict[\"normalization_type\"] = \"LNPre\"\n",
        "        elif cfg_dict[\"normalization_type\"] in [\"RMS\", \"RMSPre\"]:\n",
        "            cfg_dict[\"normalization_type\"] = \"RMSPre\"\n",
        "        else:\n",
        "            logging.warning(\"Cannot fold in layer norm, normalization_type is not LN.\")\n",
        "\n",
        "    if checkpoint_index is not None or checkpoint_value is not None:\n",
        "        checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(\n",
        "            official_model_name,\n",
        "            **kwargs,\n",
        "        )\n",
        "        cfg_dict[\"from_checkpoint\"] = True\n",
        "        cfg_dict[\"checkpoint_label_type\"] = checkpoint_label_type\n",
        "        if checkpoint_index is not None:\n",
        "            cfg_dict[\"checkpoint_index\"] = checkpoint_index\n",
        "            cfg_dict[\"checkpoint_value\"] = checkpoint_labels[checkpoint_index]\n",
        "        elif checkpoint_value is not None:\n",
        "            assert (\n",
        "                checkpoint_value in checkpoint_labels\n",
        "            ), f\"Checkpoint value {checkpoint_value} is not in list of available checkpoints\"\n",
        "            cfg_dict[\"checkpoint_value\"] = checkpoint_value\n",
        "            cfg_dict[\"checkpoint_index\"] = checkpoint_labels.index(checkpoint_value)\n",
        "    else:\n",
        "        cfg_dict[\"from_checkpoint\"] = False\n",
        "\n",
        "    cfg_dict[\"device\"] = device\n",
        "    cfg_dict[\"n_devices\"] = n_devices\n",
        "    cfg_dict[\"default_prepend_bos\"] = default_prepend_bos\n",
        "    if hf_cfg is not None:\n",
        "        cfg_dict[\"load_in_4bit\"] = hf_cfg.get(\"quantization_config\", {}).get(\"load_in_4bit\", False)\n",
        "    if first_n_layers is not None:\n",
        "        cfg_dict[\"n_layers\"] = first_n_layers\n",
        "\n",
        "    cfg = HookedTransformerConfig.from_dict(cfg_dict)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def get_num_params_of_pretrained(model_name):\n",
        "    \"\"\"\n",
        "    Returns the number of parameters of a pretrained model, used to filter to only run code for sufficiently small models.\n",
        "    \"\"\"\n",
        "    cfg = get_pretrained_model_config(model_name)\n",
        "    return cfg.n_params\n",
        "\n",
        "\n",
        "# %% Load checkpointed model state dicts\n",
        "# The steps for which there are checkpoints in the stanford crfm models\n",
        "STANFORD_CRFM_CHECKPOINTS = (\n",
        "    list(range(0, 100, 10))\n",
        "    + list(range(100, 2000, 50))\n",
        "    + list(range(2000, 20000, 100))\n",
        "    + list(range(20000, 400000 + 1, 1000))\n",
        ")\n",
        "\n",
        "# Linearly spaced checkpoints for Pythia models, taken every 1000 steps.\n",
        "# Batch size 2,097,152 tokens, so checkpoints every 2.1B tokens\n",
        "PYTHIA_CHECKPOINTS = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512] + list(\n",
        "    range(1000, 143000 + 1, 1000)\n",
        ")\n",
        "# Pythia V1 has log-spaced early checkpoints (see line above), but V0 doesn't\n",
        "PYTHIA_V0_CHECKPOINTS = list(range(1000, 143000 + 1, 1000))\n",
        "\n",
        "\n",
        "def get_checkpoint_labels(model_name: str, **kwargs):\n",
        "    \"\"\"Returns the checkpoint labels for a given model, and the label_type\n",
        "    (step or token). Raises an error for models that are not checkpointed.\"\"\"\n",
        "    official_model_name = get_official_model_name(model_name)\n",
        "    if official_model_name.startswith(\"stanford-crfm/\"):\n",
        "        return STANFORD_CRFM_CHECKPOINTS, \"step\"\n",
        "    elif official_model_name.startswith(\"EleutherAI/pythia\"):\n",
        "        if \"v0\" in official_model_name:\n",
        "            return PYTHIA_V0_CHECKPOINTS, \"step\"\n",
        "        else:\n",
        "            logging.warning(\n",
        "                \"Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\"\n",
        "            )\n",
        "            return PYTHIA_CHECKPOINTS, \"step\"\n",
        "    elif official_model_name.startswith(\"NeelNanda/\"):\n",
        "        api = HfApi()\n",
        "        files_list = api.list_repo_files(\n",
        "            official_model_name,\n",
        "            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),\n",
        "        )\n",
        "        labels = []\n",
        "        for file_name in files_list:\n",
        "            match = re.match(r\"checkpoints/.*_(\\d*)\\.pth\", file_name)\n",
        "            if match:\n",
        "                labels.append(int(match.group(1)))\n",
        "        if labels[-1] > 1e9:\n",
        "            label_type = \"token\"\n",
        "        else:\n",
        "            label_type = \"step\"\n",
        "        return labels, label_type\n",
        "    elif ( # NEW\n",
        "        official_model_name.startswith(\"zwimpee\") # NEW\n",
        "        or official_model_name.startswith(\"sam.greydanus\") # NEW\n",
        "        or official_model_name.startswith(\"cursivetransformer\") # NEW\n",
        "    ): # NEW\n",
        "        # NEW\n",
        "        # - [ ] TODO: Implement this!\n",
        "        ...\n",
        "    else:\n",
        "        raise ValueError(f\"Model {official_model_name} is not checkpointed.\")\n",
        "\n",
        "\n",
        "# %% Loading state dicts\n",
        "def get_pretrained_state_dict(\n",
        "    official_model_name: str,\n",
        "    cfg: HookedTransformerConfig,\n",
        "    hf_model=None,\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "    **kwargs,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Loads in the model weights for a pretrained model, and processes them to\n",
        "    have the HookedTransformer parameter names and shapes. Supports checkpointed\n",
        "    models (and expects the checkpoint info to be stored in the config object)\n",
        "\n",
        "    hf_model: Optionally, a HuggingFace model object. If provided, we will use\n",
        "        these weights rather than reloading the model.\n",
        "    dtype: The dtype to load the HuggingFace model in.\n",
        "    kwargs: Other optional arguments passed to HuggingFace's from_pretrained.\n",
        "        Also given to other HuggingFace functions when compatible.\n",
        "    \"\"\"\n",
        "    if \"torch_dtype\" in kwargs:\n",
        "        dtype = kwargs[\"torch_dtype\"]\n",
        "        del kwargs[\"torch_dtype\"]\n",
        "    if Path(official_model_name).exists():\n",
        "        official_model_name = str(Path(official_model_name).resolve())\n",
        "        logging.info(f\"Loading model from local path {official_model_name}\")\n",
        "    else:\n",
        "        official_model_name = get_official_model_name(official_model_name)\n",
        "    if official_model_name.startswith(NEED_REMOTE_CODE_MODELS) and not kwargs.get(\n",
        "        \"trust_remote_code\", False\n",
        "    ):\n",
        "        logging.warning(\n",
        "            f\"Loading model {official_model_name} state dict requires setting trust_remote_code=True\"\n",
        "        )\n",
        "        kwargs[\"trust_remote_code\"] = True\n",
        "    if (\n",
        "        official_model_name.startswith(\"NeelNanda\")\n",
        "        or official_model_name.startswith(\"ArthurConmy\")\n",
        "        or official_model_name.startswith(\"Baidicoot\")\n",
        "        or official_model_name.startswith(\"cursivetransformer\") # NEW\n",
        "    ):\n",
        "        api = HfApi()\n",
        "        repo_files = api.list_repo_files(\n",
        "            official_model_name,\n",
        "            **utils.select_compatible_kwargs(kwargs, api.list_repo_files),\n",
        "        )\n",
        "        if cfg.from_checkpoint:\n",
        "            file_name = list(\n",
        "                filter(lambda x: x.endswith(f\"{cfg.checkpoint_value}.pth\"), repo_files)\n",
        "            )[0]\n",
        "        else:\n",
        "            file_name = list(filter(lambda x: x.endswith(\"final.pth\"), repo_files))[0]\n",
        "        if official_model_name.startswith(\"cursivetransformer\"):\n",
        "            state_dict = kwargs.get(\"model\").state_dict()\n",
        "        else:\n",
        "            state_dict = utils.download_file_from_hf(official_model_name, file_name, **kwargs)\n",
        "\n",
        "        # Convert to dtype\n",
        "        state_dict = {k: v.to(dtype) for k, v in state_dict.items()}\n",
        "\n",
        "        if cfg.original_architecture == \"neel-solu-old\":\n",
        "            state_dict = convert_neel_solu_old_weights(state_dict, cfg)\n",
        "        elif cfg.original_architecture == \"mingpt\":\n",
        "            state_dict = convert_mingpt_weights(state_dict, cfg)\n",
        "        elif cfg.original_architecture == \"cursivetransformer\": # NEW\n",
        "            state_dict = convert_cursivetransformer_weights(state_dict, cfg)\n",
        "        return state_dict\n",
        "    else:\n",
        "        if cfg.from_checkpoint:\n",
        "            huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "            if official_model_name.startswith(\"stanford-crfm\"):\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    revision=f\"checkpoint-{cfg.checkpoint_value}\",\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            elif official_model_name.startswith(\"EleutherAI/pythia\"):\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    revision=f\"step{cfg.checkpoint_value}\",\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Checkpoints for model {official_model_name} are not supported\")\n",
        "        elif hf_model is None:\n",
        "            huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "            if official_model_name in NON_HF_HOSTED_MODEL_NAMES:\n",
        "                raise NotImplementedError(\"Model not hosted on HuggingFace, must pass in hf_model\")\n",
        "            elif \"bert\" in official_model_name:\n",
        "                hf_model = BertForPreTraining.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            elif \"t5\" in official_model_name:\n",
        "                hf_model = T5ForConditionalGeneration.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            else:\n",
        "                hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    official_model_name,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=huggingface_token,\n",
        "                    **kwargs,\n",
        "                )\n",
        "\n",
        "            # Load model weights, and fold in layer norm weights\n",
        "\n",
        "        for param in hf_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if cfg.original_architecture == \"GPT2LMHeadModel\":\n",
        "            state_dict = convert_gpt2_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTNeoForCausalLM\":\n",
        "            state_dict = convert_neo_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"OPTForCausalLM\":\n",
        "            state_dict = convert_opt_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTJForCausalLM\":\n",
        "            state_dict = convert_gptj_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPTNeoXForCausalLM\":\n",
        "            state_dict = convert_neox_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"LlamaForCausalLM\":\n",
        "            state_dict = convert_llama_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"BertForMaskedLM\":\n",
        "            state_dict = convert_bert_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"T5ForConditionalGeneration\":\n",
        "            state_dict = convert_t5_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"MistralForCausalLM\":\n",
        "            state_dict = convert_mistral_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"MixtralForCausalLM\":\n",
        "            state_dict = convert_mixtral_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"BloomForCausalLM\":\n",
        "            state_dict = convert_bloom_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GPT2LMHeadCustomModel\":\n",
        "            state_dict = convert_coder_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"QWenLMHeadModel\":\n",
        "            state_dict = convert_qwen_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Qwen2ForCausalLM\":\n",
        "            state_dict = convert_qwen2_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"PhiForCausalLM\":\n",
        "            state_dict = convert_phi_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Phi3ForCausalLM\":\n",
        "            state_dict = convert_phi3_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"GemmaForCausalLM\":\n",
        "            state_dict = convert_gemma_weights(hf_model, cfg)\n",
        "        elif cfg.original_architecture == \"Gemma2ForCausalLM\":\n",
        "            state_dict = convert_gemma_weights(hf_model, cfg)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Loading weights from the architecture is not currently supported: {cfg.original_architecture}, generated from model name {cfg.model_name}. Feel free to open an issue on GitHub to request this feature.\"\n",
        "            )\n",
        "\n",
        "        return state_dict\n",
        "\n",
        "\n",
        "def fill_missing_keys(model, state_dict):\n",
        "    \"\"\"Takes in a state dict from a pretrained model, and fills in any missing keys with the default initialization.\n",
        "\n",
        "    This function is assumed to be run before weights are initialized.\n",
        "\n",
        "    Args:\n",
        "        state_dict (dict): State dict from a pretrained model\n",
        "\n",
        "    Returns:\n",
        "        dict: State dict with missing keys filled in\n",
        "    \"\"\"\n",
        "    # Get the default state dict\n",
        "    default_state_dict = model.state_dict()\n",
        "    # Get the keys that are missing from the pretrained model\n",
        "    missing_keys = set(default_state_dict.keys()) - set(state_dict.keys())\n",
        "    # Fill in the missing keys with the default initialization\n",
        "    for key in missing_keys:\n",
        "        if \"hf_model\" in key:\n",
        "            # Skip keys that are from the HuggingFace model, if loading from HF.\n",
        "            continue\n",
        "        if \"W_\" in key:\n",
        "            logging.warning(\n",
        "                \"Missing key for a weight matrix in pretrained, filled in with an empty tensor: {}\".format(\n",
        "                    key\n",
        "                )\n",
        "            )\n",
        "        state_dict[key] = default_state_dict[key]\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    d_model: int = 768\n",
        "    debug: bool = True\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024\n",
        "    d_head: int = 64\n",
        "    d_mlp: int = 3072\n",
        "    n_heads: int = 12\n",
        "    n_layers: int = 12\n",
        "\n",
        "\n",
        "# Returns the configuration parameters of the model as a basic Config dataclass\n",
        "def get_basic_config(model_name: str, **kwargs) -> Config:\n",
        "    return Config(\n",
        "        **{\n",
        "            k: v\n",
        "            for k, v in get_pretrained_model_config(model_name, **kwargs).to_dict().items()\n",
        "            if k\n",
        "            in [\n",
        "                \"d_model\",\n",
        "                \"debug\",\n",
        "                \"layer_norm_eps\",\n",
        "                \"d_vocab\",\n",
        "                \"init_range\",\n",
        "                \"n_ctx\",\n",
        "                \"d_head\",\n",
        "                \"d_mlp\",\n",
        "                \"n_heads\",\n",
        "                \"n_layers\",\n",
        "            ]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "KJxG7qFMQsVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerLens/transformer_lens/HookedTransformer.py\n",
        "- https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/HookedTransformer.py#L1055"
      ],
      "metadata": {
        "id": "ZfwtFkhDdQSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "ftMdFuYndiud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Hooked Transformer.\n",
        "\n",
        "The Hooked Transformer is the core part of TransformerLens.\n",
        "\n",
        "In common PyTorch model implementations (e.g. ones from HuggingFace) it's fairly easy to extract\n",
        "model weights, but much harder to extract activations. TransformerLens aims to simplify this task by\n",
        "attaching hooks to every notable activation within the model. This enables the inspection and/or\n",
        "alteration of activations in individual components like attention heads and MLP layers, facilitating\n",
        "a deeper understanding of the internal workings of transformers like GPT-2.\n",
        "\"\"\"\n",
        "import logging\n",
        "import os\n",
        "from typing import Dict, List, NamedTuple, Optional, Tuple, Union, cast, overload\n",
        "\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm.auto as tqdm\n",
        "from fancy_einsum import einsum\n",
        "from jaxtyping import Float, Int\n",
        "from packaging import version\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
        "from typing_extensions import Literal\n",
        "\n",
        "import transformer_lens.loading_from_pretrained as loading\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.ActivationCache import ActivationCache\n",
        "from transformer_lens.components import (\n",
        "    Embed,\n",
        "    LayerNorm,\n",
        "    LayerNormPre,\n",
        "    PosEmbed,\n",
        "    RMSNorm,\n",
        "    RMSNormPre,\n",
        "    TransformerBlock,\n",
        "    Unembed,\n",
        ")\n",
        "from transformer_lens.FactoredMatrix import FactoredMatrix\n",
        "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
        "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
        "from transformer_lens.loading_from_pretrained import NON_HF_HOSTED_MODEL_NAMES\n",
        "\n",
        "# Note - activation cache is used with run_with_cache, past_key_value_caching is used for\n",
        "# generation.\n",
        "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
        "from transformer_lens.utilities import devices\n",
        "from transformer_lens.utils import (\n",
        "    USE_DEFAULT_VALUE,\n",
        "    init_kaiming_normal_,\n",
        "    init_kaiming_uniform_,\n",
        "    init_xavier_normal_,\n",
        "    init_xavier_uniform_,\n",
        ")\n",
        "\n",
        "SingleLoss = Float[torch.Tensor, \"\"]  # Type alias for a single element tensor\n",
        "LossPerToken = Float[torch.Tensor, \"batch pos-1\"]\n",
        "Loss = Union[SingleLoss, LossPerToken]\n",
        "\n",
        "DTYPE_FROM_STRING = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"fp32\": torch.float32,\n",
        "    \"float16\": torch.float16,\n",
        "    \"fp16\": torch.float16,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"bf16\": torch.bfloat16,\n",
        "}"
      ],
      "metadata": {
        "id": "YjpS6wegdij4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "HNZPwM7odnao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Output(NamedTuple):\n",
        "    \"\"\"Output Named Tuple.\n",
        "\n",
        "    Named tuple object for if we want to output both logits and loss.\n",
        "    \"\"\"\n",
        "\n",
        "    logits: Float[torch.Tensor, \"batch pos d_vocab\"]\n",
        "    loss: Loss\n",
        "\n",
        "\n",
        "class HookedTransformer(HookedRootModule):\n",
        "    \"\"\"Hooked Transformer.\n",
        "\n",
        "    Implements a full Transformer using the components :doc:`here <transformer_lens.components>`,\n",
        "    with a :class:`transformer_lens.hook_points.HookPoint` on every interesting activation.\n",
        "\n",
        "    TransformerLens comes loaded with >50 GPT-style models. Typically you initialise it with one of\n",
        "    these via :meth:`from_pretrained`, although it can also be instantiated with randomly\n",
        "    initialized weights via :meth:`__init__`.\n",
        "\n",
        "    Once you've initialized the model, a common next step is to test it can do the task you're\n",
        "    investigating. This can be done with :func:`transformer_lens.utils.test_prompt`.\n",
        "    \"\"\"\n",
        "\n",
        "    ln_final: nn.Module\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg: Union[HookedTransformerConfig, Dict],\n",
        "        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
        "        move_to_device: bool = True,\n",
        "        default_padding_side: Literal[\"left\", \"right\"] = \"right\",\n",
        "    ):\n",
        "        \"\"\"Model initialization.\n",
        "\n",
        "        Note that if you want to load the model from pretrained weights, you should use\n",
        "        :meth:`from_pretrained` instead.\n",
        "\n",
        "        Args:\n",
        "            cfg: The config to use for the model.\n",
        "            tokenizer: The tokenizer to use for the model. If not provided, it is inferred from\n",
        "                `cfg.tokenizer_name` or initialized to `None`. If `None`, then the model cannot be\n",
        "                passed strings, and d_vocab must be explicitly set.\n",
        "            move_to_device: Whether to move the model to the device specified in cfg.\n",
        "                device. Must be true if `n_devices` in the config is greater than 1, since the\n",
        "                model's layers will be split across multiple devices.\n",
        "            default_padding_side: Which side to pad on.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if isinstance(cfg, str):\n",
        "            raise ValueError(\n",
        "                \"Please pass in a config dictionary or HookedTransformerConfig object. If you want to load a \"\n",
        "                \"pretrained model, use HookedTransformer.from_pretrained() instead.\"\n",
        "            )\n",
        "\n",
        "        self.cfg = HookedTransformerConfig.unwrap(cfg)\n",
        "\n",
        "        if tokenizer is not None:\n",
        "            self.set_tokenizer(tokenizer, default_padding_side=default_padding_side)\n",
        "        elif self.cfg.tokenizer_name is not None:\n",
        "            # If we have a tokenizer name, we can load it from HuggingFace\n",
        "            if self.cfg.tokenizer_name in NON_HF_HOSTED_MODEL_NAMES:\n",
        "                logging.warning(\n",
        "                    \"%s tokenizer not loaded. Please load manually.\",\n",
        "                    self.cfg.tokenizer_name,\n",
        "                )\n",
        "            else:\n",
        "                # Hugging Face defaults to use_fast to True\n",
        "                use_fast = True\n",
        "                # Phi model's fast tokenizer does not support adding a BOS token, use_fast\n",
        "                # should be False\n",
        "                if \"phi\" in self.cfg.tokenizer_name.lower():\n",
        "                    use_fast = False\n",
        "                huggingface_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "                self.set_tokenizer(\n",
        "                    AutoTokenizer.from_pretrained(\n",
        "                        self.cfg.tokenizer_name,\n",
        "                        add_bos_token=True,\n",
        "                        trust_remote_code=self.cfg.trust_remote_code,\n",
        "                        use_fast=use_fast,\n",
        "                        token=huggingface_token,\n",
        "                    ),\n",
        "                    default_padding_side=default_padding_side,\n",
        "                )\n",
        "        else:\n",
        "            # If no tokenizer name is provided, we assume we're training on an algorithmic task and\n",
        "            # will pass in tokens directly. In this case, we don't need a tokenizer.\n",
        "            assert self.cfg.d_vocab != -1, \"Must provide a tokenizer if d_vocab is not provided\"\n",
        "            self.tokenizer = None\n",
        "            if default_padding_side != \"right\":\n",
        "                logging.warning(\n",
        "                    \"default_padding_side is explictly given but ignored because tokenizer is not set.\"\n",
        "                )\n",
        "\n",
        "        self.embed = Embed(self.cfg)\n",
        "        self.hook_embed = HookPoint()  # [batch, pos, d_model]\n",
        "\n",
        "        if self.cfg.positional_embedding_type != \"rotary\":\n",
        "            self.pos_embed = PosEmbed(self.cfg)\n",
        "            self.hook_pos_embed = HookPoint()  # [batch, pos, d__dictmodel]\n",
        "\n",
        "        if self.cfg.use_hook_tokens:\n",
        "            self.hook_tokens = HookPoint()  # [batch, pos]\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(self.cfg, block_index) for block_index in range(self.cfg.n_layers)]\n",
        "        )\n",
        "\n",
        "        if self.cfg.normalization_type == \"RMS\":\n",
        "            self.ln_final = RMSNorm(self.cfg)\n",
        "        elif self.cfg.normalization_type == \"RMSPre\":\n",
        "            self.ln_final = RMSNormPre(self.cfg)\n",
        "        elif self.cfg.normalization_type == \"LN\":\n",
        "            if self.cfg.final_rms:\n",
        "                self.ln_final = RMSNorm(self.cfg)\n",
        "            else:\n",
        "                self.ln_final = LayerNorm(self.cfg)\n",
        "        elif self.cfg.normalization_type == \"LNPre\":\n",
        "            # We've folded in LayerNorm weights, so just need the center + scale parts\n",
        "            if self.cfg.final_rms:\n",
        "                self.ln_final = RMSNormPre(self.cfg)\n",
        "            else:\n",
        "                self.ln_final = LayerNormPre(self.cfg)\n",
        "        elif self.cfg.normalization_type is None:\n",
        "            # If it's None, don't create either layer\n",
        "            pass\n",
        "        else:\n",
        "            logging.warning(\"Invalid normalization_type passed in %s\", self.cfg.normalization_type)\n",
        "        self.unembed = Unembed(self.cfg)\n",
        "\n",
        "        if self.cfg.init_weights:\n",
        "            self.init_weights()\n",
        "\n",
        "        if move_to_device:\n",
        "            # We load the devices in a pipeline manner - the first device gets the embed and\n",
        "            # pos_embed layers and the first n_layers // n_devices blocks, the second gets the next\n",
        "            # n_layers // n_devices blocks ... the last gets the last n_layers // n_devices blocks,\n",
        "            # the final normalization layer (if it exists) and the unembed layer\n",
        "            self.move_model_modules_to_device()\n",
        "\n",
        "        # Helper variable to store a small (10K-20K) dataset of training data. Empty by default, can\n",
        "        # be loaded with load_sample_training_dataset\n",
        "        self.dataset = None\n",
        "\n",
        "        # Gives each module a parameter with its name (relative to this root module)\n",
        "        # Needed for HookPoints to work\n",
        "        self.setup()\n",
        "\n",
        "    def check_hooks_to_add(\n",
        "        self,\n",
        "        hook_point,\n",
        "        hook_point_name,\n",
        "        hook,\n",
        "        dir=\"fwd\",\n",
        "        is_permanent=False,\n",
        "        prepend=False,\n",
        "    ) -> None:\n",
        "        if hook_point_name.endswith(\"attn.hook_result\"):\n",
        "            assert (\n",
        "                self.cfg.use_attn_result\n",
        "            ), f\"Cannot add hook {hook_point_name} if use_attn_result_hook is False\"\n",
        "        if hook_point_name.endswith((\"hook_q_input\", \"hook_k_input\", \"hook_v_input\")):\n",
        "            assert (\n",
        "                self.cfg.use_split_qkv_input\n",
        "            ), f\"Cannot add hook {hook_point_name} if use_split_qkv_input is False\"\n",
        "        if hook_point_name.endswith(\"mlp_in\"):\n",
        "            assert (\n",
        "                self.cfg.use_hook_mlp_in\n",
        "            ), f\"Cannot add hook {hook_point_name} if use_hook_mlp_in is False\"\n",
        "        if hook_point_name.endswith(\"attn_in\"):\n",
        "            assert (\n",
        "                self.cfg.use_attn_in\n",
        "            ), f\"Cannot add hook {hook_point_name} if use_attn_in is False\"\n",
        "\n",
        "    def input_to_embed(\n",
        "        self,\n",
        "        input: Union[str, List[str], Int[torch.Tensor, \"batch pos\"]],\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> Tuple[\n",
        "        Float[torch.Tensor, \"batch pos d_model\"],  # residual\n",
        "        Optional[Int[torch.Tensor, \"batch pos\"]],  # tokens\n",
        "        Optional[Float[torch.Tensor, \"batch pos d_model\"]],  # shortformer_pos_embed\n",
        "        Optional[torch.Tensor],  # attention_mask [batch pos]\n",
        "    ]:\n",
        "        \"\"\"Convert input to first residual stream.\n",
        "\n",
        "        Args:\n",
        "            input (Union[str, List[str], Int[torch.Tensor, \"batch pos\"]]): The input to the model.\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (only applies when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\n",
        "                otherwise. Pass True or False to locally override the default.\n",
        "            padding_side ([Literal[\"left\", \"right\"], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing\n",
        "                multiple strings of different lengths.\n",
        "            past_kv_cache (HookedTransformerKeyValueCache, optional): If passed, we're doing caching\n",
        "                and attention_mask will be stored in the cache.\n",
        "        \"\"\"\n",
        "        if isinstance(input, str) or isinstance(input, list):\n",
        "            # If text, convert to tokens (batch_size=1)\n",
        "            assert (\n",
        "                self.tokenizer is not None\n",
        "            ), \"Must provide a tokenizer if passing a string to the model\"\n",
        "            # This is only intended to support passing in a single string\n",
        "            tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)\n",
        "        else:\n",
        "            tokens = input\n",
        "        if len(tokens.shape) == 1:\n",
        "            # If tokens are a rank 1 tensor, add a dummy batch dimension to avoid things breaking.\n",
        "            tokens = tokens[None]\n",
        "        if tokens.device.type != self.cfg.device:\n",
        "            tokens = tokens.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            assert attention_mask.shape == tokens.shape, (\n",
        "                f\"Attention mask shape {attention_mask.shape} does not match tokens shape \"\n",
        "                f\"{tokens.shape}\"\n",
        "            )\n",
        "            attention_mask = attention_mask.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "        elif (\n",
        "            self.tokenizer and self.tokenizer.padding_side == \"left\"\n",
        "        ) or past_kv_cache is not None:\n",
        "            # If the padding side is left or we are using caching, we need to compute the attention\n",
        "            # mask for the adjustment of absolute positional embeddings and attention masking so\n",
        "            # that pad tokens are not attended.\n",
        "\n",
        "            if prepend_bos is USE_DEFAULT_VALUE:\n",
        "                prepend_bos = self.cfg.default_prepend_bos\n",
        "            attention_mask = utils.get_attention_mask(self.tokenizer, tokens, prepend_bos)\n",
        "\n",
        "            if past_kv_cache is not None:\n",
        "                # past_kv_cache is not None, so we're doing caching.\n",
        "                # We need to extend the previous attention_mask.\n",
        "                # Update the past_kv_cache with the new attention_mask (unless it's frozen)\n",
        "                attention_mask = past_kv_cache.append_attention_mask(attention_mask)\n",
        "        else:\n",
        "            # We separate this case from for computational efficiency.\n",
        "            attention_mask = None\n",
        "\n",
        "        # If we're doing caching, then we reuse keys and values from previous runs, as that's the\n",
        "        # only way that past activations will affect the final logits. The cache contains those so\n",
        "        # we don't need to recompute them. This is useful for generating text. As we have absolute\n",
        "        # positional encodings, to implement this we have a `pos_offset` variable, defaulting to\n",
        "        # zero, which says to offset which positional encodings are used (cached keys and values\n",
        "        # were calculated with their own positional encodings).\n",
        "        if past_kv_cache is None:\n",
        "            pos_offset = 0\n",
        "        else:\n",
        "            batch_size, ctx_length = tokens.shape\n",
        "            (\n",
        "                cached_batch_size,\n",
        "                cache_ctx_length,\n",
        "                num_heads_in_cache,\n",
        "                d_head_in_cache,\n",
        "            ) = past_kv_cache[0].past_keys.shape\n",
        "            assert cached_batch_size == batch_size\n",
        "            if self.cfg.n_key_value_heads is None:\n",
        "                assert num_heads_in_cache == self.cfg.n_heads\n",
        "            else:\n",
        "                assert num_heads_in_cache == self.cfg.n_key_value_heads\n",
        "            assert d_head_in_cache == self.cfg.d_head\n",
        "            pos_offset = cache_ctx_length\n",
        "        if self.cfg.use_hook_tokens:\n",
        "            tokens = self.hook_tokens(tokens)\n",
        "        embed = self.hook_embed(self.embed(tokens))  # [batch, pos, d_model]\n",
        "        if self.cfg.positional_embedding_type == \"standard\":\n",
        "            pos_embed = self.hook_pos_embed(\n",
        "                self.pos_embed(tokens, pos_offset, attention_mask)\n",
        "            )  # [batch, pos, d_model]\n",
        "            residual = embed + pos_embed  # [batch, pos, d_model]\n",
        "            shortformer_pos_embed = None\n",
        "        elif self.cfg.positional_embedding_type == \"shortformer\":\n",
        "            # If we're using shortformer style attention, we don't add the positional embedding to\n",
        "            # the residual stream. See HookedTransformerConfig for details\n",
        "            pos_embed = self.hook_pos_embed(\n",
        "                self.pos_embed(tokens, pos_offset, attention_mask)\n",
        "            )  # [batch, pos, d_model]\n",
        "            residual = embed\n",
        "            shortformer_pos_embed = pos_embed\n",
        "        elif self.cfg.positional_embedding_type == \"rotary\":\n",
        "            # Rotary doesn't use positional embeddings, instead they're applied when dot producting\n",
        "            # keys and queries. See HookedTransformerConfig for details\n",
        "            residual = embed\n",
        "            shortformer_pos_embed = None\n",
        "        elif self.cfg.positional_embedding_type == \"alibi\":\n",
        "            # ALiBi does not add positional embeddings to word embeddings,instead it biases QK attention scores.\n",
        "            residual = embed\n",
        "            shortformer_pos_embed = None\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid positional_embedding_type passed in {self.cfg.positional_embedding_type}\"\n",
        "            )\n",
        "        return residual, tokens, shortformer_pos_embed, attention_mask\n",
        "\n",
        "    @overload\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        return_type: Literal[\"logits\"],\n",
        "        loss_per_token: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        start_at_layer: Optional[int] = None,\n",
        "        tokens: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]\n",
        "        stop_at_layer: Optional[int] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> Loss:\n",
        "        ...\n",
        "\n",
        "    @overload\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        return_type: Literal[\"loss\"],\n",
        "        loss_per_token: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        start_at_layer: Optional[int] = None,\n",
        "        tokens: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]\n",
        "        stop_at_layer: Optional[int] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> Loss:\n",
        "        ...\n",
        "\n",
        "    @overload\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        return_type: Literal[\"both\"],\n",
        "        loss_per_token: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        start_at_layer: Optional[int] = None,\n",
        "        tokens: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]\n",
        "        stop_at_layer: Optional[int] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss]:\n",
        "        ...\n",
        "\n",
        "    @overload\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        return_type: Literal[None],\n",
        "        loss_per_token: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        start_at_layer: Optional[int] = None,\n",
        "        tokens: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]\n",
        "        stop_at_layer: Optional[int] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> None:\n",
        "        ...\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input: Union[\n",
        "            str,\n",
        "            List[str],\n",
        "            Int[torch.Tensor, \"batch pos\"],\n",
        "            Float[torch.Tensor, \"batch pos d_model\"],\n",
        "        ],\n",
        "        return_type: Optional[str] = \"logits\",\n",
        "        loss_per_token: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Literal[\"left\", \"right\"]] = USE_DEFAULT_VALUE,\n",
        "        start_at_layer: Optional[int] = None,\n",
        "        tokens: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,  # [batch pos]\n",
        "        stop_at_layer: Optional[int] = None,\n",
        "        past_kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n",
        "    ) -> Union[\n",
        "        None,\n",
        "        Float[torch.Tensor, \"batch pos d_vocab\"],\n",
        "        Loss,\n",
        "        Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n",
        "    ]:\n",
        "        \"\"\"Forward Pass.\n",
        "\n",
        "        Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically\n",
        "        tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a\n",
        "        text string.\n",
        "\n",
        "        Note that loss is the standard \"predict the next token\" cross-entropy loss for GPT-2 style\n",
        "        language models - if you want a custom loss function, the recommended behaviour is returning\n",
        "        the logits and then applying your custom loss function.\n",
        "\n",
        "        Args:\n",
        "            return_type Optional[str]: The type of output to return. Can be one of: None (return\n",
        "                nothing, don't calculate logits), 'logits' (return logits), 'loss' (return\n",
        "                cross-entropy loss), 'both' (return logits and loss).\n",
        "            loss_per_token bool: Whether to return the (next token prediction) loss per token (True)\n",
        "                or average (False). Average loss is a scalar (averaged over position *and* batch),\n",
        "                per-token loss is a tensor ([batch, position-1]) - position-1 because we're\n",
        "                predicting the next token, and there's no specified next token for the final token.\n",
        "                Defaults to False.\n",
        "            prepend_bos Optional[bool]: Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (only applies when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\n",
        "                otherwise. (Even for models not explicitly trained with a prepended BOS token, heads\n",
        "                often use the first position as a resting position and accordingly lose information\n",
        "                from the first token, so this empirically seems to give better results.) Pass True\n",
        "                or False to locally override the default.\n",
        "            padding_side Optional[Literal[\"left\", \"right\"]]: Overrides self.tokenizer.padding_side.\n",
        "                Specifies which side to pad on when tokenizing multiple strings of different\n",
        "                lengths.\n",
        "            start_at_layer Optional[int]: If not None, start the forward pass at the specified\n",
        "                layer. Requires input to be the residual stream before the specified layer with\n",
        "                shape [batch, pos, d_model]. Inclusive - ie, start_at_layer = 0 skips the embedding\n",
        "                then runs the rest of the model. Supports negative indexing. start_at_layer = -1\n",
        "                only runs the final block and the unembedding. Defaults to None (run the full\n",
        "                model).\n",
        "            tokens: Optional[Int[torch.Tensor, \"batch pos\"]]: Tokenized input. Only use if\n",
        "                start_at_layer is not None and return type is \"loss\" or \"both\".\n",
        "            shortformer_pos_embed: Optional[Float[torch.Tensor, \"batch pos d_model\"]]: Positional\n",
        "                embedding for shortformer models. Only use if start_at_layer is not None and\n",
        "                self.cfg.positional_embedding_type == \"shortformer\".\n",
        "            attention_mask: Optional[torch.Tensor]: Override the attention mask used to ignore\n",
        "                padded tokens. If start_at_layer is not None and (self.tokenizer.padding_side ==\n",
        "                \"left\" or past_kv_cache is not None), this should be passed as the attention mask\n",
        "                is not computed automatically. Defaults to None.\n",
        "            stop_at_layer Optional[int]: If not None, stop the forward pass at the specified layer.\n",
        "                Exclusive - ie, stop_at_layer = 0 will only run the embedding layer, stop_at_layer =\n",
        "                1 will run the embedding layer and the first transformer block, etc. Supports\n",
        "                negative indexing. Useful for analysis of intermediate layers, eg finding neuron\n",
        "                activations in layer 3 of a 24 layer model. Defaults to None (run the full model).\n",
        "                If not None, we return the last residual stream computed.\n",
        "            past_kv_cache Optional[HookedTransformerKeyValueCache]: If not None, keys and values\n",
        "                will be stored for every attention head (unless the cache is frozen). If there are\n",
        "                keys and values already in the cache, these will be prepended to the keys and values\n",
        "                for the new input, so that the new tokens can pay attention to previous tokens. This\n",
        "                is useful for generating text, because we don't need to repeat computation for\n",
        "                tokens that have already been through the model. Also caches attention_mask so\n",
        "                previous tokens are masked correctly (unless frozen). Padding should be ignored in\n",
        "                all cases, so it's okay to eg. pass in left padded tokens twice in a row.\n",
        "                Warning: Don't accidentally prepend_bos to the second half of a prompt.\n",
        "                Defaults to None (don't use caching).\n",
        "        \"\"\"\n",
        "\n",
        "        with utils.LocallyOverridenDefaults(\n",
        "            self, prepend_bos=prepend_bos, padding_side=padding_side\n",
        "        ):\n",
        "            if start_at_layer is None:\n",
        "                (\n",
        "                    residual,\n",
        "                    tokens,\n",
        "                    shortformer_pos_embed,\n",
        "                    attention_mask,\n",
        "                ) = self.input_to_embed(\n",
        "                    input,\n",
        "                    prepend_bos=prepend_bos,\n",
        "                    padding_side=padding_side,\n",
        "                    attention_mask=attention_mask,\n",
        "                    past_kv_cache=past_kv_cache,\n",
        "                )\n",
        "            else:\n",
        "                assert type(input) == torch.Tensor\n",
        "                residual = input\n",
        "\n",
        "            if start_at_layer is None:\n",
        "                start_at_layer = 0\n",
        "            # If we explicitly want to start or stop at a layer, we only iterate through the blocks\n",
        "            # between those indices. Note that start_at_layer is inclusive and stop_at_layer is\n",
        "            # exclusive.\n",
        "            # Eg: start_at_layer==None + stop_at_layer==0 means to only run the embed.\n",
        "            # Eg: start_at_layer==3 + stop_at_layer==-1 means to run from layer 3 until the end of the PENULTIMATE layer\n",
        "            blocks_and_idxs = list(zip(range(self.cfg.n_layers), self.blocks))\n",
        "            for i, block in blocks_and_idxs[start_at_layer:stop_at_layer]:  # type: ignore\n",
        "                # Note that each block includes skip connections, so we don't need\n",
        "                # residual + block(residual)\n",
        "                # If we're using multiple GPUs, we need to send the residual and shortformer_pos_embed to the correct GPU\n",
        "                residual = residual.to(devices.get_device_for_block_index(i, self.cfg))\n",
        "                if shortformer_pos_embed is not None:\n",
        "                    shortformer_pos_embed = shortformer_pos_embed.to(\n",
        "                        devices.get_device_for_block_index(i, self.cfg)\n",
        "                    )\n",
        "\n",
        "                residual = block(\n",
        "                    residual,\n",
        "                    # Cache contains a list of HookedTransformerKeyValueCache objects, one for each\n",
        "                    # block\n",
        "                    past_kv_cache_entry=past_kv_cache[i] if past_kv_cache is not None else None,\n",
        "                    shortformer_pos_embed=shortformer_pos_embed,\n",
        "                    attention_mask=attention_mask,\n",
        "                )  # [batch, pos, d_model]\n",
        "\n",
        "            if stop_at_layer is not None:\n",
        "                # When we stop at an early layer, we end here rather than doing further computation\n",
        "                return residual\n",
        "\n",
        "            if self.cfg.normalization_type is not None:\n",
        "                residual = self.ln_final(residual)  # [batch, pos, d_model]\n",
        "            if return_type is None:\n",
        "                return None\n",
        "            else:\n",
        "                logits = self.unembed(residual)  # [batch, pos, d_vocab]\n",
        "                if self.cfg.output_logits_soft_cap > 0.0:\n",
        "                    logits = self.cfg.output_logits_soft_cap * F.tanh(\n",
        "                        logits / self.cfg.output_logits_soft_cap\n",
        "                    )\n",
        "                if return_type == \"logits\":\n",
        "                    return logits\n",
        "                else:\n",
        "                    assert (\n",
        "                        tokens is not None\n",
        "                    ), \"tokens must be passed in if return_type is 'loss' or 'both'\"\n",
        "                    loss = self.loss_fn(logits, tokens, attention_mask, per_token=loss_per_token)\n",
        "                    if return_type == \"loss\":\n",
        "                        return loss\n",
        "                    elif return_type == \"both\":\n",
        "                        return Output(logits, loss)\n",
        "                    else:\n",
        "                        logging.warning(f\"Invalid return_type passed in: {return_type}\")\n",
        "                        return None\n",
        "\n",
        "    def loss_fn(\n",
        "        self,\n",
        "        logits: Float[torch.Tensor, \"batch pos d_vocab\"],\n",
        "        tokens: Int[torch.Tensor, \"batch pos\"],\n",
        "        attention_mask: Optional[Int[torch.Tensor, \"batch pos\"]] = None,\n",
        "        per_token: bool = False,\n",
        "    ):\n",
        "        \"\"\"Wrapper around `utils.lm_cross_entropy_loss`.\n",
        "\n",
        "        Used in forward() with return_type==\"loss\" or \"both\".\n",
        "        \"\"\"\n",
        "        if tokens.device != logits.device:\n",
        "            tokens = tokens.to(logits.device)\n",
        "        return utils.lm_cross_entropy_loss(logits, tokens, attention_mask, per_token)\n",
        "\n",
        "    @overload\n",
        "    def run_with_cache(\n",
        "        self, *model_args, return_cache_object: Literal[True] = True, **kwargs\n",
        "    ) -> Tuple[Output, ActivationCache]:\n",
        "        ...\n",
        "\n",
        "    @overload\n",
        "    def run_with_cache(\n",
        "        self, *model_args, return_cache_object: Literal[False], **kwargs\n",
        "    ) -> Tuple[Output, Dict[str, torch.Tensor]]:\n",
        "        ...\n",
        "\n",
        "    def run_with_cache(\n",
        "        self, *model_args, return_cache_object=True, remove_batch_dim=False, **kwargs\n",
        "    ) -> Tuple[\n",
        "        Union[\n",
        "            None,\n",
        "            Float[torch.Tensor, \"batch pos d_vocab\"],\n",
        "            Loss,\n",
        "            Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n",
        "        ],\n",
        "        Union[ActivationCache, Dict[str, torch.Tensor]],\n",
        "    ]:\n",
        "        \"\"\"Wrapper around `run_with_cache` in HookedRootModule.\n",
        "\n",
        "        If return_cache_object is True, this will return an ActivationCache object, with a bunch of\n",
        "        useful HookedTransformer specific methods, otherwise it will return a dictionary of\n",
        "        activations as in HookedRootModule.\n",
        "        \"\"\"\n",
        "        out, cache_dict = super().run_with_cache(\n",
        "            *model_args, remove_batch_dim=remove_batch_dim, **kwargs\n",
        "        )\n",
        "        if return_cache_object:\n",
        "            cache = ActivationCache(cache_dict, self, has_batch_dim=not remove_batch_dim)\n",
        "            return out, cache\n",
        "        else:\n",
        "            return out, cache_dict\n",
        "\n",
        "    def set_tokenizer(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        default_padding_side=\"right\",\n",
        "    ):\n",
        "        \"\"\"Set the tokenizer to use for this model.\n",
        "\n",
        "        Args:\n",
        "            tokenizer (PreTrainedTokenizer): a pretrained HuggingFace tokenizer.\n",
        "            default_padding_side (str): \"right\" or \"left\", which side to pad on.\n",
        "\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            tokenizer, PreTrainedTokenizerBase\n",
        "        ), f\"{type(tokenizer)} is not a supported tokenizer, please use PreTrainedTokenizer or PreTrainedTokenizerFast\"\n",
        "\n",
        "        assert default_padding_side in [\n",
        "            \"right\",\n",
        "            \"left\",\n",
        "        ], f\"padding_side must be 'right' or 'left', got {default_padding_side}\"\n",
        "\n",
        "        # Use a tokenizer that is initialized with add_bos_token=True as the default tokenizer.\n",
        "        # Such a tokenizer should be set as the default tokenizer because the tokenization of some\n",
        "        # tokenizers like LlamaTokenizer are different when bos token is automatically/manually\n",
        "        # prepended, and add_bos_token cannot be dynamically controlled after initialization\n",
        "        # (https://github.com/huggingface/transformers/issues/25886).\n",
        "        tokenizer_with_bos = utils.get_tokenizer_with_bos(tokenizer)\n",
        "        self.tokenizer = tokenizer_with_bos\n",
        "        assert self.tokenizer is not None  # keep mypy happy\n",
        "        self.tokenizer.padding_side = default_padding_side\n",
        "\n",
        "        # Some tokenizers doesn't automatically prepend the BOS token even when they are initialized\n",
        "        # with add_bos_token=True. Therefore, we need this information to dynamically control prepend_bos.\n",
        "        self.cfg.tokenizer_prepends_bos = len(self.tokenizer.encode(\"\")) > 0\n",
        "\n",
        "        if self.tokenizer.eos_token is None:\n",
        "            self.tokenizer.eos_token = \"<|endoftext|>\"\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        if self.tokenizer.bos_token is None:\n",
        "            self.tokenizer.bos_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Infer vocab size from tokenizer\n",
        "        if self.cfg.d_vocab == -1:\n",
        "            self.cfg.d_vocab = max(self.tokenizer.vocab.values()) + 1\n",
        "        if self.cfg.d_vocab_out == -1:\n",
        "            self.cfg.d_vocab_out = self.cfg.d_vocab\n",
        "\n",
        "    def to_tokens(\n",
        "        self,\n",
        "        input: Union[str, List[str]],\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "        move_to_device: bool = True,\n",
        "        truncate: bool = True,\n",
        "    ) -> Int[torch.Tensor, \"batch pos\"]:\n",
        "        \"\"\"Converts a string to a tensor of tokens.\n",
        "\n",
        "        If prepend_bos is True, prepends the BOS token to the input - this is recommended when\n",
        "        creating a sequence of tokens to be input to a model.\n",
        "\n",
        "        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when\n",
        "        inputting a prompt to the model as the first token is often treated weirdly, but should only\n",
        "        be done at the START of the prompt. Make sure to turn it off if you're looking at the\n",
        "        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS\n",
        "        token, others (OPT and my models) were)\n",
        "\n",
        "        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether\n",
        "        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not\n",
        "        careful!\n",
        "\n",
        "        Args:\n",
        "            input (Union[str, List[str]]): The input to tokenize.\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (only applies when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\n",
        "                otherwise. Pass True or False to locally override the default.\n",
        "            padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing\n",
        "                multiple strings of different lengths.\n",
        "            move_to_device (bool): Whether to move the output tensor of tokens to the device the\n",
        "                model lives on. Defaults to True truncate (bool): If the output tokens are too long,\n",
        "                whether to truncate the output tokens to the model's max context window. Does nothing\n",
        "                for shorter inputs. Defaults to True.\n",
        "        \"\"\"\n",
        "        with utils.LocallyOverridenDefaults(\n",
        "            self, prepend_bos=prepend_bos, padding_side=padding_side\n",
        "        ):\n",
        "            assert self.tokenizer is not None, \"Cannot use to_tokens without a tokenizer\"\n",
        "            assert (\n",
        "                self.cfg.tokenizer_prepends_bos is not None\n",
        "            ), \"Set the tokenizer for the model by calling set_tokenizer\"\n",
        "\n",
        "            if self.cfg.default_prepend_bos and not self.cfg.tokenizer_prepends_bos:\n",
        "                # We want to prepend bos but the tokenizer doesn't automatically do it, so we add it manually\n",
        "                input = utils.get_input_with_manually_prepended_bos(self.tokenizer, input)\n",
        "\n",
        "            tokens = self.tokenizer(\n",
        "                input,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=truncate,\n",
        "                max_length=self.cfg.n_ctx if truncate else None,\n",
        "            )[\"input_ids\"]\n",
        "\n",
        "            if not self.cfg.default_prepend_bos and self.cfg.tokenizer_prepends_bos:\n",
        "                # We don't want to prepend bos but the tokenizer does it automatically, so we remove it manually\n",
        "                tokens = utils.get_tokens_with_bos_removed(self.tokenizer, tokens)\n",
        "\n",
        "            if move_to_device:\n",
        "                tokens = tokens.to(self.cfg.device)\n",
        "            return tokens\n",
        "\n",
        "    def to_string(\n",
        "        self,\n",
        "        tokens: Union[\n",
        "            List[int],\n",
        "            Int[torch.Tensor, \"\"],\n",
        "            Int[torch.Tensor, \"batch pos\"],\n",
        "            Int[torch.Tensor, \"pos\"],\n",
        "            np.ndarray,\n",
        "            List[Int[torch.Tensor, \"pos\"]],\n",
        "        ],\n",
        "    ) -> Union[str, List[str]]:\n",
        "        \"\"\"Tokens to String(s).\n",
        "\n",
        "        Converts a tensor of tokens to a string (if rank 1) or a list of strings (if rank 2).\n",
        "\n",
        "        Accepts lists of tokens and numpy arrays as inputs too (and converts to tensors internally)\n",
        "        \"\"\"\n",
        "        assert self.tokenizer is not None, \"Cannot use to_string without a tokenizer\"\n",
        "\n",
        "        if not isinstance(tokens, torch.Tensor):\n",
        "            # We allow lists to be input\n",
        "            tokens = torch.tensor(tokens)\n",
        "\n",
        "        # I'm not sure what exactly clean_up_tokenization_spaces does, but if\n",
        "        # it's set, then tokenization is no longer invertible, and some tokens\n",
        "        # with a bunch of whitespace get collapsed together\n",
        "        if len(tokens.shape) == 2:\n",
        "            return self.tokenizer.batch_decode(tokens, clean_up_tokenization_spaces=False)\n",
        "        elif len(tokens.shape) <= 1:\n",
        "            return self.tokenizer.decode(tokens, clean_up_tokenization_spaces=False)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid shape passed in: {tokens.shape}\")\n",
        "\n",
        "    def to_str_tokens(\n",
        "        self,\n",
        "        input: Union[\n",
        "            str,\n",
        "            Int[torch.Tensor, \"pos\"],\n",
        "            Int[torch.Tensor, \"1 pos\"],\n",
        "            Int[np.ndarray, \"pos\"],\n",
        "            Int[np.ndarray, \"1 pos\"],\n",
        "            list,\n",
        "        ],\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "    ) -> Union[List[str], List[List[str]]]:\n",
        "        \"\"\"Map text, a list of text or tokens to a list of tokens as strings.\n",
        "\n",
        "        Gotcha: prepend_bos prepends a beginning of string token. This is a recommended default when\n",
        "        inputting a prompt to the model as the first token is often treated weirdly, but should only\n",
        "        be done at the START of the prompt. If prepend_bos=None is passed, it implies the usage of\n",
        "        self.cfg.default_prepend_bos which is set to True unless specified otherwise. Therefore,\n",
        "        make sure to locally turn it off by passing prepend_bos=False if you're looking at the\n",
        "        tokenization of part of the prompt! (Note: some models eg GPT-2 were not trained with a BOS\n",
        "        token, others (OPT and my models) were)\n",
        "\n",
        "        Gotcha2: Tokenization of a string depends on whether there is a preceding space and whether\n",
        "        the first letter is capitalized. It's easy to shoot yourself in the foot here if you're not\n",
        "        careful!\n",
        "\n",
        "        Gotcha3: If passing a string that exceeds the model's context length (model.cfg.n_ctx), it\n",
        "        will be truncated.\n",
        "\n",
        "        Args:\n",
        "            input (Union[str, list, torch.Tensor]): The input - either a string or a tensor of\n",
        "                tokens. If tokens, should be a tensor of shape [pos] or [1, pos].\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (only applies when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\n",
        "                otherwise. Pass True or False to locally override the default.\n",
        "            padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple\n",
        "                strings of different lengths.\n",
        "\n",
        "        Returns:\n",
        "            str_tokens: List of individual tokens as strings\n",
        "        \"\"\"\n",
        "        with utils.LocallyOverridenDefaults(\n",
        "            self, prepend_bos=prepend_bos, padding_side=padding_side\n",
        "        ):\n",
        "            assert self.tokenizer is not None  # keep mypy happy\n",
        "            tokens: Union[np.ndarray, torch.Tensor]\n",
        "            if isinstance(input, list):\n",
        "                return list(\n",
        "                    map(\n",
        "                        lambda tokens: self.to_str_tokens(tokens, prepend_bos, padding_side),\n",
        "                        input,\n",
        "                    )\n",
        "                )  # type: ignore\n",
        "            elif isinstance(input, str):\n",
        "                tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)[\n",
        "                    0\n",
        "                ]\n",
        "                # Gemma tokenizer expects a batch dimension\n",
        "                if \"gemma\" in self.tokenizer.name_or_path and tokens.ndim == 1:\n",
        "                    tokens = tokens.unsqueeze(1)\n",
        "            elif isinstance(input, torch.Tensor):\n",
        "                tokens = input\n",
        "                tokens = tokens.squeeze()  # Get rid of a trivial batch dimension\n",
        "                if tokens.dim() == 0:\n",
        "                    # Don't pass dimensionless tensor\n",
        "                    tokens = tokens.unsqueeze(0)\n",
        "                assert (\n",
        "                    tokens.dim() == 1\n",
        "                ), f\"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}\"\n",
        "            elif isinstance(input, np.ndarray):\n",
        "                tokens = input\n",
        "                tokens = tokens.squeeze()  # Get rid of a trivial batch dimension\n",
        "                if tokens.ndim == 0:\n",
        "                    # Don't pass dimensionless tensor\n",
        "                    tokens = np.expand_dims(tokens, axis=0)\n",
        "                assert (\n",
        "                    tokens.ndim == 1\n",
        "                ), f\"Invalid tokens input to to_str_tokens, has shape: {tokens.shape}\"\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid input type to to_str_tokens: {type(input)}\")\n",
        "            str_tokens = self.tokenizer.batch_decode(tokens, clean_up_tokenization_spaces=False)\n",
        "            return str_tokens\n",
        "\n",
        "    def to_single_token(self, string):\n",
        "        \"\"\"Map a string that makes up a single token to the id for that token.\n",
        "\n",
        "        Raises an error for strings that are not a single token! If uncertain use to_tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # We use the to_tokens method, do not append a BOS token\n",
        "        token = self.to_tokens(string, prepend_bos=False).squeeze()\n",
        "        # If token shape is non-empty, raise error\n",
        "        assert not token.shape, f\"Input string: {string} is not a single token!\"\n",
        "        return token.item()\n",
        "\n",
        "    def to_single_str_token(self, int_token: int) -> str:\n",
        "        # Gives the single token corresponding to an int in string form\n",
        "        assert isinstance(int_token, int)\n",
        "        token = self.to_str_tokens(torch.tensor([int_token]))\n",
        "        assert len(token) == 1\n",
        "        return cast(str, token[0])\n",
        "\n",
        "    def get_token_position(\n",
        "        self,\n",
        "        single_token: Union[str, int],\n",
        "        input: Union[str, Union[Float[torch.Tensor, \"pos\"], Float[torch.Tensor, \"1 pos\"]]],\n",
        "        mode=\"first\",\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Union[Literal[\"left\", \"right\"], None]] = USE_DEFAULT_VALUE,\n",
        "    ):\n",
        "        \"\"\"Get the position of a single_token in a string or sequence of tokens.\n",
        "\n",
        "        Raises an error if the token is not present.\n",
        "\n",
        "        Gotcha: If you're inputting a string, it'll automatically be tokenized. Be careful about the\n",
        "        setting for prepend_bos! When a string is input to the model, a BOS (beginning of sequence)\n",
        "        token is prepended by default when the string is tokenized because\n",
        "        self.cfg.default_prepend_bos is set to True unless specified otherwise. But this should only\n",
        "        be done at the START of the input, not when inputting part of the prompt. If you're getting\n",
        "        weird off-by-one errors, check carefully for what the setting should be!\n",
        "\n",
        "        Args:\n",
        "            single_token (Union[str, int]): The token to search for. Can\n",
        "                be a token index, or a string (but the string must correspond to a single token).\n",
        "            input (Union[str, torch.Tensor]): The sequence to\n",
        "                search in. Can be a string or a rank 1 tensor of tokens or a rank 2 tensor of tokens\n",
        "                with a dummy batch dimension.\n",
        "            mode (str, optional): If there are multiple matches, which match to return. Supports\n",
        "                \"first\" or \"last\". Defaults to \"first\".\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (only applies when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos which is set to True unless specified\n",
        "                otherwise. Pass True or False to locally override the default.\n",
        "            padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple\n",
        "                strings of different lengths.\n",
        "        \"\"\"\n",
        "        if isinstance(input, str):\n",
        "            # If the input is a string, convert to tensor\n",
        "            tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)\n",
        "        else:\n",
        "            tokens = input\n",
        "\n",
        "        if len(tokens.shape) == 2:\n",
        "            # If the tokens have shape [1, seq_len], flatten to [seq_len]\n",
        "            assert (\n",
        "                tokens.shape[0] == 1\n",
        "            ), f\"If tokens are rank two, they must have shape [1, seq_len], not {tokens.shape}\"\n",
        "            tokens = tokens[0]\n",
        "\n",
        "        if isinstance(single_token, str):\n",
        "            # If the single token is a string, convert to an integer\n",
        "            single_token = self.to_single_token(single_token)\n",
        "        elif isinstance(single_token, torch.Tensor):\n",
        "            single_token = single_token.item()\n",
        "\n",
        "        indices = torch.arange(len(tokens), device=tokens.device)[tokens == single_token]\n",
        "        assert len(indices) > 0, \"The token does not occur in the prompt\"\n",
        "        if mode == \"first\":\n",
        "            return indices[0].item()\n",
        "        elif mode == \"last\":\n",
        "            return indices[-1].item()\n",
        "        else:\n",
        "            raise ValueError(f\"mode must be 'first' or 'last', not {mode}\")\n",
        "\n",
        "    def tokens_to_residual_directions(\n",
        "        self,\n",
        "        tokens: Union[\n",
        "            str,\n",
        "            int,\n",
        "            Int[torch.Tensor, \"\"],\n",
        "            Int[torch.Tensor, \"pos\"],\n",
        "            Int[torch.Tensor, \"batch pos\"],\n",
        "        ],\n",
        "    ) -> Union[\n",
        "        Float[torch.Tensor, \"d_model\"],\n",
        "        Float[torch.Tensor, \"pos d_model\"],\n",
        "        Float[torch.Tensor, \"batch pos d_model\"],\n",
        "    ]:\n",
        "        \"\"\"Map tokens to a tensor with the unembedding vector for those tokens.\n",
        "\n",
        "        I.e. the vector in the residual stream that we dot with to the get the logit for that token.\n",
        "\n",
        "        WARNING: If you use this without folding in LayerNorm, the results will be misleading and\n",
        "        may be incorrect, as the LN weights change the unembed map. This is done automatically with\n",
        "        the fold_ln flag on from_pretrained\n",
        "\n",
        "        WARNING 2: LayerNorm scaling will scale up or down the effective direction in the residual\n",
        "        stream for each output token on any given input token position.\n",
        "        ActivationCache.apply_ln_to_stack will apply the appropriate scaling to these directions.\n",
        "\n",
        "        Args:\n",
        "            tokens (Union[str, int, torch.Tensor]): The token(s). If a single token, can be a single\n",
        "                element tensor, an integer, or string. If string, will be mapped to a single token\n",
        "                using to_single_token, and an error raised if it's multiple tokens. The method also\n",
        "                works for a batch of input tokens.\n",
        "\n",
        "        Returns:\n",
        "            residual_direction torch.Tensor: The unembedding vector for the token(s), a stack of\n",
        "                [d_model] tensor.\n",
        "        \"\"\"\n",
        "        if isinstance(tokens, torch.Tensor) and tokens.numel() > 1:\n",
        "            # If the tokens are a tensor, and have more than one element, assume they are a batch of\n",
        "            # tokens.\n",
        "            residual_directions = self.W_U[:, tokens]\n",
        "            residual_directions = einops.rearrange(\n",
        "                residual_directions, \"d_model ... -> ... d_model\"\n",
        "            )\n",
        "            return residual_directions\n",
        "        else:\n",
        "            # Otherwise there is a single token\n",
        "            if isinstance(tokens, str):\n",
        "                token = self.to_single_token(tokens)\n",
        "            elif isinstance(tokens, int):\n",
        "                token = tokens\n",
        "            elif isinstance(tokens, torch.Tensor) and tokens.numel() == 1:\n",
        "                token = tokens.item()\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid token type: {type(tokens)}\")\n",
        "            residual_direction = self.W_U[:, token]\n",
        "            return residual_direction\n",
        "\n",
        "    def to(  # type: ignore\n",
        "        self,\n",
        "        device_or_dtype: Union[torch.device, str, torch.dtype],\n",
        "        print_details: bool = True,\n",
        "    ):\n",
        "        return devices.move_to_and_update_config(self, device_or_dtype, print_details)\n",
        "\n",
        "    def cuda(self):\n",
        "        \"\"\"Wrapper around cuda that also changes `self.cfg.device`.\"\"\"\n",
        "        return self.to(\"cuda\")\n",
        "\n",
        "    def cpu(self):\n",
        "        \"\"\"Wrapper around cuda that also changes `self.cfg.device`.\"\"\"\n",
        "        return self.to(\"cpu\")\n",
        "\n",
        "    def mps(self):\n",
        "        \"\"\"Wrapper around mps that also changes `self.cfg.device`.\"\"\"\n",
        "        return self.to(\"mps\")\n",
        "\n",
        "    def move_model_modules_to_device(self):\n",
        "        self.embed.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "        self.hook_embed.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "        if self.cfg.positional_embedding_type != \"rotary\":\n",
        "            self.pos_embed.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "            self.hook_pos_embed.to(devices.get_device_for_block_index(0, self.cfg))\n",
        "\n",
        "        if hasattr(self, \"ln_final\"):\n",
        "            self.ln_final.to(devices.get_device_for_block_index(self.cfg.n_layers - 1, self.cfg))\n",
        "        self.unembed.to(devices.get_device_for_block_index(self.cfg.n_layers - 1, self.cfg))\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            block.to(devices.get_device_for_block_index(i, self.cfg))\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        model_name: str,\n",
        "        fold_ln: bool = True,\n",
        "        center_writing_weights: bool = True,\n",
        "        center_unembed: bool = True,\n",
        "        refactor_factored_attn_matrices: bool = False,\n",
        "        checkpoint_index: Optional[int] = None,\n",
        "        checkpoint_value: Optional[int] = None,\n",
        "        hf_model: Optional[AutoModelForCausalLM] = None,\n",
        "        device: Optional[Union[str, torch.device]] = None,\n",
        "        n_devices: int = 1,\n",
        "        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
        "        move_to_device: bool = True,\n",
        "        fold_value_biases: bool = True,\n",
        "        default_prepend_bos: bool = True,\n",
        "        default_padding_side: Literal[\"left\", \"right\"] = \"right\",\n",
        "        dtype=\"float32\",\n",
        "        first_n_layers: Optional[int] = None,\n",
        "        **from_pretrained_kwargs,\n",
        "    ) -> \"HookedTransformer\":\n",
        "        \"\"\"Load in a Pretrained Model.\n",
        "\n",
        "        Load in pretrained model weights to the HookedTransformer format and optionally to do some\n",
        "        processing to make the model easier to interpret. Currently supports loading from most\n",
        "        autoregressive HuggingFace models (``gpt2``, ``neo``, ``gptj``, ``opt``...) and from a range\n",
        "        of toy models and SoLU models trained by Neel Nanda. The full list is available in the docs\n",
        "        under :doc:`model properties</generated/model_properties_table>`. Also supports loading from\n",
        "        a checkpoint for checkpointed models (currently, models trained by NeelNanda and the\n",
        "        stanford-crfm models (using parameters ``checkpoint_index`` and ``checkpoint_value``).\n",
        "\n",
        "        See :meth:`load_and_process_state_dict` for details on the processing (folding layer norm,\n",
        "        centering the unembedding and centering the writing weights).\n",
        "\n",
        "        Example:\n",
        "\n",
        "        >>> from transformer_lens import HookedTransformer\n",
        "        >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n",
        "        Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
        "\n",
        "        Args:\n",
        "            model_name: The model name - must be an element of\n",
        "                :const:`transformer_lens.loading_from_pretrained.OFFICIAL_MODEL_NAMES` or an alias\n",
        "                of one. The full list of available models can be found in the docs under :doc:`model\n",
        "                properties</generated/model_properties_table>`.\n",
        "            fold_ln: Whether to fold in the LayerNorm weights to the\n",
        "                subsequent linear layer. This does not change the computation.\n",
        "\n",
        "                `LayerNorm\n",
        "                <https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1>`_\n",
        "                is a common regularization technique used in transformers. Unlike BatchNorm, it\n",
        "                cannot be turned off at inference time, as it significantly alters the mathematical\n",
        "                function implemented by the transformer.\n",
        "\n",
        "                When `fold_ln` is set to True, LayerNorm (with weights :math:`w_{ln}` and\n",
        "                :math:`b_{ln}`) followed by a linear layer (:math:`W + b`) is optimized to\n",
        "                LayerNormPre (just centering & normalizing) followed by a new linear layer with\n",
        "                :math:`W_{eff} = w[:, \\text{None}] * W` (element-wise multiplication) and\n",
        "                :math:`b_{eff} = b + b_{ln} @ W`. This transformation is computationally equivalent\n",
        "                and simplifies the model's interpretability. It essentially merges LayerNorm weights\n",
        "                into the subsequent linear layer's weights, which is handled by HookedTransformer\n",
        "                when loading pre-trained weights. Set `fold_ln` to False when loading a state dict\n",
        "                if you wish to turn this off.\n",
        "\n",
        "                Mathematically, LayerNorm is defined as follows:\n",
        "\n",
        "                .. math::\n",
        "                    x_1 &= x_0 - \\\\text{mean}(x_0)\n",
        "\n",
        "                    x_2 &= \\\\frac{x_1}{\\\\sqrt{\\\\text{mean}(x_1^2)}}\n",
        "\n",
        "                    x_3 &= x_2 \\\\cdot w\n",
        "\n",
        "                    x_4 &= x_3 + b\n",
        "\n",
        "                For further details, refer to `this document\n",
        "                <https://transformer-circuits.pub/2021/framework/index.html#:~:text=Handling%20Layer%20Normalization>`_.\n",
        "            center_writing_weights: Whether to center weights\n",
        "                writing to the residual stream (ie set mean to be zero). Due to LayerNorm this\n",
        "                doesn't change the computation.\n",
        "\n",
        "                A related idea to folding layernorm (``fold_ln``) - *every* component reading an\n",
        "                input from the residual stream is preceded by a LayerNorm, which means that the mean\n",
        "                of a residual stream vector (ie the component in the direction of all ones) never\n",
        "                matters. This means we can remove the all ones component of weights and biases whose\n",
        "                output *writes* to the residual stream. Mathematically, ``W_writing -=\n",
        "                W_writing.mean(dim=1, keepdim=True)``.\n",
        "            center_unembed: Whether to center W_U (ie set mean\n",
        "                to be zero). Softmax is translation invariant so this doesn't affect log probs or\n",
        "                loss, but does change logits.\n",
        "\n",
        "                The logits are fed into a softmax. Softmax is translation invariant (eg, adding 1 to\n",
        "                every logit doesn't change the output), so we can simplify things by setting the\n",
        "                mean of the logits to be zero. This is equivalent to setting the mean of every\n",
        "                output vector of ``W_U`` to zero. In code, ``W_U -= W_U.mean(dim=-1,\n",
        "                keepdim=True)``.\n",
        "            refactor_factored_attn_matrices: Whether to convert the factored\n",
        "                matrices (W_Q & W_K, and W_O & W_V) to be \"even\". Defaults to False\n",
        "            checkpoint_index: If loading from a checkpoint, the index of\n",
        "                the checkpoint to load.\n",
        "            checkpoint_value: If loading from a checkpoint, the value of\n",
        "                the checkpoint to load, ie the step or token number (each model has checkpoints\n",
        "                labelled with exactly one of these). E.g. ``1000`` for a checkpoint taken at step\n",
        "                1000 or after 1000 tokens. If `checkpoint_index` is also specified, this will be\n",
        "                ignored.\n",
        "            hf_model: If you have already loaded in the\n",
        "                HuggingFace model, you can pass it in here rather than needing to recreate the\n",
        "                object. Defaults to None.\n",
        "            device: The device to load the model onto. By\n",
        "                default will load to CUDA if available, else CPU.\n",
        "            n_devices: The number of devices to split the model\n",
        "                across. Defaults to 1. If greater than 1, `device` must be cuda.\n",
        "            tokenizer: The tokenizer to use for the model. If not\n",
        "                provided, it is inferred from cfg.tokenizer_name or initialized to None. If None,\n",
        "                then the model cannot be passed strings, and d_vocab must be explicitly set.\n",
        "            move_to_device: Whether to move the model to the device specified in\n",
        "                cfg. device. Must be true if `n_devices` in the config is greater than 1, since the\n",
        "                model's layers will be split across multiple devices.\n",
        "            fold_value_biases: Each attention head has a value bias. Values are averaged to create\n",
        "                mixed values (``z``), weighted by the attention pattern, but as the bias is\n",
        "                constant, its contribution to ``z`` is exactly the same. The output of a head is ``z\n",
        "                @ W_O``, and so the value bias just linearly adds to the output of the head. This\n",
        "                means that the value bias of a head has nothing to do with the head, and is just a\n",
        "                constant added to the attention layer outputs. We can take the sum across these and\n",
        "                b_O to get an \"effective bias\" for the layer. In code, we set ``b_V=0``. and ``b_O =\n",
        "                (b_V @ W_O).sum(dim=0) + b_O``.\n",
        "\n",
        "                The technical derivation of this is as follows. ``v = residual @ W_V[h] +\n",
        "                broadcast_b_V[h]`` for each head ``h`` (where ``b_V`` is broadcast up from shape\n",
        "                ``d_head`` to shape ``[position, d_head]``). And ``z = pattern[h] @ v = pattern[h] @\n",
        "                residual @ W_V[h] + pattern[h] @ broadcast_b_V[h]``. Because ``pattern[h]`` is\n",
        "                ``[destination_position, source_position]`` and ``broadcast_b_V`` is constant along\n",
        "                the ``(source_)position`` dimension, we're basically just multiplying it by the sum\n",
        "                of the pattern across the ``source_position`` dimension, which is just ``1``. So it\n",
        "                remains exactly the same, and so is just broadcast across the destination positions.\n",
        "            default_prepend_bos: Default behavior of whether to prepend the BOS\n",
        "                token when the methods of HookedTransformer process input text to tokenize (only\n",
        "                when input is a string). Defaults to True - even for models not explicitly trained\n",
        "                with this, heads often use the first position as a resting position and accordingly\n",
        "                lose information from the first token, so this empirically seems to give better\n",
        "                results. To change the default behavior to False, pass in default_prepend_bos=False.\n",
        "                Note that you can also locally override the default behavior by passing in\n",
        "                prepend_bos=True/False when you call a method that processes the input string.\n",
        "            from_pretrained_kwargs: Any other optional argument passed to\n",
        "                HuggingFace's from_pretrained (e.g. \"cache_dir\" or \"torch_dtype\"). Also passed to\n",
        "                other HuggingFace functions when compatible. For some models or arguments it doesn't\n",
        "                work, especially for models that are not internally loaded with HuggingFace's\n",
        "                from_pretrained (e.g. SoLU models).\n",
        "            dtype: What data type to load the model in (also sets the dtype of\n",
        "                the HuggingFace model). Set to bfloat16 or float16 if you get out of memory errors when loading\n",
        "                the model.\n",
        "            default_padding_side: Which side to pad on when tokenizing. Defaults to\n",
        "                \"right\".\n",
        "            first_n_layers: If specified, only load the first n layers of the model.\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (\n",
        "            from_pretrained_kwargs.get(\"load_in_8bit\", False)\n",
        "            or from_pretrained_kwargs.get(\"load_in_4bit\", False)\n",
        "        ), \"Quantization not supported\"\n",
        "\n",
        "        if hf_model is not None:\n",
        "            hf_cfg = hf_model.config.to_dict()\n",
        "            qc = hf_cfg.get(\"quantization_config\", {})\n",
        "            load_in_4bit = qc.get(\"load_in_4bit\", False)\n",
        "            load_in_8bit = qc.get(\"load_in_8bit\", False)\n",
        "            quant_method = qc.get(\"quant_method\", \"\")\n",
        "            assert not load_in_8bit, \"8-bit quantization is not supported\"\n",
        "            assert not (\n",
        "                load_in_4bit and (version.parse(torch.__version__) < version.parse(\"2.1.1\"))\n",
        "            ), \"Quantization is only supported for torch versions >= 2.1.1\"\n",
        "            assert not (\n",
        "                load_in_4bit and (\"llama\" not in model_name.lower())\n",
        "            ), \"Quantization is only supported for Llama models\"\n",
        "            if load_in_4bit:\n",
        "                assert (\n",
        "                    qc.get(\"quant_method\", \"\") == \"bitsandbytes\"\n",
        "                ), \"Only bitsandbytes quantization is supported\"\n",
        "        else:\n",
        "            hf_cfg = {}\n",
        "\n",
        "        if isinstance(dtype, str):\n",
        "            # Convert from string to a torch dtype\n",
        "            dtype = DTYPE_FROM_STRING[dtype]\n",
        "        if \"torch_dtype\" in from_pretrained_kwargs:\n",
        "            # For backwards compatibility with the previous way to do low precision loading\n",
        "            # This should maybe check the user did not explicitly set dtype *and* torch_dtype\n",
        "            dtype = from_pretrained_kwargs[\"torch_dtype\"]\n",
        "\n",
        "        if (\n",
        "            (from_pretrained_kwargs.get(\"torch_dtype\", None) == torch.float16)\n",
        "            or dtype == torch.float16\n",
        "        ) and device in [\"cpu\", None]:\n",
        "            logging.warning(\"float16 models may not work on CPU. Consider using a GPU or bfloat16.\")\n",
        "\n",
        "        # Get the model name used in HuggingFace, rather than the alias.\n",
        "        official_model_name = loading.get_official_model_name(model_name)\n",
        "\n",
        "        # Load the config into an HookedTransformerConfig object. If loading from a\n",
        "        # checkpoint, the config object will contain the information about the\n",
        "        # checkpoint\n",
        "        cfg = loading.get_pretrained_model_config(\n",
        "            official_model_name,\n",
        "            hf_cfg=hf_cfg,\n",
        "            checkpoint_index=checkpoint_index,\n",
        "            checkpoint_value=checkpoint_value,\n",
        "            fold_ln=fold_ln,\n",
        "            device=device,\n",
        "            n_devices=n_devices,\n",
        "            default_prepend_bos=default_prepend_bos,\n",
        "            dtype=dtype,\n",
        "            first_n_layers=first_n_layers,\n",
        "            **from_pretrained_kwargs,\n",
        "        )\n",
        "\n",
        "        if cfg.positional_embedding_type == \"shortformer\":\n",
        "            if fold_ln:\n",
        "                logging.warning(\n",
        "                    \"You tried to specify fold_ln=True for a shortformer model, but this can't be done! Setting fold_\"\n",
        "                    \"ln=False instead.\"\n",
        "                )\n",
        "                fold_ln = False\n",
        "            if center_unembed:\n",
        "                logging.warning(\n",
        "                    \"You tried to specify center_unembed=True for a shortformer model, but this can't be done! \"\n",
        "                    \"Setting center_unembed=False instead.\"\n",
        "                )\n",
        "                center_unembed = False\n",
        "            if center_writing_weights:\n",
        "                logging.warning(\n",
        "                    \"You tried to specify center_writing_weights=True for a shortformer model, but this can't be done! \"\n",
        "                    \"Setting center_writing_weights=False instead.\"\n",
        "                )\n",
        "                center_writing_weights = False\n",
        "        if center_unembed and cfg.output_logits_soft_cap > 0.0:\n",
        "            logging.warning(\n",
        "                \"You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant\"\n",
        "                \"Setting center_unembed=False instead.\"\n",
        "            )\n",
        "            center_unembed = False\n",
        "\n",
        "        # Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\n",
        "        # match the HookedTransformer parameter names.\n",
        "        state_dict = loading.get_pretrained_state_dict(\n",
        "            official_model_name, cfg, hf_model, dtype=dtype, **from_pretrained_kwargs\n",
        "        )\n",
        "\n",
        "        # Create the HookedTransformer object\n",
        "        model = cls(\n",
        "            cfg,\n",
        "            tokenizer,\n",
        "            move_to_device=False,\n",
        "            default_padding_side=default_padding_side,\n",
        "        )\n",
        "\n",
        "        model.load_and_process_state_dict(\n",
        "            state_dict,\n",
        "            fold_ln=fold_ln,\n",
        "            center_writing_weights=center_writing_weights,\n",
        "            center_unembed=center_unembed,\n",
        "            fold_value_biases=fold_value_biases,\n",
        "            refactor_factored_attn_matrices=refactor_factored_attn_matrices,\n",
        "        )\n",
        "\n",
        "        if move_to_device:\n",
        "            model.move_model_modules_to_device()\n",
        "\n",
        "        print(f\"Loaded pretrained model {model_name} into HookedTransformer\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained_no_processing(\n",
        "        cls,\n",
        "        model_name: str,\n",
        "        fold_ln=False,\n",
        "        center_writing_weights=False,\n",
        "        center_unembed=False,\n",
        "        refactor_factored_attn_matrices=False,\n",
        "        fold_value_biases=False,\n",
        "        dtype=torch.float32,\n",
        "        default_prepend_bos=True,\n",
        "        default_padding_side=\"right\",\n",
        "        **from_pretrained_kwargs,\n",
        "    ):\n",
        "        \"\"\"Wrapper for from_pretrained.\n",
        "\n",
        "        Wrapper for from_pretrained with all boolean flags related to simplifying the model set to\n",
        "        False. Refer to from_pretrained for details.\n",
        "        \"\"\"\n",
        "        return cls.from_pretrained(\n",
        "            model_name,\n",
        "            fold_ln=fold_ln,\n",
        "            center_writing_weights=center_writing_weights,\n",
        "            center_unembed=center_unembed,\n",
        "            fold_value_biases=fold_value_biases,\n",
        "            refactor_factored_attn_matrices=refactor_factored_attn_matrices,\n",
        "            dtype=dtype,\n",
        "            default_prepend_bos=default_prepend_bos,\n",
        "            default_padding_side=default_padding_side,\n",
        "            **from_pretrained_kwargs,\n",
        "        )\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights.\n",
        "\n",
        "        LayerNorm weights are already initialized to 1.0, and all biases are initialized to 0.0\n",
        "        (including LayerNorm), so this just initializes weight matrices.\n",
        "\n",
        "        Weight matrices are set to empty by default (to save space + compute, since they're the bulk\n",
        "        of the parameters), so it is important to call this if you are not loading in pretrained\n",
        "        weights! Note that this function assumes that weight names being with `W_`.\n",
        "\n",
        "        Set seed here to ensure determinism.\n",
        "\n",
        "        This does NOT follow the PyTorch scheme, which as far as I can tell is super out of date but\n",
        "        no one has gotten round to updating it? https://github.com/pytorch/pytorch/issues/18182\n",
        "\n",
        "        The default PyTorch scheme is the following: all linear layers use uniform(-1/sqrt(fan_in),\n",
        "        1/sqrt(fan_in)) for weights, and uniform(-1/sqrt(fan_in), 1/sqrt(fan_in)) for biases. For\n",
        "        biases, fan_in is computed using the fan_in for the weight matrix of the linear layer. Note\n",
        "        tha it *does not actually* use Kaiming initialization, despite the fact that it calls the\n",
        "        function.\n",
        "\n",
        "        However, for Transformer blocks, it instead initializes biases to zero and weights using Xavier uniform, that\n",
        "        is: uniform(-sqrt(6 / (fan_in + fan_out)), sqrt(6 / (fan_in + fan_out))) for weights.\n",
        "\n",
        "        PyTorch Transformers are especially bad - TransformerEncoder initializes all layers to the\n",
        "        exact same weights?! https://github.com/pytorch/pytorch/issues/72253.\n",
        "\n",
        "        The best paper I've found on transformer initialization is the muP paper, but haven't\n",
        "        integrated those ideas yet: https://arxiv.org/abs/2203.03466\n",
        "\n",
        "        We split off the initialization into separate functions because muP initialization handles\n",
        "        different parts of the model differently.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.cfg.seed is not None:\n",
        "            torch.manual_seed(self.cfg.seed)\n",
        "\n",
        "        if self.cfg.init_mode == \"gpt2\":\n",
        "            self._init_weights_gpt2()\n",
        "        elif self.cfg.init_mode == \"xavier_uniform\":\n",
        "            self._init_weights_xavier(dist_type=\"uniform\")\n",
        "        elif self.cfg.init_mode == \"xavier_normal\":\n",
        "            self._init_weights_xavier(dist_type=\"normal\")\n",
        "        elif self.cfg.init_mode == \"kaiming_uniform\":\n",
        "            self._init_weights_kaiming(dist_type=\"uniform\")\n",
        "        elif self.cfg.init_mode == \"kaiming_normal\":\n",
        "            self._init_weights_kaiming(dist_type=\"normal\")\n",
        "        elif self.cfg.init_mode == \"muP\":\n",
        "            self._init_weights_muP(dist_type=\"normal\")  # muP uses normal initialization\n",
        "\n",
        "    def _init_weights_gpt2(self):\n",
        "        \"\"\"Initialize weights with GPT-2 initialization. Biases are initialized to 0.0 and weights\n",
        "        are initialized to N(0, 0.64/d_model) if initializer_range is not set, otherwise std is initializer_range.\n",
        "        \"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"W_\" in name:\n",
        "                nn.init.normal_(param, std=self.cfg.initializer_range)\n",
        "\n",
        "    def _init_weights_xavier(self, dist_type=\"normal\"):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier initialization -- that is, scale the weights by sqrt(6 /\n",
        "        (fan_in + fan_out)) for a [-1, 1] uniform distribution, or sqrt(2 / (fan_in + fan_out)) for a\n",
        "        standard normal.\n",
        "\n",
        "        Note that since TransformerLens implements the matrices in the opposite orientation to what\n",
        "        torch does (e.g. it's d_in x d_out, not d_out x d_in as in torch), we need to calculate it\n",
        "        ourselves.\n",
        "        \"\"\"\n",
        "        gain = self.cfg.initializer_range\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"W_\" in name:\n",
        "                if dist_type == \"uniform\":\n",
        "                    init_xavier_uniform_(param, gain=gain)\n",
        "                elif dist_type == \"normal\":\n",
        "                    init_xavier_normal_(param, gain=gain)\n",
        "\n",
        "    def _init_weights_kaiming(self, dist_type=\"uniform\"):\n",
        "        \"\"\"\n",
        "        Initialize weights with Kaiming initialization -- that is, scale the weights by\n",
        "        c / sqrt(fan_in), where c = sqrt(2) if the params were immediately preceded by a relu and 1 for\n",
        "        everything else.\n",
        "\n",
        "        Note that the numbers are actually incorrect here when you're using a nonlinearity other\n",
        "        than relu, e.g. the correct c for SiLu is ~1.74, for tanh it's 5/3 ~= 1.67, and for GeLU it's ~1.57.\n",
        "        But this is unlikely to matter in practice.\n",
        "\n",
        "        I'm just using fan_mode = \"fan_in\" for now, but it should be trivial to add fan_out.\n",
        "\n",
        "        Again, we have to implement it ourselves because of the orientation of the matrices.\n",
        "        \"\"\"\n",
        "        gain = self.cfg.initializer_range\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"W_\" in name:\n",
        "                if dist_type == \"uniform\":\n",
        "                    init_kaiming_uniform_(param, gain=gain, nonlinearity=\"relu\", mode=\"fan_in\")\n",
        "                elif dist_type == \"normal\":\n",
        "                    init_kaiming_normal_(param, gain=gain, nonlinearity=\"relu\", mode=\"fan_in\")\n",
        "\n",
        "    def _init_weights_muP(self, dist_type=\"uniform\"):\n",
        "        \"\"\"\n",
        "        Initialize weights with muParameterization. This involves scaling output weights by a factor\n",
        "        of 1/fan_in, input weights and biases by 1, everything else by a factor of 1/sqrt(fan_in).\n",
        "\n",
        "        Also, you need to use muAdamW, which rescales the learning rate for output weights and\n",
        "        hidden weights by a factor of 1/fan_in.\n",
        "\n",
        "        All biases are still assumed to be initialized to 0.0, so we only need to change the\n",
        "        weights.\n",
        "        \"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if \"W_\" in name:\n",
        "                fan_in, _ = utils.calc_fan_in_and_fan_out(param)\n",
        "                if \"embed\" in name:\n",
        "                    scale = float(1)\n",
        "                elif \"unembed\" in name:\n",
        "                    scale = 1 / fan_in\n",
        "                else:\n",
        "                    scale = 1 / fan_in**0.5\n",
        "\n",
        "                if dist_type == \"uniform\":\n",
        "                    scale *= 3**0.5\n",
        "                    nn.init.uniform_(param, -scale, scale)\n",
        "                elif dist_type == \"normal\":\n",
        "                    nn.init.normal_(param, std=scale)\n",
        "\n",
        "    def load_and_process_state_dict(\n",
        "        self,\n",
        "        state_dict: Dict[str, torch.Tensor],\n",
        "        fold_ln: bool = True,\n",
        "        center_writing_weights: bool = True,\n",
        "        center_unembed: bool = True,\n",
        "        fold_value_biases: bool = True,\n",
        "        refactor_factored_attn_matrices: bool = False,\n",
        "    ):\n",
        "        \"\"\"Load & Process State Dict.\n",
        "\n",
        "        Load a state dict into the model, and to apply processing to simplify it. The state dict is\n",
        "        assumed to be in the HookedTransformer format.\n",
        "\n",
        "        See the relevant method (same name as the flag) for more details on the folding, centering\n",
        "        and processing flags.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): The state dict of the model, in HookedTransformer format. fold_ln\n",
        "            fold_ln (bool, optional): Whether to fold in the LayerNorm weights to the\n",
        "                subsequent linear layer. This does not change the computation. Defaults to True.\n",
        "            center_writing_weights (bool, optional): Whether to center weights writing to the\n",
        "                residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the\n",
        "                computation. Defaults to True.\n",
        "            center_unembed (bool, optional): Whether to center W_U (ie set mean to be zero).\n",
        "                Softmax is translation invariant so this doesn't affect log probs or loss, but does\n",
        "                change logits. Defaults to True.\n",
        "            fold_value_biases (bool, optional): Whether to fold the value biases into the output\n",
        "                bias. Because attention patterns add up to 1, the value biases always have a\n",
        "                constant effect on a layer's output, and it doesn't matter which head a bias is\n",
        "                associated with. We can factor this all into a single output bias to the layer, and\n",
        "                make it easier to interpret the head's output.\n",
        "            refactor_factored_attn_matrices (bool, optional): Whether to convert the factored\n",
        "                matrices (W_Q & W_K, and W_O & W_V) to be \"even\". Defaults to False.\n",
        "            model_name (str, optional): checks the model name for special cases of state dict\n",
        "                loading. Only used for Redwood 2L model currently.\n",
        "        \"\"\"\n",
        "        if self.cfg.dtype not in [torch.float32, torch.float64] and fold_ln:\n",
        "            logging.warning(\n",
        "                \"With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\"\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            self.cfg.dtype not in [torch.float32, torch.float64]\n",
        "            and self.cfg.num_experts\n",
        "            and self.cfg.num_experts > 1\n",
        "        ):\n",
        "            logging.warning(\n",
        "                \"When running MoE models, it is advised to use a higher precision data type. See docs for more info.\"\n",
        "            )\n",
        "\n",
        "        state_dict = self.fill_missing_keys(state_dict)\n",
        "        if fold_ln:\n",
        "            if self.cfg.num_experts and self.cfg.num_experts > 1:\n",
        "                logging.warning(\n",
        "                    \"You are using MoE, so the layer norm weights can't be folded! Skipping\"\n",
        "                )\n",
        "            elif self.cfg.normalization_type in [\"LN\", \"LNPre\"]:\n",
        "                state_dict = self.fold_layer_norm(state_dict)\n",
        "            elif self.cfg.normalization_type in [\"RMS\", \"RMSPre\"]:\n",
        "                state_dict = self.fold_layer_norm(\n",
        "                    state_dict, fold_biases=False, center_weights=False\n",
        "                )\n",
        "            else:\n",
        "                logging.warning(\n",
        "                    \"You are not using LayerNorm or RMSNorm, so the layer norm weights can't be folded! Skipping\"\n",
        "                )\n",
        "\n",
        "        if center_writing_weights:\n",
        "            if self.cfg.normalization_type not in [\"LN\", \"LNPre\"]:\n",
        "                logging.warning(\n",
        "                    \"You are not using LayerNorm, so the writing weights can't be centered! Skipping\"\n",
        "                )\n",
        "            elif self.cfg.final_rms:\n",
        "                logging.warning(\n",
        "                    \"This model is using final RMS normalization, so the writing weights can't be centered! Skipping\"\n",
        "                )\n",
        "            else:\n",
        "                state_dict = self.center_writing_weights(state_dict)\n",
        "\n",
        "        if center_unembed:\n",
        "            state_dict = self.center_unembed(state_dict)\n",
        "        if fold_value_biases:\n",
        "            state_dict = self.fold_value_biases(state_dict)\n",
        "        if refactor_factored_attn_matrices:\n",
        "            state_dict = self.refactor_factored_attn_matrices(state_dict)\n",
        "\n",
        "        if self.cfg.load_in_4bit:\n",
        "            # with quantization, parameters should be assigned\n",
        "            # so that quantization settings are not lost\n",
        "            self.load_state_dict(state_dict, assign=True, strict=False)\n",
        "        else:\n",
        "            state_dict_keys = list(state_dict.keys())\n",
        "            for key in state_dict_keys:\n",
        "                self.load_state_dict({key: state_dict[key]}, strict=False)\n",
        "                del state_dict[key]\n",
        "\n",
        "    def fill_missing_keys(self, state_dict):\n",
        "        return loading.fill_missing_keys(self, state_dict)\n",
        "\n",
        "    def fold_layer_norm(\n",
        "        self, state_dict: Dict[str, torch.Tensor], fold_biases=True, center_weights=True\n",
        "    ):\n",
        "        \"\"\"Fold Layer Norm. Can also be used to fold RMS Norm, when fold_biases and center_weights are set to False.\n",
        "\n",
        "        Takes in a state dict from a pretrained model, formatted to be consistent with\n",
        "        HookedTransformer but with LayerNorm weights and biases. Folds these into the neighbouring\n",
        "        weights. See further_comments.md for more details.\n",
        "\n",
        "        Args:\n",
        "            state_dict (Dict[str, torch.Tensor]): State dict of pretrained model.\n",
        "            fold_biases (bool): Enables folding of LN biases. Should be disabled when RMS Norm is used.\n",
        "            center_weights (bool): Enables the centering of weights after folding in LN. Should be disabled when RMS Norm is used.\n",
        "        \"\"\"\n",
        "\n",
        "        # Models that use Grouped Query Attention (Only Mistral at the time of writing) prefix their K/V weights and\n",
        "        # biases with an underscore in order to distinguish them, but folding the LN into them still works the same,\n",
        "        # so we just add the underscore if GQA is used (i.e. if `cfg.n_key_value_heads is specified`).\n",
        "        gqa = \"\" if self.cfg.n_key_value_heads is None else \"_\"\n",
        "\n",
        "        for l in range(self.cfg.n_layers):\n",
        "            # Fold ln1 into attention - it's important to fold biases first, since biases depend on\n",
        "            # weights but not vice versa The various indexing is just to broadcast ln.b and ln.w\n",
        "            # along every axis other than d_model. Each weight matrix right multiplies. To fold in\n",
        "            # the bias, we use the W_ matrix to map it to the hidden space of the layer, so we need\n",
        "            # to sum along axis -2, which is the residual stream space axis.\n",
        "            if fold_biases:\n",
        "                state_dict[f\"blocks.{l}.attn.b_Q\"] = state_dict[f\"blocks.{l}.attn.b_Q\"] + (\n",
        "                    state_dict[f\"blocks.{l}.attn.W_Q\"]\n",
        "                    * state_dict[f\"blocks.{l}.ln1.b\"][None, :, None]\n",
        "                ).sum(-2)\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}b_K\"] = state_dict[\n",
        "                    f\"blocks.{l}.attn.{gqa}b_K\"\n",
        "                ] + (\n",
        "                    state_dict[f\"blocks.{l}.attn.{gqa}W_K\"]\n",
        "                    * state_dict[f\"blocks.{l}.ln1.b\"][None, :, None]\n",
        "                ).sum(\n",
        "                    -2\n",
        "                )\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}b_V\"] = state_dict[\n",
        "                    f\"blocks.{l}.attn.{gqa}b_V\"\n",
        "                ] + (\n",
        "                    state_dict[f\"blocks.{l}.attn.{gqa}W_V\"]\n",
        "                    * state_dict[f\"blocks.{l}.ln1.b\"][None, :, None]\n",
        "                ).sum(\n",
        "                    -2\n",
        "                )\n",
        "                del state_dict[f\"blocks.{l}.ln1.b\"]\n",
        "\n",
        "            state_dict[f\"blocks.{l}.attn.W_Q\"] = (\n",
        "                state_dict[f\"blocks.{l}.attn.W_Q\"] * state_dict[f\"blocks.{l}.ln1.w\"][None, :, None]\n",
        "            )\n",
        "            state_dict[f\"blocks.{l}.attn.{gqa}W_K\"] = (\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}W_K\"]\n",
        "                * state_dict[f\"blocks.{l}.ln1.w\"][None, :, None]\n",
        "            )\n",
        "            state_dict[f\"blocks.{l}.attn.{gqa}W_V\"] = (\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}W_V\"]\n",
        "                * state_dict[f\"blocks.{l}.ln1.w\"][None, :, None]\n",
        "            )\n",
        "            del state_dict[f\"blocks.{l}.ln1.w\"]\n",
        "\n",
        "            # Finally, we center the weights reading from the residual stream. The output of the\n",
        "            # first part of the LayerNorm is mean 0 and standard deviation 1, so the mean of any\n",
        "            # input vector of the matrix doesn't matter and can be set to zero. Equivalently, the\n",
        "            # output of LayerNormPre is orthogonal to the vector of all 1s (because dotting with\n",
        "            # that gets the sum), so we can remove the component of the matrix parallel to this.\n",
        "            if center_weights:\n",
        "                state_dict[f\"blocks.{l}.attn.W_Q\"] -= einops.reduce(\n",
        "                    state_dict[f\"blocks.{l}.attn.W_Q\"],\n",
        "                    \"head_index d_model d_head -> head_index 1 d_head\",\n",
        "                    \"mean\",\n",
        "                )\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}W_K\"] -= einops.reduce(\n",
        "                    state_dict[f\"blocks.{l}.attn.{gqa}W_K\"],\n",
        "                    \"head_index d_model d_head -> head_index 1 d_head\",\n",
        "                    \"mean\",\n",
        "                )\n",
        "                state_dict[f\"blocks.{l}.attn.{gqa}W_V\"] -= einops.reduce(\n",
        "                    state_dict[f\"blocks.{l}.attn.{gqa}W_V\"],\n",
        "                    \"head_index d_model d_head -> head_index 1 d_head\",\n",
        "                    \"mean\",\n",
        "                )\n",
        "\n",
        "            # Fold ln2 into MLP\n",
        "            if not self.cfg.attn_only:\n",
        "                if fold_biases:\n",
        "                    state_dict[f\"blocks.{l}.mlp.b_in\"] = state_dict[f\"blocks.{l}.mlp.b_in\"] + (\n",
        "                        state_dict[f\"blocks.{l}.mlp.W_in\"]\n",
        "                        * state_dict[f\"blocks.{l}.ln2.b\"][:, None]\n",
        "                    ).sum(-2)\n",
        "                    del state_dict[f\"blocks.{l}.ln2.b\"]\n",
        "\n",
        "                state_dict[f\"blocks.{l}.mlp.W_in\"] = (\n",
        "                    state_dict[f\"blocks.{l}.mlp.W_in\"] * state_dict[f\"blocks.{l}.ln2.w\"][:, None]\n",
        "                )\n",
        "\n",
        "                if self.cfg.gated_mlp:\n",
        "                    state_dict[f\"blocks.{l}.mlp.W_gate\"] = (\n",
        "                        state_dict[f\"blocks.{l}.mlp.W_gate\"]\n",
        "                        * state_dict[f\"blocks.{l}.ln2.w\"][:, None]\n",
        "                    )\n",
        "\n",
        "                del state_dict[f\"blocks.{l}.ln2.w\"]\n",
        "\n",
        "                if center_weights:\n",
        "                    # Center the weights that read in from the LayerNormPre\n",
        "                    state_dict[f\"blocks.{l}.mlp.W_in\"] -= einops.reduce(\n",
        "                        state_dict[f\"blocks.{l}.mlp.W_in\"],\n",
        "                        \"d_model d_mlp -> 1 d_mlp\",\n",
        "                        \"mean\",\n",
        "                    )\n",
        "\n",
        "                if self.cfg.act_fn is not None and self.cfg.act_fn.startswith(\"solu\"):\n",
        "                    # Fold ln3 into activation\n",
        "                    if fold_biases:\n",
        "                        state_dict[f\"blocks.{l}.mlp.b_out\"] = state_dict[\n",
        "                            f\"blocks.{l}.mlp.b_out\"\n",
        "                        ] + (\n",
        "                            state_dict[f\"blocks.{l}.mlp.W_out\"]\n",
        "                            * state_dict[f\"blocks.{l}.mlp.ln.b\"][:, None]\n",
        "                        ).sum(\n",
        "                            -2\n",
        "                        )\n",
        "\n",
        "                        del state_dict[f\"blocks.{l}.mlp.ln.b\"]\n",
        "\n",
        "                    state_dict[f\"blocks.{l}.mlp.W_out\"] = (\n",
        "                        state_dict[f\"blocks.{l}.mlp.W_out\"]\n",
        "                        * state_dict[f\"blocks.{l}.mlp.ln.w\"][:, None]\n",
        "                    )\n",
        "\n",
        "                    if center_weights:\n",
        "                        # Center the weights that read in from the LayerNormPre\n",
        "                        state_dict[f\"blocks.{l}.mlp.W_out\"] -= einops.reduce(\n",
        "                            state_dict[f\"blocks.{l}.mlp.W_out\"],\n",
        "                            \"d_mlp d_model -> 1 d_model\",\n",
        "                            \"mean\",\n",
        "                        )\n",
        "\n",
        "                    del state_dict[f\"blocks.{l}.mlp.ln.w\"]\n",
        "\n",
        "        # Fold ln_final into Unembed\n",
        "        if not self.cfg.final_rms and fold_biases:\n",
        "            # Dumb bug from my old SoLU training code, some models have RMSNorm instead of LayerNorm\n",
        "            # pre unembed.\n",
        "            state_dict[f\"unembed.b_U\"] = state_dict[f\"unembed.b_U\"] + (\n",
        "                state_dict[f\"unembed.W_U\"] * state_dict[f\"ln_final.b\"][:, None]\n",
        "            ).sum(dim=-2)\n",
        "            del state_dict[f\"ln_final.b\"]\n",
        "\n",
        "        state_dict[f\"unembed.W_U\"] = state_dict[f\"unembed.W_U\"] * state_dict[f\"ln_final.w\"][:, None]\n",
        "        del state_dict[f\"ln_final.w\"]\n",
        "\n",
        "        if center_weights:\n",
        "            # Center the weights that read in from the LayerNormPre\n",
        "            state_dict[f\"unembed.W_U\"] -= einops.reduce(\n",
        "                state_dict[f\"unembed.W_U\"], \"d_model d_vocab -> 1 d_vocab\", \"mean\"\n",
        "            )\n",
        "\n",
        "        return state_dict\n",
        "\n",
        "    def center_writing_weights(self, state_dict: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Center Writing Weights.\n",
        "\n",
        "        Centers the weights of the model that write to the residual stream - W_out, W_E, W_pos and\n",
        "        W_out. This is done by subtracting the mean of the weights from the weights themselves. This\n",
        "        is done in-place. See fold_layer_norm for more details.\n",
        "        \"\"\"\n",
        "        state_dict[\"embed.W_E\"] = state_dict[\"embed.W_E\"] - state_dict[\"embed.W_E\"].mean(\n",
        "            -1, keepdim=True\n",
        "        )\n",
        "        if self.cfg.positional_embedding_type != \"rotary\":\n",
        "            state_dict[\"pos_embed.W_pos\"] = state_dict[\"pos_embed.W_pos\"] - state_dict[\n",
        "                \"pos_embed.W_pos\"\n",
        "            ].mean(-1, keepdim=True)\n",
        "        for l in range(self.cfg.n_layers):\n",
        "            state_dict[f\"blocks.{l}.attn.W_O\"] = state_dict[f\"blocks.{l}.attn.W_O\"] - state_dict[\n",
        "                f\"blocks.{l}.attn.W_O\"\n",
        "            ].mean(\n",
        "                -1, keepdim=True\n",
        "            )  # W_O is [head_index, d_model, d_head]\n",
        "            state_dict[f\"blocks.{l}.attn.b_O\"] = (\n",
        "                state_dict[f\"blocks.{l}.attn.b_O\"] - state_dict[f\"blocks.{l}.attn.b_O\"].mean()\n",
        "            )  # b_O is [d_model]\n",
        "            if not self.cfg.attn_only:\n",
        "                state_dict[f\"blocks.{l}.mlp.W_out\"] = state_dict[\n",
        "                    f\"blocks.{l}.mlp.W_out\"\n",
        "                ] - state_dict[f\"blocks.{l}.mlp.W_out\"].mean(-1, keepdim=True)\n",
        "                state_dict[f\"blocks.{l}.mlp.b_out\"] = (\n",
        "                    state_dict[f\"blocks.{l}.mlp.b_out\"] - state_dict[f\"blocks.{l}.mlp.b_out\"].mean()\n",
        "                )\n",
        "        return state_dict\n",
        "\n",
        "    def center_unembed(self, state_dict: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Center the unembedding weights W_U.\n",
        "\n",
        "        This is done by subtracting the mean of the weights from the weights themselves. This is\n",
        "        done in-place. As softmax is translation invariant, this changes the logits but not the log\n",
        "        probs, and makes the model logits (slightly) more interpretable - when trying to understand\n",
        "        how components contribute to the logits, we'll be less misled by components that just add\n",
        "        something to every logit.\n",
        "        \"\"\"\n",
        "        state_dict[\"unembed.W_U\"] = state_dict[\"unembed.W_U\"] - state_dict[\"unembed.W_U\"].mean(\n",
        "            -1, keepdim=True\n",
        "        )\n",
        "        state_dict[\"unembed.b_U\"] = state_dict[\"unembed.b_U\"] - state_dict[\"unembed.b_U\"].mean()\n",
        "        return state_dict\n",
        "\n",
        "    def fold_value_biases(self, state_dict: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Fold the value biases into the output bias.\n",
        "\n",
        "        Because attention patterns add up to 1, the value biases always have a constant effect on a\n",
        "        head's output. Further, as the outputs of each head in a layer add together, each head's\n",
        "        value bias has a constant effect on the *layer's* output, which can make it harder to\n",
        "        interpret the effect of any given head, and it doesn't matter which head a bias is\n",
        "        associated with. We can factor this all into a single output bias to the layer, and make it\n",
        "        easier to interpret the head's output. Formally, we take b_O_new = b_O_original +\n",
        "        sum_head(b_V_head @ W_O_head).\n",
        "        \"\"\"\n",
        "        for layer in range(self.cfg.n_layers):\n",
        "            # shape [head_index, d_head]\n",
        "            if self.cfg.n_key_value_heads is None:\n",
        "                b_V = state_dict[f\"blocks.{layer}.attn.b_V\"]\n",
        "            else:\n",
        "                b_V = state_dict[f\"blocks.{layer}.attn._b_V\"]\n",
        "                b_V = torch.repeat_interleave(\n",
        "                    b_V, dim=0, repeats=self.cfg.n_heads // self.cfg.n_key_value_heads\n",
        "                )\n",
        "            # [head_index, d_head, d_model]\n",
        "            W_O = state_dict[f\"blocks.{layer}.attn.W_O\"]\n",
        "            # [d_model]\n",
        "            b_O_original = state_dict[f\"blocks.{layer}.attn.b_O\"]\n",
        "            folded_b_O = b_O_original + (b_V[:, :, None] * W_O).sum([0, 1])\n",
        "\n",
        "            state_dict[f\"blocks.{layer}.attn.b_O\"] = folded_b_O\n",
        "            if self.cfg.n_key_value_heads is None:\n",
        "                state_dict[f\"blocks.{layer}.attn.b_V\"] = torch.zeros_like(b_V)\n",
        "            else:\n",
        "                state_dict[f\"blocks.{layer}.attn._b_V\"] = torch.zeros_like(\n",
        "                    state_dict[f\"blocks.{layer}.attn._b_V\"]\n",
        "                )\n",
        "        return state_dict\n",
        "\n",
        "    def refactor_factored_attn_matrices(self, state_dict: Dict[str, torch.Tensor]):\n",
        "        \"\"\"Experimental method for managing queries, keys and values.\n",
        "\n",
        "        As argued in [A Mathematical Framework for Transformer\n",
        "        Circuits](https://transformer-circuits.pub/2021/framework/index.html), queries, keys and\n",
        "        values are somewhat arbitrary intermediate terms when computing with the low rank factored\n",
        "        matrices W_QK = W_Q @ W_K.T and W_OV = W_V @ W_O, and these matrices are the only thing\n",
        "        determining head behaviour. But there are many ways to find a low rank factorization to a\n",
        "        given matrix, and hopefully some of these are more interpretable than others! This method is\n",
        "        one attempt, which makes all of the matrices have orthogonal rows or columns, W_O into a\n",
        "        rotation and W_Q and W_K having the nth column in each having the same norm. The formula is\n",
        "        $W_V = U @ S,W_O=Vh.T,W_Q=U@S.sqrt(),W_K=Vh@S.sqrt()$.\n",
        "\n",
        "        More details:\n",
        "\n",
        "        If W_OV = U @ S @ Vh.T in its singular value decomposition, (where S is in R^d_head not\n",
        "        R^d_model, as W_OV is low rank), W_OV = (U @ S) @ (Vh.T) is an equivalent low rank\n",
        "        factorisation, where rows/columns of each matrix are orthogonal! So setting $W_V=US$ and\n",
        "        $W_O=Vh.T$ works just as well. I *think* this is a more interpretable setup, because now\n",
        "        $W_O$ is just a rotation, and doesn't change the norm, so $z$ has the same norm as the\n",
        "        result of the head.\n",
        "\n",
        "        For $W_QK = W_Q @ W_K.T$ we use the refactor $W_Q = U @ S.sqrt()$ and $W_K = Vh @ S.sqrt()$,\n",
        "        which is also equivalent ($S==S.sqrt() @ S.sqrt()$ as $S$ is diagonal). Here we keep the\n",
        "        matrices as having the same norm, since there's not an obvious asymmetry between the keys\n",
        "        and queries.\n",
        "\n",
        "        Biases are more fiddly to deal with. For OV it's pretty easy - we just need (x @ W_V + b_V)\n",
        "        @ W_O + b_O to be preserved, so we can set b_V' = 0. and b_O' = b_V @ W_O + b_O (note that\n",
        "        b_V in R^{head_index x d_head} while b_O in R^{d_model}, so we need to sum b_V @ W_O along\n",
        "        the head_index dimension too).\n",
        "\n",
        "        For QK it's messy - we need to preserve the bilinear form of (x @ W_Q + b_Q) * (y @ W_K +\n",
        "        b_K), which is fairly messy. To deal with the biases, we concatenate them to W_Q and W_K to\n",
        "        simulate a d_model+1 dimensional input (whose final coordinate is always 1), do the SVD\n",
        "        factorization on this effective matrix, then separate out into final weights and biases.\n",
        "        \"\"\"\n",
        "\n",
        "        assert (\n",
        "            self.cfg.positional_embedding_type != \"rotary\"\n",
        "        ), \"You can't refactor the QK circuit when using rotary embeddings (as the QK matrix depends on the position of the query and key)\"\n",
        "\n",
        "        for l in range(self.cfg.n_layers):\n",
        "            # W_QK = W_Q @ W_K.T\n",
        "            # Concatenate biases to make a d_model+1 input dimension\n",
        "            W_Q_eff = torch.cat(\n",
        "                [\n",
        "                    state_dict[f\"blocks.{l}.attn.W_Q\"],\n",
        "                    state_dict[f\"blocks.{l}.attn.b_Q\"][:, None, :],\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "            W_K_eff = torch.cat(\n",
        "                [\n",
        "                    state_dict[f\"blocks.{l}.attn.W_K\"],\n",
        "                    state_dict[f\"blocks.{l}.attn.b_K\"][:, None, :],\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "            W_Q_eff_even, W_K_eff_even_T = (\n",
        "                FactoredMatrix(W_Q_eff, W_K_eff.transpose(-1, -2)).make_even().pair\n",
        "            )\n",
        "            W_K_eff_even = W_K_eff_even_T.transpose(-1, -2)\n",
        "\n",
        "            state_dict[f\"blocks.{l}.attn.W_Q\"] = W_Q_eff_even[:, :-1, :]\n",
        "            state_dict[f\"blocks.{l}.attn.b_Q\"] = W_Q_eff_even[:, -1, :]\n",
        "            state_dict[f\"blocks.{l}.attn.W_K\"] = W_K_eff_even[:, :-1, :]\n",
        "            state_dict[f\"blocks.{l}.attn.b_K\"] = W_K_eff_even[:, -1, :]\n",
        "\n",
        "            # W_OV = W_V @ W_O\n",
        "            W_V = state_dict[f\"blocks.{l}.attn.W_V\"]\n",
        "            W_O = state_dict[f\"blocks.{l}.attn.W_O\"]\n",
        "\n",
        "            # Factors the bias to be consistent.\n",
        "            b_V = state_dict[f\"blocks.{l}.attn.b_V\"]\n",
        "            b_O = state_dict[f\"blocks.{l}.attn.b_O\"]\n",
        "            effective_bias = b_O + einsum(\n",
        "                \"head_index d_head, head_index d_head d_model -> d_model\", b_V, W_O\n",
        "            )\n",
        "            state_dict[f\"blocks.{l}.attn.b_V\"] = torch.zeros_like(b_V)\n",
        "            state_dict[f\"blocks.{l}.attn.b_O\"] = effective_bias\n",
        "\n",
        "            # Helper class to efficiently deal with low rank factored matrices.\n",
        "            W_OV = FactoredMatrix(W_V, W_O)\n",
        "            U, S, Vh = W_OV.svd()\n",
        "            state_dict[f\"blocks.{l}.attn.W_V\"] = U @ S.diag_embed()\n",
        "            state_dict[f\"blocks.{l}.attn.W_O\"] = utils.transpose(Vh)\n",
        "\n",
        "        return state_dict\n",
        "\n",
        "    def set_use_attn_result(self, use_attn_result: bool):\n",
        "        \"\"\"Toggle whether to explicitly calculate and expose the result for each attention head.\n",
        "\n",
        "        Useful for interpretability but can easily burn through GPU memory.\n",
        "        \"\"\"\n",
        "        self.cfg.use_attn_result = use_attn_result\n",
        "\n",
        "    def set_use_split_qkv_input(self, use_split_qkv_input: bool):\n",
        "        \"\"\"\n",
        "        Toggles whether to allow editing of inputs to each attention head.\n",
        "        \"\"\"\n",
        "        self.cfg.use_split_qkv_input = use_split_qkv_input\n",
        "\n",
        "    def set_use_hook_mlp_in(self, use_hook_mlp_in: bool):\n",
        "        \"\"\"Toggles whether to allow storing and editing inputs to each MLP layer.\"\"\"\n",
        "\n",
        "        assert not self.cfg.attn_only, \"Can't use hook_mlp_in with attn_only model\"\n",
        "        self.cfg.use_hook_mlp_in = use_hook_mlp_in\n",
        "\n",
        "    def set_use_attn_in(self, use_attn_in: bool):\n",
        "        \"\"\"\n",
        "        Toggles whether to allow editing of inputs to each attention head.\n",
        "        \"\"\"\n",
        "        self.cfg.use_attn_in = use_attn_in\n",
        "\n",
        "    def set_ungroup_grouped_query_attention(self, ungroup_grouped_query_attention: bool):\n",
        "        \"\"\"\n",
        "        Toggles whether to ungroup the grouped key and value heads in models with grouped query attention (GQA).\n",
        "        \"\"\"\n",
        "        self.cfg.ungroup_grouped_query_attention = ungroup_grouped_query_attention\n",
        "\n",
        "    def process_weights_(\n",
        "        self,\n",
        "        fold_ln: bool = True,\n",
        "        center_writing_weights: bool = True,\n",
        "        center_unembed: bool = True,\n",
        "        refactor_factored_attn_matrices: bool = False,\n",
        "    ):\n",
        "        \"\"\"Wrapper around `load_and_process_state_dict`.\n",
        "\n",
        "        Wrapper around load_and_process_state_dict to allow for in-place processing of the weights.\n",
        "        This is useful if using HookedTransformer for training, if we then want to analyse a cleaner\n",
        "        version of the same model.\n",
        "        \"\"\"\n",
        "        state_dict = self.state_dict()\n",
        "        if fold_ln and self.cfg.num_experts and self.cfg.num_experts > 1:\n",
        "            # If we're using MoE, we don't fold the layer norm weights, so we don't need to do any preprocessing\n",
        "            # A warning is already issued in `load_and_process_state_dict`\n",
        "            pass\n",
        "        elif fold_ln and self.cfg.normalization_type == \"LN\":\n",
        "            # If we're folding the LN into the weights, we need to replace all the layernorm layers\n",
        "            # with LayerNormPres, which do not have learnable parameters. This is somewhat hacky,\n",
        "            # but it's the easiest way to do it.\n",
        "            self.cfg.normalization_type = \"LNPre\"\n",
        "            self.ln_final = LayerNormPre(self.cfg)\n",
        "            for layer in self.blocks:\n",
        "                layer.ln1 = LayerNormPre(self.cfg)\n",
        "                layer.ln2 = LayerNormPre(self.cfg)\n",
        "                if self.cfg.is_layer_norm_activation():\n",
        "                    layer.mlp.ln = LayerNormPre(self.cfg)\n",
        "        elif fold_ln and self.cfg.normalization_type == \"RMS\":\n",
        "            # We do the same for RMSNorm if used\n",
        "            self.cfg.normalization_type = \"RMSPre\"\n",
        "            self.ln_final = RMSNormPre(self.cfg)\n",
        "            for layer in self.blocks:\n",
        "                layer.ln1 = RMSNormPre(self.cfg)\n",
        "                layer.ln2 = RMSNormPre(self.cfg)\n",
        "                if self.cfg.is_layer_norm_activation():\n",
        "                    layer.mlp.ln = RMSNormPre(self.cfg)\n",
        "\n",
        "        self.load_and_process_state_dict(\n",
        "            state_dict,\n",
        "            fold_ln=fold_ln,\n",
        "            center_writing_weights=center_writing_weights,\n",
        "            center_unembed=center_unembed,\n",
        "            refactor_factored_attn_matrices=refactor_factored_attn_matrices,\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self,\n",
        "        input: Union[str, Float[torch.Tensor, \"batch pos\"]] = \"\",\n",
        "        max_new_tokens: int = 10,\n",
        "        stop_at_eos: bool = True,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        do_sample: bool = True,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[float] = None,\n",
        "        temperature: float = 1.0,\n",
        "        freq_penalty: float = 0.0,\n",
        "        use_past_kv_cache: bool = True,\n",
        "        prepend_bos: Optional[bool] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Literal[\"left\", \"right\"]] = USE_DEFAULT_VALUE,\n",
        "        return_type: Optional[str] = \"input\",\n",
        "        verbose: bool = True,\n",
        "    ) -> Union[Int[torch.Tensor, \"batch pos_plus_new_tokens\"], str]:\n",
        "        \"\"\"Sample Tokens from the Model.\n",
        "\n",
        "        Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.\n",
        "\n",
        "        To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish\n",
        "        (by producing an EOT token), we keep running the model on the entire batch, but throw away\n",
        "        the output for a finished sequence and just keep adding EOTs to pad.\n",
        "\n",
        "        This supports entering a single string, but not a list of strings - if the strings don't\n",
        "        tokenize to exactly the same length, this gets messy. If that functionality is needed,\n",
        "        convert them to a batch of tokens and input that instead.\n",
        "\n",
        "        Args:\n",
        "            input (Union[str, Int[torch.Tensor, \"batch pos\"])]): Either a batch of tokens ([batch,\n",
        "                pos]) or a text string (this will be converted to a batch of tokens with batch size\n",
        "                1).\n",
        "            max_new_tokens (int): Maximum number of tokens to generate.\n",
        "            stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token.\n",
        "            eos_token_id (Optional[Union[int, Sequence]]): The token ID to use for end\n",
        "                of sentence. If None, use the tokenizer's eos_token_id - required if using\n",
        "                stop_at_eos. It's also possible to provide a list of token IDs (not just the\n",
        "                eos_token_id), in which case the generation will stop when any of them are output\n",
        "                (useful e.g. for stable_lm).\n",
        "            do_sample (bool): If True, sample from the model's output distribution. Otherwise, use\n",
        "                greedy search (take the max logit each time).\n",
        "            top_k (int): Number of tokens to sample from. If None, sample from all tokens.\n",
        "            top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0,\n",
        "                we take the top tokens with cumulative probability >= top_p.\n",
        "            temperature (float): Temperature for sampling. Higher values will make the model more\n",
        "                random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is\n",
        "                sampling from a uniform distribution).\n",
        "            freq_penalty (float): Frequency penalty for sampling - how much to penalise previous\n",
        "                tokens. Higher values will make the model more random.\n",
        "            use_past_kv_cache (bool): If True, create and use cache to speed up generation.\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (applicable when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos (default is True unless specified\n",
        "                otherwise). Pass True or False to override the default.\n",
        "            padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple\n",
        "                strings of different lengths.\n",
        "            return_type (Optional[str]): The type of the output to return - either a string (str),\n",
        "                a tensor of tokens (tensor) or whatever the format of the input was (input).\n",
        "            verbose (bool): If True, show tqdm progress bars for generation.\n",
        "\n",
        "        Returns:\n",
        "            outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens\n",
        "                (by default returns same type as input).\n",
        "        \"\"\"\n",
        "\n",
        "        with utils.LocallyOverridenDefaults(\n",
        "            self, prepend_bos=prepend_bos, padding_side=padding_side\n",
        "        ):\n",
        "            if type(input) == str:\n",
        "                # If text, convert to tokens (batch_size=1)\n",
        "                assert (\n",
        "                    self.tokenizer is not None\n",
        "                ), \"Must provide a tokenizer if passing a string to the model\"\n",
        "                tokens = self.to_tokens(input, prepend_bos=prepend_bos, padding_side=padding_side)\n",
        "            else:\n",
        "                tokens = input\n",
        "\n",
        "            if return_type == \"input\":\n",
        "                if type(input) == str:\n",
        "                    return_type = \"str\"\n",
        "                else:\n",
        "                    return_type = \"tensor\"\n",
        "\n",
        "            assert isinstance(tokens, torch.Tensor)\n",
        "            batch_size, ctx_length = tokens.shape\n",
        "            device = devices.get_device_for_block_index(0, self.cfg)\n",
        "            tokens = tokens.to(device)\n",
        "            if use_past_kv_cache:\n",
        "                past_kv_cache = HookedTransformerKeyValueCache.init_cache(\n",
        "                    self.cfg, self.cfg.device, batch_size\n",
        "                )\n",
        "            else:\n",
        "                past_kv_cache = None\n",
        "\n",
        "            stop_tokens: List[int] = []\n",
        "            eos_token_for_padding = 0\n",
        "            assert self.tokenizer is not None\n",
        "            if stop_at_eos:\n",
        "                tokenizer_has_eos_token = (\n",
        "                    self.tokenizer is not None and self.tokenizer.eos_token_id is not None\n",
        "                )\n",
        "                if eos_token_id is None:\n",
        "                    assert (\n",
        "                        tokenizer_has_eos_token\n",
        "                    ), \"Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id\"\n",
        "\n",
        "                    eos_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "                if isinstance(eos_token_id, int):\n",
        "                    stop_tokens = [eos_token_id]\n",
        "                    eos_token_for_padding = eos_token_id\n",
        "                else:\n",
        "                    # eos_token_id is a Sequence (e.g. list or tuple)\n",
        "                    stop_tokens = eos_token_id\n",
        "                    eos_token_for_padding = (\n",
        "                        self.tokenizer.eos_token_id if tokenizer_has_eos_token else eos_token_id[0]\n",
        "                    )\n",
        "\n",
        "            # An array to track which sequences in the batch have finished.\n",
        "            finished_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.cfg.device)\n",
        "\n",
        "            # Currently nothing in HookedTransformer changes with eval, but this is here in case\n",
        "            # that changes in the future.\n",
        "            self.eval()\n",
        "            for index in tqdm.tqdm(range(max_new_tokens), disable=not verbose):\n",
        "                # While generating, we keep generating logits, throw away all but the final logits,\n",
        "                # and then use those logits to sample from the distribution We keep adding the\n",
        "                # sampled tokens to the end of tokens.\n",
        "                if use_past_kv_cache:\n",
        "                    # We just take the final tokens, as a [batch, 1] tensor\n",
        "                    if index > 0:\n",
        "                        logits = self.forward(\n",
        "                            tokens[:, -1:],\n",
        "                            return_type=\"logits\",\n",
        "                            prepend_bos=prepend_bos,\n",
        "                            padding_side=padding_side,\n",
        "                            past_kv_cache=past_kv_cache,\n",
        "                        )\n",
        "                    else:\n",
        "                        logits = self.forward(\n",
        "                            tokens,\n",
        "                            return_type=\"logits\",\n",
        "                            prepend_bos=prepend_bos,\n",
        "                            padding_side=padding_side,\n",
        "                            past_kv_cache=past_kv_cache,\n",
        "                        )\n",
        "                else:\n",
        "                    # We input the entire sequence, as a [batch, pos] tensor, since we aren't using\n",
        "                    # the cache.\n",
        "                    logits = self.forward(\n",
        "                        tokens,\n",
        "                        return_type=\"logits\",\n",
        "                        prepend_bos=prepend_bos,\n",
        "                        padding_side=padding_side,\n",
        "                    )\n",
        "                final_logits = logits[:, -1, :]\n",
        "\n",
        "                if do_sample:\n",
        "                    sampled_tokens = utils.sample_logits(\n",
        "                        final_logits,\n",
        "                        top_k=top_k,\n",
        "                        top_p=top_p,\n",
        "                        temperature=temperature,\n",
        "                        freq_penalty=freq_penalty,\n",
        "                        tokens=tokens,\n",
        "                    ).to(devices.get_device_for_block_index(0, self.cfg))\n",
        "                else:\n",
        "                    sampled_tokens = final_logits.argmax(-1).to(\n",
        "                        devices.get_device_for_block_index(0, self.cfg)\n",
        "                    )\n",
        "\n",
        "                if stop_at_eos:\n",
        "                    # For all unfinished sequences, add on the next token. If a sequence was\n",
        "                    # finished, throw away the generated token and add eos_token_for_padding\n",
        "                    # instead.\n",
        "                    sampled_tokens[finished_sequences] = eos_token_for_padding\n",
        "                    finished_sequences.logical_or_(\n",
        "                        torch.isin(\n",
        "                            sampled_tokens.to(self.cfg.device),\n",
        "                            torch.tensor(stop_tokens).to(self.cfg.device),\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                tokens = torch.cat([tokens, sampled_tokens.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "                if stop_at_eos and finished_sequences.all():\n",
        "                    break\n",
        "\n",
        "            if return_type == \"str\":\n",
        "                if self.cfg.default_prepend_bos:\n",
        "                    # If we prepended a BOS token, remove it when returning output.\n",
        "                    return self.tokenizer.decode(tokens[0, 1:])\n",
        "                else:\n",
        "                    return self.tokenizer.decode(tokens[0])\n",
        "\n",
        "            else:\n",
        "                return tokens\n",
        "\n",
        "    # Give access to all weights as properties.\n",
        "    @property\n",
        "    def W_U(self) -> Float[torch.Tensor, \"d_model d_vocab\"]:\n",
        "        \"\"\"Convenience to get the unembedding matrix.\n",
        "\n",
        "        I.e. the linear map from the final residual stream to the output logits).\n",
        "        \"\"\"\n",
        "        return self.unembed.W_U\n",
        "\n",
        "    @property\n",
        "    def b_U(self) -> Float[torch.Tensor, \"d_vocab\"]:\n",
        "        return self.unembed.b_U\n",
        "\n",
        "    @property\n",
        "    def W_E(self) -> Float[torch.Tensor, \"d_vocab d_model\"]:\n",
        "        \"\"\"Convenience to get the embedding matrix.\"\"\"\n",
        "        return self.embed.W_E\n",
        "\n",
        "    @property\n",
        "    def W_pos(self) -> Float[torch.Tensor, \"n_ctx d_model\"]:\n",
        "        \"\"\"Convenience function to get the positional embedding.\n",
        "\n",
        "        Only works on models with absolute positional embeddings!\n",
        "        \"\"\"\n",
        "        return self.pos_embed.W_pos\n",
        "\n",
        "    @property\n",
        "    def W_E_pos(self) -> Float[torch.Tensor, \"d_vocab+n_ctx d_model\"]:\n",
        "        \"\"\"Concatenated W_E and W_pos.\n",
        "\n",
        "        Used as a full (overcomplete) basis of the input space, useful for full QK and full OV\n",
        "        circuits.\n",
        "        \"\"\"\n",
        "        return torch.cat([self.W_E, self.W_pos], dim=0)\n",
        "\n",
        "    # Layer-specific weights are stacked into one massive tensor and given as properties for\n",
        "    # convenience and a cache is used to avoid repeated computation. Often a useful convenience when\n",
        "    # we want to do analysis on weights across all layers. If GPU memory is a bottleneck, don't use\n",
        "    # these properties!\n",
        "\n",
        "    @property\n",
        "    def W_K(self) -> Float[torch.Tensor, \"n_layers n_heads d_model d_head\"]:\n",
        "        \"\"\"Stack the key weights across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.W_K for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def W_Q(self) -> Float[torch.Tensor, \"n_layers n_heads d_model d_head\"]:\n",
        "        \"\"\"Stack the query weights across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.W_Q for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def W_V(self) -> Float[torch.Tensor, \"n_layers n_heads d_model d_head\"]:\n",
        "        \"\"\"Stack the value weights across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.W_V for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def W_O(self) -> Float[torch.Tensor, \"n_layers n_heads d_head d_model\"]:\n",
        "        \"\"\"Stack the attn output weights across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.W_O for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def W_in(self) -> Float[torch.Tensor, \"n_layers d_model d_mlp\"]:\n",
        "        \"\"\"Stack the MLP input weights across all layers.\"\"\"\n",
        "        return torch.stack([block.mlp.W_in for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def W_gate(self) -> Union[Float[torch.Tensor, \"n_layers d_model d_mlp\"], None]:\n",
        "        \"\"\"Stack the MLP gate weights across all layers.\n",
        "\n",
        "        Only works for models with gated MLPs.\n",
        "        \"\"\"\n",
        "        if self.cfg.gated_mlp:\n",
        "            return torch.stack([block.mlp.W_gate for block in self.blocks], dim=0)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @property\n",
        "    def W_out(self) -> Float[torch.Tensor, \"n_layers d_mlp d_model\"]:\n",
        "        \"\"\"Stack the MLP output weights across all layers.\"\"\"\n",
        "        return torch.stack([block.mlp.W_out for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_K(self) -> Float[torch.Tensor, \"n_layers n_heads d_head\"]:\n",
        "        \"\"\"Stack the key biases across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.b_K for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_Q(self) -> Float[torch.Tensor, \"n_layers n_heads d_head\"]:\n",
        "        \"\"\"Stack the query biases across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.b_Q for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_V(self) -> Float[torch.Tensor, \"n_layers n_heads d_head\"]:\n",
        "        \"\"\"Stack the value biases across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.b_V for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_O(self) -> Float[torch.Tensor, \"n_layers d_model\"]:\n",
        "        \"\"\"Stack the attn output biases across all layers.\"\"\"\n",
        "        return torch.stack([block.attn.b_O for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_in(self) -> Float[torch.Tensor, \"n_layers d_mlp\"]:\n",
        "        \"\"\"Stack the MLP input biases across all layers.\"\"\"\n",
        "        return torch.stack([block.mlp.b_in for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def b_out(self) -> Float[torch.Tensor, \"n_layers d_model\"]:\n",
        "        \"\"\"Stack the MLP output biases across all layers.\"\"\"\n",
        "        return torch.stack([block.mlp.b_out for block in self.blocks], dim=0)\n",
        "\n",
        "    @property\n",
        "    def QK(self):\n",
        "        return FactoredMatrix(self.W_Q, self.W_K.transpose(-2, -1))\n",
        "\n",
        "    @property\n",
        "    def OV(self):\n",
        "        return FactoredMatrix(self.W_V, self.W_O)\n",
        "\n",
        "    # Various utility functions\n",
        "    def accumulated_bias(\n",
        "        self, layer: int, mlp_input: bool = False, include_mlp_biases=True\n",
        "    ) -> Float[torch.Tensor, \"d_model\"]:\n",
        "        \"\"\"Accumulated Bias.\n",
        "\n",
        "        Returns the accumulated bias from all layer outputs (ie the b_Os and b_outs), up to the\n",
        "        input of layer L.\n",
        "\n",
        "        Args:\n",
        "            layer (int): Layer number, in [0, n_layers]. layer==0 means no layers, layer==n_layers\n",
        "                means all layers.\n",
        "            mlp_input (bool): If True, we take the bias up to the input of the MLP\n",
        "                of layer L (ie we include the bias from the attention output of the current layer,\n",
        "                otherwise just biases from previous layers)\n",
        "            include_mlp_biases (bool): Whether to include the biases of MLP layers. Often useful to\n",
        "                have as False if we're expanding attn_out into individual heads, but keeping mlp_out\n",
        "                as is.\n",
        "\n",
        "        Returns:\n",
        "            bias (torch.Tensor): [d_model], accumulated bias\n",
        "        \"\"\"\n",
        "        accumulated_bias = torch.zeros(self.cfg.d_model, device=self.cfg.device)\n",
        "\n",
        "        for i in range(layer):\n",
        "            accumulated_bias += self.blocks[i].attn.b_O\n",
        "            if include_mlp_biases:\n",
        "                accumulated_bias += self.blocks[i].mlp.b_out\n",
        "        if mlp_input:\n",
        "            assert layer < self.cfg.n_layers, \"Cannot include attn_bias from beyond the final layer\"\n",
        "            accumulated_bias += self.blocks[layer].attn.b_O\n",
        "        return accumulated_bias\n",
        "\n",
        "    def all_composition_scores(\n",
        "        self, mode\n",
        "    ) -> Float[torch.Tensor, \"n_layers n_heads n_layers n_heads\"]:\n",
        "        \"\"\"All Composition Scores.\n",
        "\n",
        "        Returns the Composition scores for all pairs of heads, as a L1, H1, L2, H2 tensor (which is\n",
        "        upper triangular on the first and third axes).\n",
        "\n",
        "        See\n",
        "        https://transformer-circuits.pub/2021/framework/index.html#:~:text=The%20above%20diagram%20shows%20Q%2D%2C%20K%2D%2C%20and%20V%2DComposition\n",
        "        for three metrics used.\n",
        "\n",
        "        Args:\n",
        "            mode (str): One of [\"Q\", \"K\", \"V\"], the mode to use for the composition score.\n",
        "        \"\"\"\n",
        "        left = self.OV\n",
        "        if mode == \"Q\":\n",
        "            right = self.QK\n",
        "        elif mode == \"K\":\n",
        "            right = self.QK.T\n",
        "        elif mode == \"V\":\n",
        "            right = self.OV\n",
        "        else:\n",
        "            raise ValueError(f\"mode must be one of ['Q', 'K', 'V'] not {mode}\")\n",
        "\n",
        "        scores = utils.composition_scores(left, right, broadcast_dims=True)\n",
        "        # Mask scores to be zero for all pairs with the right head in the same layer or earlier\n",
        "        # layer than the left head.\n",
        "        mask = (\n",
        "            torch.arange(self.cfg.n_layers, device=self.cfg.device)[:, None, None, None]\n",
        "            < torch.arange(self.cfg.n_layers, device=self.cfg.device)[None, None, :, None]\n",
        "        )\n",
        "        scores = torch.where(mask, scores, torch.zeros_like(scores))\n",
        "        return scores\n",
        "\n",
        "    def all_head_labels(self):\n",
        "        \"\"\"Returns a list of all head names in the model.\"\"\"\n",
        "        return [f\"L{l}H{h}\" for l in range(self.cfg.n_layers) for h in range(self.cfg.n_heads)]\n",
        "\n",
        "    def load_sample_training_dataset(self, **kwargs):\n",
        "        \"\"\"Load Sample Training Dataset.\n",
        "\n",
        "        Helper function to load in a 10K-20K dataset of elements from the model's training data\n",
        "        distribution.\n",
        "\n",
        "        Wrapper around utils.get_dataset, which identifies the appropriate dataset the pretrained\n",
        "        models. Each dataset has a 'text' field, which contains the relevant info, some have several\n",
        "        meta data fields.\n",
        "\n",
        "        Kwargs will be passed to utils.get_dataset (e.g. cache_dir to set download location)\n",
        "\n",
        "        Notes:\n",
        "\n",
        "        - PT-2's training data is not open source. OpenWebText is a replication (links with\n",
        "            >3 karma on Reddit)\n",
        "        - OPT's training data is not open source, and is a mess of different things that is hard to\n",
        "          replicate. I default to the Pile, which covers some of it, but imperfectly.\n",
        "\n",
        "        (Some models will have actually been trained on the data supplied here, for some it's from\n",
        "        the validation set).\n",
        "        \"\"\"\n",
        "        model_dataset_map = {\n",
        "            \"neel\": \"c4_code\",\n",
        "            \"neel-solu-old\": \"pile\",\n",
        "            \"GPT2LMHeadModel\": \"openwebtext\",\n",
        "            \"GPTNeoForCausalLM\": \"pile\",\n",
        "            \"GPTNeoXForCausalLM\": \"pile\",\n",
        "            \"GPTJForCausalLM\": \"pile\",\n",
        "            \"OPTForCausalLM\": \"pile\",\n",
        "        }\n",
        "        if self.cfg.original_architecture in model_dataset_map:\n",
        "            self.dataset = utils.get_dataset(\n",
        "                model_dataset_map[self.cfg.original_architecture], **kwargs\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"We do not have an available dataset for the relevant model: {self.cfg.original_architecture}\"\n",
        "            )\n",
        "        return self.dataset\n",
        "\n",
        "    def sample_datapoint(\n",
        "        self,\n",
        "        tokenize: bool = False,\n",
        "        prepend_bos: Optional[Union[bool, None]] = USE_DEFAULT_VALUE,\n",
        "        padding_side: Optional[Literal[\"left\", \"right\"]] = USE_DEFAULT_VALUE,\n",
        "    ) -> Union[str, Float[torch.Tensor, \"1 pos\"]]:\n",
        "        \"\"\"Sample Data Point from Dataset.\n",
        "\n",
        "        Helper function to randomly sample a data point from self.dataset, a small dataset from the\n",
        "        data distribution the model was trained on.\n",
        "\n",
        "        Implicitly calls self.load_sample_training_dataset if it hasn't already been called. Only\n",
        "        works for pretrained models with an associated dataset. But you can manually replace\n",
        "        self.dataset with a dataset of your choice if you want.\n",
        "\n",
        "        Args:\n",
        "            tokenize (bool): Whether to return tokens (instead of text). Defaults to False. Note\n",
        "                that the returned tokens will be automatically truncated to the model's max context\n",
        "                size.\n",
        "            prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
        "                the BOS token to the input (applicable when input is a string). Defaults to None,\n",
        "                implying usage of self.cfg.default_prepend_bos (default is True unless specified\n",
        "                otherwise). Pass True or False to override the default.\n",
        "            padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
        "                self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple\n",
        "                strings of different lengths.\n",
        "        \"\"\"\n",
        "        if self.dataset is None:\n",
        "            self.load_sample_training_dataset()\n",
        "        assert self.dataset is not None  # keep mypy happy\n",
        "        sample_dataset_size = len(self.dataset)\n",
        "        index = np.random.randint(0, sample_dataset_size)\n",
        "        if not tokenize:\n",
        "            return self.dataset[index][\"text\"]\n",
        "        else:\n",
        "            return self.to_tokens(\n",
        "                self.dataset[index][\"text\"],\n",
        "                prepend_bos=prepend_bos,\n",
        "                padding_side=padding_side,\n",
        "                truncate=True,\n",
        "            )"
      ],
      "metadata": {
        "id": "rZ61cGxadPxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Functions"
      ],
      "metadata": {
        "id": "7U_6MxmEZuQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J094s2Swi6vL",
        "outputId": "005b254e-ce12-4e2e-9150-7b96458d9447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(max_steps=110000,\n",
              "          print_every=100,\n",
              "          log_every=2500,\n",
              "          lr_decay=0.333,\n",
              "          step_lr_every=33000,\n",
              "          device='cuda',\n",
              "          seed=42,\n",
              "          n_layer=4,\n",
              "          n_embd=64,\n",
              "          n_embd2=64,\n",
              "          n_ctx_head=4,\n",
              "          learning_rate=0.01,\n",
              "          weight_decay=0.0001,\n",
              "          batch_size=32,\n",
              "          train_size=497000,\n",
              "          test_size=3000,\n",
              "          num_words=4,\n",
              "          max_seq_length=1000,\n",
              "          augment=True,\n",
              "          ablate_cross_attention=False,\n",
              "          downsample_mean=0.65,\n",
              "          downsample_width=0.1,\n",
              "          add_digits=True,\n",
              "          alphabet=' enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"\\'(BCQLMWYU,ZF!DXV?KPGJ',\n",
              "          dataset_name='bigbank',\n",
              "          wandb_project='bigbank_experiments',\n",
              "          wandb_entity='sam-greydanus',\n",
              "          wandb_run_name='cursivetransformer_dictionary_learning',\n",
              "          wandb_api_key=None,\n",
              "          load_from_run_id='6le6tujz',\n",
              "          sample_only=True,\n",
              "          local_checkpoint_path='best_checkpoint.pt',\n",
              "          block_size=1000,\n",
              "          context_block_size=50,\n",
              "          vocab_size=455,\n",
              "          context_vocab_size=72)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HookedCursiveTransformerConfig(HookedTransformerConfig):\n",
        "    def __init__(self, cfg_dict):\n",
        "        super().__init__(cfg_dict)\n",
        "        self.load_from_run_id: str = cfg_dict[\"load_from_run_id\"]\n",
        "\n",
        "# - [ ] TODO: Update this for our config!\n",
        "def convert_cursivetransformer_model_config(official_model_name: str, args, **kwargs):\n",
        "    \"\"\"\n",
        "    Loads the config for a model trained by us (ZachWimpee & SamGreydanus), converted to a dictionary\n",
        "    in the HookedTransformerConfig format.\n",
        "\n",
        "    AutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.\n",
        "    \"\"\"\n",
        "    official_model_name = get_official_model_name(official_model_name)\n",
        "    cfg_dict = {\n",
        "        \"d_model\": args.n_embd,\n",
        "        \"n_layers\": args.n_layer,\n",
        "        \"d_mlp\": args.n_embd,\n",
        "        \"d_head\": args.n_embd,\n",
        "        \"n_heads\": args.n_ctx_head,\n",
        "        \"n_ctx\": args.max_seq_length,\n",
        "        \"d_vocab\": args.vocab_size,\n",
        "        \"tokenizer_name\": None,\n",
        "        \"act_fn\": \"NewGELU\",\n",
        "        \"attn_only\": False,\n",
        "        \"final_rms\": False,\n",
        "        \"original_architecture\": \"cursivetransformer\",\n",
        "        \"normalization_type\": \"LN\",\n",
        "        \"load_from_run_id\": args.load_from_run_id\n",
        "    }\n",
        "    cfg = HookedTransformerConfig.from_dict(cfg_dict)\n",
        "    return cfg\n",
        "\n",
        "def convert_cursivetransformer_weights(cursivetransformer, cfg: HookedTransformerConfig):\n",
        "    state_dict = {}\n",
        "\n",
        "    state_dict[\"embed.W_E\"] = cursivetransformer.transformer.wte.weight\n",
        "    state_dict[\"pos_embed.W_pos\"] = cursivetransformer.transformer.wpe.weight\n",
        "\n",
        "    for l in range(cfg.n_layers):\n",
        "        state_dict[f\"blocks.{l}.ln1.w\"] = cursivetransformer.transformer.h[l].ln_1.weight\n",
        "        state_dict[f\"blocks.{l}.ln1.b\"] = cursivetransformer.transformer.h[l].ln_1.bias\n",
        "\n",
        "        # In GPT-2, q,k,v are produced by one big linear map, whose output is\n",
        "        # concat([q, k, v])\n",
        "        W = cursivetransformer.transformer.h[l].attn.c_attn.weight\n",
        "        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
        "        W_Q = einops.rearrange(W_Q, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "        W_K = einops.rearrange(W_K, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "        W_V = einops.rearrange(W_V, \"m (i h)->i m h\", i=cfg.n_heads)\n",
        "\n",
        "        state_dict[f\"blocks.{l}.attn.W_Q\"] = W_Q\n",
        "        state_dict[f\"blocks.{l}.attn.W_K\"] = W_K\n",
        "        state_dict[f\"blocks.{l}.attn.W_V\"] = W_V\n",
        "\n",
        "        qkv_bias = cursivetransformer.transformer.h[l].attn.c_attn.bias\n",
        "        qkv_bias = einops.rearrange(\n",
        "            qkv_bias,\n",
        "            \"(qkv index head)->qkv index head\",\n",
        "            qkv=3,\n",
        "            index=cfg.n_heads,\n",
        "            head=cfg.d_head,\n",
        "        )\n",
        "        state_dict[f\"blocks.{l}.attn.b_Q\"] = qkv_bias[0]\n",
        "        state_dict[f\"blocks.{l}.attn.b_K\"] = qkv_bias[1]\n",
        "        state_dict[f\"blocks.{l}.attn.b_V\"] = qkv_bias[2]\n",
        "\n",
        "        W_O = cursivetransformer.transformer.h[l].attn.c_proj.weight\n",
        "        W_O = einops.rearrange(W_O, \"(i h) m->i h m\", i=cfg.n_heads)\n",
        "        state_dict[f\"blocks.{l}.attn.W_O\"] = W_O\n",
        "        state_dict[f\"blocks.{l}.attn.b_O\"] = cursivetransformer.transformer.h[l].attn.c_proj.bias\n",
        "\n",
        "        state_dict[f\"blocks.{l}.ln2.w\"] = cursivetransformer.transformer.h[l].ln_2.weight\n",
        "        state_dict[f\"blocks.{l}.ln2.b\"] = cursivetransformer.transformer.h[l].ln_2.bias\n",
        "\n",
        "        W_in = cursivetransformer.transformer.h[l].mlp.c_fc.weight\n",
        "        state_dict[f\"blocks.{l}.mlp.W_in\"] = W_in\n",
        "        state_dict[f\"blocks.{l}.mlp.b_in\"] = cursivetransformer.transformer.h[l].mlp.c_fc.bias\n",
        "\n",
        "        W_out = cursivetransformer.transformer.h[l].mlp.c_proj.weight\n",
        "        state_dict[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
        "        state_dict[f\"blocks.{l}.mlp.b_out\"] = cursivetransformer.transformer.h[l].mlp.c_proj.bias\n",
        "    state_dict[\"unembed.W_U\"] = cursivetransformer.lm_head.weight.T\n",
        "\n",
        "    state_dict[\"ln_final.w\"] = cursivetransformer.transformer.ln_f.weight\n",
        "    state_dict[\"ln_final.b\"] = cursivetransformer.transformer.ln_f.bias\n",
        "    return state_dict"
      ],
      "metadata": {
        "id": "FUibmGELZbLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cursivetransformer\"\n",
        "cfg = convert_cursivetransformer_model_config(model_name, args)\n",
        "cfg"
      ],
      "metadata": {
        "id": "7MLJBBKQg5Yk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "e68e4940-e40b-4c08-e7d5-93bd4cb0ab71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "HookedTransformerConfig.__init__() got an unexpected keyword argument 'load_from_run_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fd0fda57169e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cursivetransformer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_cursivetransformer_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-60c9b358e95a>\u001b[0m in \u001b[0;36mconvert_cursivetransformer_model_config\u001b[0;34m(official_model_name, args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"load_from_run_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_run_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     }\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHookedTransformerConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformerConfig.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, config_dict)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: HookedTransformerConfig.__init__() got an unexpected keyword argument 'load_from_run_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = convert_cursivetransformer_weights(model, cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "gxOIpArGi2pO",
        "outputId": "7e338acd-11d6-48ee-9d12-81386170c957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'n_layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-254fe1ddaab6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_cursivetransformer_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-a2d2efe3d924>\u001b[0m in \u001b[0;36mconvert_cursivetransformer_weights\u001b[0;34m(cursivetransformer, cfg)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_embed.W_pos\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursivetransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"blocks.{l}.ln1.w\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursivetransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"blocks.{l}.ln1.b\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursivetransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'n_layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LbGHjFFn74r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}