{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/synthbank_zach_v1_6word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3v2biu1Ttg7"
      },
      "source": [
        "# Training a Cursive Transformer\n",
        "Zach Wimpee | 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QvXUjGZHl7w",
        "outputId": "57b3da1c-be78-4d26-dd22-68d5ad292d66"
      },
      "outputs": [],
      "source": [
        "! pip -q install wandb\n",
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0WRHSM_Kksp",
        "outputId": "1f8323b6-fd5f-4ed4-8da7-14da3faddb7e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "import os, sys, time, math, argparse, io, copy, json, pdb\n",
        "import pickle, glob, json, zipfile\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from math import comb\n",
        "import functools\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Try attaching to GPU\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)\n",
        "RUN_TAG = \"synthbank_experiments\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxM2-WQjBJ8f"
      },
      "source": [
        "## Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfFC9VAC3oPz"
      },
      "outputs": [],
      "source": [
        "def plot_strokes(stroke, title, fig=None, ax=None):\n",
        "    \"\"\"Plot a single stroke\"\"\"\n",
        "    if fig is None or ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 2), dpi=300)\n",
        "\n",
        "    # Separate strokes based on pen lifts\n",
        "    strokes = []\n",
        "    current_stroke = []\n",
        "    for point in stroke:\n",
        "        if point[2] == 1:  # Pen is down\n",
        "            current_stroke.append(point)\n",
        "        else:  # Pen is up\n",
        "            if current_stroke:\n",
        "                strokes.append(current_stroke)\n",
        "                current_stroke = []\n",
        "    if current_stroke:\n",
        "        strokes.append(current_stroke)\n",
        "\n",
        "    # Plot each stroke\n",
        "    for stroke in strokes:\n",
        "        x, y = zip(*[(p[0], 1 - p[1]) for p in stroke])  # Invert y-axis\n",
        "        ax.plot(x, y, 'b-', linewidth=1)\n",
        "\n",
        "    ax.set_aspect('equal') ; ax.set_title(title)\n",
        "    if fig is None: plt.show()\n",
        "    return fig, ax\n",
        "\n",
        "@functools.lru_cache(maxsize=5)\n",
        "def load_and_parse_data(min_ascii_length=3):\n",
        "    uploaded = files.upload()\n",
        "    file_content = next(iter(uploaded.values()))\n",
        "    data = json.loads(file_content.decode('utf-8'))\n",
        "    for i in range(len(data)):\n",
        "      strokes = np.array(data[i]['points'])\n",
        "      strokes[:,0:1] *= data[i]['metadata']['aspectRatio']\n",
        "      strokes[:, 0] -= strokes[0, 0]\n",
        "      data[i]['points'] = strokes\n",
        "    data = [d for d in data if len(d['metadata']['asciiSequence']) >= min_ascii_length]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jHeGQgJnxaZ"
      },
      "outputs": [],
      "source": [
        "def combine_handwriting_examples(examples, space_width=0.17):\n",
        "    assert len(set(ex['metadata']['author'] for ex in examples)) == 1, \"All examples must have the same author\"\n",
        "\n",
        "    combined_metadata = {\n",
        "        'author': examples[0]['metadata']['author'],\n",
        "        'asciiSequence': ' '.join(ex['metadata']['asciiSequence'] for ex in examples),\n",
        "        'pointCount': sum(ex['metadata']['pointCount'] for ex in examples),\n",
        "        'strokeCount': sum(ex['metadata']['strokeCount'] for ex in examples),\n",
        "        'aspectRatio': examples[0]['metadata']['aspectRatio']\n",
        "    }\n",
        "\n",
        "    combined_points, current_x_offset, total_width = [], 0, 0\n",
        "\n",
        "    for i, example in enumerate(examples):\n",
        "        points = example['points']\n",
        "        word_width = np.max(points[:, 0]) - np.min(points[:, 0])\n",
        "        total_width += word_width\n",
        "\n",
        "        normalized_points = points.copy()\n",
        "        normalized_points[:, 0] -= np.min(points[:, 0])\n",
        "        normalized_points[:, 0] += current_x_offset\n",
        "\n",
        "        combined_points.append(normalized_points)\n",
        "        current_x_offset += word_width\n",
        "\n",
        "        if i < len(examples) - 1:\n",
        "            combined_points.append(np.array([[current_x_offset + space_width, normalized_points[-1, 1], 0]]))\n",
        "            current_x_offset += space_width\n",
        "            total_width += space_width\n",
        "            combined_metadata['pointCount'] += 1\n",
        "\n",
        "    combined_points = np.vstack(combined_points)\n",
        "    return {'metadata': combined_metadata, 'points': combined_points}\n",
        "\n",
        "def generate_word_combos(raw_json, desired_num_combos=10000, num_words=3):\n",
        "  num_combos = comb(len(raw_json), num_words)\n",
        "  print(f'For a dataset of {len(raw_json)} examples we can generate {num_combos} combinations of {num_words} examples.')\n",
        "  print(f'Generating {desired_num_combos} random (and thus possibly overlapping) combos...')\n",
        "  combo_json = []\n",
        "  for i in range(desired_num_combos):\n",
        "    ixs = np.random.choice(len(raw_json), size=num_words, replace=False)\n",
        "    words_to_merge = [raw_json[i] for i in ixs]\n",
        "    combo_json.append( combine_handwriting_examples(words_to_merge) )\n",
        "  return combo_json\n",
        "\n",
        "def load_and_combine_examples(desired_num_combos=10000, num_words=3):\n",
        "  data = load_and_parse_data()\n",
        "  return generate_word_combos(data, desired_num_combos, num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "bTsYyYxXW2Th",
        "outputId": "4ccc16c6-be8d-4763-b7d1-871014438ca8"
      },
      "outputs": [],
      "source": [
        "data = load_and_combine_examples()\n",
        "print(len(data))\n",
        "k = 700 ; data[k]['points'][:,1].min(), data[k]['points'][:,1].max()\n",
        "\n",
        "ix = np.random.randint(len(data))\n",
        "combo = data[ix]\n",
        "print(len(combo['points']))\n",
        "_ = plot_strokes(combo['points'], title=combo['metadata']['asciiSequence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OobnYMcDKf78"
      },
      "outputs": [],
      "source": [
        "def decompose_offsets(offsets):\n",
        "    dx, dy = offsets[:, 0], offsets[:, 1]\n",
        "    r = np.hypot(dx, dy)\n",
        "    theta = np.arctan2(dy, dx)\n",
        "    return np.column_stack((r, theta, offsets[:, 2]))\n",
        "\n",
        "def reconstruct_offsets(polar_data):\n",
        "    r, theta = polar_data[:, 0], polar_data[:, 1]\n",
        "    dx = r * np.cos(theta)\n",
        "    dy = r * np.sin(theta)\n",
        "    return np.column_stack((dx, dy, polar_data[:, 2]))\n",
        "\n",
        "def strokes_to_offsets(points):\n",
        "    # Calculate differences (dx, dy), not considering pen_down\n",
        "    offsets = np.zeros_like(points)\n",
        "    offsets[1:, 0:2] = np.diff(points[:, 0:2], axis=0)  # Compute dx, dy\n",
        "    offsets[:, 2] = points[:, 2]  # Copy pen_down directly\n",
        "\n",
        "    # Decouple direction from magnitude (this will help with tokenization)\n",
        "    offsets_dec = decompose_offsets(offsets)\n",
        "    return offsets_dec\n",
        "\n",
        "def offsets_to_strokes(offsets_dec):\n",
        "    # Calculate cumulative sums to get absolute positions\n",
        "    offsets = reconstruct_offsets(offsets_dec)\n",
        "\n",
        "    absolute_coords = np.cumsum(offsets[:, :2], axis=0)\n",
        "    stroke_data = np.hstack((absolute_coords, offsets[:, 2:3]))\n",
        "    return stroke_data\n",
        "\n",
        "def horizontal_shear(stroke, shear_range=(-0.4, 0.4)):\n",
        "    shear_factor = np.random.uniform(*shear_range)\n",
        "    shear_matrix = np.array([\n",
        "        [1, shear_factor],\n",
        "        [0, 1]])\n",
        "    stroke[:, :2] = np.dot(stroke[:, :2], shear_matrix.T)\n",
        "    return stroke\n",
        "\n",
        "def remove_random_points(stroke, remove_percentage=0.04):\n",
        "    num_points = np.random.randint(len(stroke))\n",
        "    num_remove = int(num_points * remove_percentage)\n",
        "    indices = np.random.choice(range(1, num_points - 1), num_remove, replace=False).astype(np.int32)\n",
        "    return np.delete(stroke, indices, axis=0)\n",
        "\n",
        "def downsample(arr, fraction):\n",
        "    if not 0 <= fraction <= 1:\n",
        "        raise ValueError(\"Fraction must be between 0 and 1\")\n",
        "    if fraction == 1:\n",
        "        return arr\n",
        "    new_length = int(len(arr) * (1 - fraction))\n",
        "    indices = np.linspace(0, len(arr) - 1, new_length, dtype=int)\n",
        "    return arr[indices]\n",
        "\n",
        "\n",
        "\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, strokes, texts, chars, max_seq_length=1100, max_text_length=50, name='', augment=False):\n",
        "        self.name = name\n",
        "        self.strokes = strokes  # List of Nx3 arrays, each representing a cursive sentence\n",
        "        self.texts = texts  # List of corresponding text strings\n",
        "        self.chars = chars  # String of all possible characters\n",
        "        self.augment = augment\n",
        "\n",
        "        self.theta_bins = np.linspace(-np.pi, np.pi, 201)  # 100 bins for theta\n",
        "\n",
        "        r_bins_pen_down = np.concatenate([\n",
        "            np.asarray([0]),\n",
        "            np.linspace(0.005, 0.050, 50),  # Close around 0.01, 30 bins\n",
        "            np.geomspace(0.051, 4, 81)[:-1]  # 150 exponential bins\n",
        "        ])\n",
        "        r_bins_pen_up = r_bins_pen_down + max(r_bins_pen_down) + 1  # Offset for pen-up states\n",
        "        self.r_bins = np.concatenate([r_bins_pen_down, r_bins_pen_up])\n",
        "\n",
        "        self.feature_sizes = [len(self.r_bins), len(self.theta_bins)]\n",
        "        self.cumulative_sizes = np.cumsum([0] + self.feature_sizes)\n",
        "\n",
        "        # Add special tokens for strokes\n",
        "        self.PAD_TOKEN = sum(self.feature_sizes)\n",
        "        self.END_TOKEN = sum(self.feature_sizes) + 1\n",
        "\n",
        "        # Character tokenization\n",
        "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:s for s,i in self.stoi.items()}\n",
        "        self.char_PAD_TOKEN = 0\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def augment_stroke(self, stroke):\n",
        "\n",
        "        stroke = remove_random_points(stroke, remove_percentage=0.01) # Drop some points\n",
        "        stroke = horizontal_shear(stroke, shear_range=(-0.33, 0.15)) # Horizontal shear\n",
        "\n",
        "        stroke[:, 0:1] *= np.random.uniform(0.95, 1.05)\n",
        "        stroke[:, 1:2] *= np.random.uniform(0.95, 1.05)\n",
        "\n",
        "        # noise = np.random.normal(0, 0.001, stroke[:, :2].shape) # Random noise\n",
        "        # stroke[:, :2] += noise\n",
        "\n",
        "        angle = np.random.uniform(-.08, .08) # Random rotation\n",
        "        rad = np.deg2rad(angle)\n",
        "        rotation_matrix = np.array([\n",
        "            [np.cos(rad), -np.sin(rad)],\n",
        "             [np.sin(rad), np.cos(rad)]])\n",
        "        stroke[:, :2] = np.dot(stroke[:, :2], rotation_matrix.T)\n",
        "\n",
        "        # Random starting point\n",
        "        # stroke = stroke[np.random.randint(1, 2):-np.random.randint(1, 2)]\n",
        "\n",
        "        # Downsample stroke\n",
        "        stroke[1:,2:3] *= stroke[:-1,2:3] # pen_up will now always come in sets of 3+\n",
        "        stroke[2:,2:3] *= stroke[:-2,2:3]\n",
        "        stroke[3:,2:3] *= stroke[:-3,2:3]\n",
        "        stroke = downsample(stroke, .63) # stroke[::2]\n",
        "        return stroke\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strokes)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return sum(self.feature_sizes) + 2  # +2 for PAD and END tokens\n",
        "\n",
        "    def get_char_vocab_size(self):\n",
        "        return len(self.chars) + 1  # +1 for PAD token\n",
        "\n",
        "    def get_stroke_seq_length(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "    def get_text_seq_length(self):\n",
        "        return self.max_text_length\n",
        "\n",
        "    def encode_stroke(self, stroke):\n",
        "        # Encode magnitude and pen state together\n",
        "        r_idx = np.digitize(stroke[:, 0], self.r_bins[:len(self.r_bins)//2]) - 1\n",
        "        r_idx[stroke[:, 2] == 0] += len(self.r_bins) // 2  # Offset for pen-up states\n",
        "\n",
        "        theta_idx = np.digitize(stroke[:, 1], self.theta_bins) - 1\n",
        "\n",
        "        encoded = np.column_stack([\n",
        "            theta_idx + self.cumulative_sizes[1],\n",
        "            r_idx + self.cumulative_sizes[0],])\n",
        "        return encoded.flatten()\n",
        "\n",
        "    def decode_stroke(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "\n",
        "        # Remove PAD and END tokens\n",
        "        ix = ix[(ix != self.PAD_TOKEN) & (ix != self.END_TOKEN)]\n",
        "\n",
        "        # Reshape the flattened array back to Nx2\n",
        "        ix = ix[:(len(ix)//2)*2]\n",
        "        ix = ix.reshape(-1, 2)\n",
        "\n",
        "        r_idx = ix[:, 1] - self.cumulative_sizes[0]\n",
        "        pen = (r_idx < len(self.r_bins) // 2).astype(int)\n",
        "        r_idx[pen == 0] -= len(self.r_bins) // 2\n",
        "        r = self.r_bins[:len(self.r_bins)//2][r_idx.clip(0, len(self.r_bins)//2 - 1)]\n",
        "        theta = self.theta_bins[(ix[:, 0] - self.cumulative_sizes[1]).clip(0, len(self.theta_bins)-1)]\n",
        "\n",
        "        return np.column_stack([r, theta, pen])\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return torch.tensor([self.stoi.get(ch, self.char_PAD_TOKEN) for ch in text], dtype=torch.long)\n",
        "\n",
        "    def decode_text(self, ix):\n",
        "        if isinstance(ix, torch.Tensor):\n",
        "            ix = ix.cpu().numpy()\n",
        "        return ''.join([self.itos.get(i, '') for i in ix if i != self.char_PAD_TOKEN])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        stroke = self.strokes[idx]\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            stroke = self.augment_stroke(stroke.copy())\n",
        "\n",
        "        # Encode stroke\n",
        "        stroke_offsets = strokes_to_offsets(stroke)\n",
        "        encoded_stroke = self.encode_stroke(stroke_offsets)\n",
        "        x = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "        y = torch.full((self.max_seq_length,), self.PAD_TOKEN, dtype=torch.long)\n",
        "\n",
        "        seq_len = min(len(encoded_stroke), self.max_seq_length - 1)  # -1 to leave room for END token\n",
        "        x[:seq_len] = torch.tensor(encoded_stroke[:seq_len], dtype=torch.long)\n",
        "        x[seq_len] = self.END_TOKEN\n",
        "\n",
        "        y[:seq_len] = x[1:seq_len+1]\n",
        "        y[seq_len] = self.END_TOKEN\n",
        "\n",
        "        # Encode text (context) and pad to max_text_length of 30\n",
        "        encoded_text = self.encode_text(text)\n",
        "        c = torch.full((self.max_text_length,), self.char_PAD_TOKEN, dtype=torch.long)\n",
        "        text_len = min(len(encoded_text), self.max_text_length)\n",
        "        c[:text_len] = encoded_text[:text_len]\n",
        "\n",
        "        return x, c, y\n",
        "\n",
        "\n",
        "def create_datasets(augment=True, max_seq_length=1100, num_words=3):\n",
        "  np.random.seed(0) ; torch.manual_seed(0)\n",
        "  data = load_and_parse_data()\n",
        "\n",
        "  # partition the input data into a training and the test set\n",
        "  test_set_size = min(1000, int(len(data) * 0.05)) # 10% of the training set, or up to 1000 examples\n",
        "  rp = torch.randperm(len(data)).tolist()\n",
        "\n",
        "  train_examples = generate_word_combos([data[i] for i in rp[:-test_set_size]], desired_num_combos=249000, num_words=num_words)\n",
        "  train_examples = [train_examples[i] for i in torch.randperm(len(train_examples)).tolist()]\n",
        "\n",
        "  test_examples = generate_word_combos([data[i] for i in rp[-test_set_size:]], desired_num_combos=1000, num_words=num_words)\n",
        "  test_examples = [test_examples[i] for i in torch.randperm(len(test_examples)).tolist()]\n",
        "\n",
        "  train_strokes = [copy.deepcopy(v['points']) for v in train_examples]\n",
        "  train_texts = [copy.deepcopy(v['metadata']['asciiSequence']) for v in train_examples]\n",
        "\n",
        "  test_strokes = [copy.deepcopy(v['points']) for v in test_examples]\n",
        "  test_texts = [copy.deepcopy(v['metadata']['asciiSequence']) for v in test_examples]\n",
        "\n",
        "  chars = \"abcdefghijklmnopqrstuvwxyz \"\n",
        "  print(f\"Number of examples in the train dataset: {len(train_examples)}\")\n",
        "  print(f\"Number of examples in the test dataset: {len(test_examples)}\")\n",
        "  print(f\"Max token sequence length: {max_seq_length}\")\n",
        "  print(f\"Number of unique characters in the ascii vocabulary: {len(chars)}\")\n",
        "  print(\"Ascii vocabulary:\")\n",
        "  print(f'\\t\"{chars}\"')\n",
        "  print(f\"Split up the dataset into {len(train_examples)} training examples and {len(test_examples)} test examples\")\n",
        "\n",
        "  # wrap in dataset objects\n",
        "  train_dataset = StrokeDataset(train_strokes, train_texts, chars, max_seq_length, name='train', augment=augment)\n",
        "  test_dataset = StrokeDataset(test_strokes, test_texts, chars, max_seq_length, name='test', augment=augment)\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader:\n",
        "    \"\"\"\n",
        "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
        "    a better way in PyTorch to just create an infinite dataloader\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, **kwargs):\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
        "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
        "        self.data_iter = iter(self.train_loader)\n",
        "\n",
        "    def next(self):\n",
        "        try:\n",
        "            batch = next(self.data_iter)\n",
        "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
        "            self.data_iter = iter(self.train_loader)\n",
        "            batch = next(self.data_iter)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRH6-roo_bZQ",
        "outputId": "cd1299fd-194c-4536-b70e-faa9e943060e"
      },
      "outputs": [],
      "source": [
        "dataset, _ = create_datasets(augment=True, max_seq_length=1250, num_words=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "0QciGSs_I4hr",
        "outputId": "8cf12d54-3d30-4016-b16e-622b212d54fe"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "example_ix = 3\n",
        "x1, c1, y1 = dataset[example_ix]  # Get tokenized version of the second example\n",
        "x2, c2, y2 = dataset[example_ix]\n",
        "\n",
        "o1 = dataset.decode_stroke(x1)\n",
        "r1 = offsets_to_strokes(o1)\n",
        "fig, ax = plot_strokes(r1, title='Reconstructed text (data augmentation seed 1)')\n",
        "\n",
        "o2 = dataset.decode_stroke(x2)\n",
        "print(o2.shape[0]*2)\n",
        "r2 = offsets_to_strokes(o2)\n",
        "fig, ax = plot_strokes(r2, title='Reconstructed from tokens (data augmentation seed 2)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqTTPtzs6ot",
        "outputId": "05f54c56-97eb-4372-89ea-d209562e0215"
      },
      "outputs": [],
      "source": [
        "decoded_stroke = o2#dataset.decode_stroke(x)\n",
        "decoded_text = c1 #dataset.decode_text(c)\n",
        "\n",
        "# print(\"Original stroke shape:\", (1082,3))\n",
        "print(\"Encoded stroke shape:\", x1.shape)\n",
        "print(\"Encoded text shape:\", c1.shape)\n",
        "print(\"Decoded stroke shape:\", decoded_stroke.shape)\n",
        "\n",
        "print(\"\\nEncoded strokes (first 10 tokens, aka 5 pen strokes):\")\n",
        "print([v.item() for v in x1[:15]])\n",
        "print(\"\\n\\nEncoded text (first 10 tokens, aka ascii chars):\")\n",
        "print(c1[:10])\n",
        "print(\"\\nDecoded stroke (first 5 rows, aka 5 pen steps):\")\n",
        "print(decoded_stroke[:5])\n",
        "print(\"\\nInferred number of tokens required to represent:\")\n",
        "print(decoded_stroke.shape[0]*2)\n",
        "print(\"\\nDecoded text:\")\n",
        "print(dataset.decode_text(c1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXAlpngErwVo"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x74SZP_qymr"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Transformer Language Model (*exactly* as used in GPT-2)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_ctx_head == 0\n",
        "        # query projections for all heads\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # key, value projections for all heads\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_ctx_head = config.n_ctx_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        _, T_ctx, _ = context.size()\n",
        "\n",
        "        # calculate query for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x).view(B, T, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # calculate key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k, v = self.c_attn_kv(context).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "        v = v.view(B, T_ctx, self.n_ctx_head, C // self.n_ctx_head).transpose(1, 2) # (B, nh, T_ctx, hs)\n",
        "\n",
        "        # cross-attention; (B, nh, T, hs) x (B, nh, hs, T_ctx) -> (B, nh, T, T_ctx)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T_ctx) x (B, nh, T_ctx, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.cross_attn = CrossAttention(config) # NEW\n",
        "        self.ln_3 = nn.LayerNorm(config.n_embd) # NEW\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.cross_attn(self.ln_2(x), context)\n",
        "        x = x + self.mlpf(self.ln_3(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wce = nn.Embedding(config.context_vocab_size, config.n_embd), # NEW\n",
        "            wcpe = nn.Embedding(config.context_block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"Number of Transformer parameters: {:.0f}\".format(n_params,))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def forward(self, idx, context, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        context_t = context.size(-1)\n",
        "        context_pos = torch.arange(0, context_t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "        context_emb = self.transformer.wce(context) # context embeddings of shape (b, t_ctx, n_embd)\n",
        "        context_pos_emb = self.transformer.wcpe(context_pos)\n",
        "        c = context_emb + context_pos_emb\n",
        "\n",
        "        if self.config.ablate_cross_attention:\n",
        "          c = torch.zeros_like(c)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, c)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int = None # length of the input sequences of integers\n",
        "    context_block_size: int = None\n",
        "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
        "    context_vocab_size: int = None # size of the context vocabulary (ASCII characters)\n",
        "    context_length: int = None # maximum length of the context sequence\n",
        "    # parameters below control the sizes of each model slightly differently\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    n_ctx_head: int = 4 # number of heads for cross-attention\n",
        "    ablate_cross_attention: bool = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoVuOu7ZB3HI"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exe5-vbitBJL",
        "outputId": "d3763b98-9955-46cc-b979-b95b434e5394"
      },
      "outputs": [],
      "source": [
        "def get_time_string(fmt='%m%d_%H%M'):\n",
        "    return datetime.now().strftime(fmt)\n",
        "\n",
        "@dataclass\n",
        "class AppConfig:\n",
        "    # system/input/output\n",
        "    work_dir: str = 'out'\n",
        "    resume: bool = False\n",
        "    sample_only: bool = False\n",
        "    num_workers: int = 1 # 4\n",
        "    max_steps: int = 50000\n",
        "    lr_decay: float = 1.\n",
        "    device: str = 'cuda'\n",
        "    seed: int = 3407\n",
        "\n",
        "    # sampling\n",
        "    top_k: int = -1\n",
        "\n",
        "    # model configuration\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 64\n",
        "    n_embd2: int = 64\n",
        "    n_head: int = 4\n",
        "    ablate_cross_attention: bool = False  # New flag to ablate cross-attention\n",
        "    augment: bool = True\n",
        "    max_seq_length: int = 1250\n",
        "\n",
        "    # optimization\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 1e-2\n",
        "    weight_decay: float = 1e-4\n",
        "\n",
        "    # wandb parameters\n",
        "    wandb_project: str = RUN_TAG\n",
        "    wandb_entity: str = 'sam-greydanus'  # Set this to your wandb username or team name\n",
        "    wandb_run_name: str = f\"{get_time_string()}_{RUN_TAG}\"\n",
        "\n",
        "args = AppConfig()\n",
        "\n",
        "# system inits\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "os.makedirs(args.work_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=args.work_dir)\n",
        "\n",
        "# init datasets\n",
        "train_dataset, test_dataset = create_datasets(augment=args.augment, max_seq_length=args.max_seq_length, num_words=6)\n",
        "vocab_size = train_dataset.get_vocab_size()\n",
        "block_size = train_dataset.get_stroke_seq_length()\n",
        "context_block_size = train_dataset.get_text_seq_length()\n",
        "context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "print(f\"Dataset determined that: {vocab_size=}, {block_size=}\")\n",
        "\n",
        "# init model\n",
        "config = ModelConfig(vocab_size=vocab_size,\n",
        "                     block_size=block_size,\n",
        "                     context_block_size=context_block_size,\n",
        "                     context_vocab_size=context_vocab_size,\n",
        "                     n_layer=args.n_layer, n_head=args.n_head,\n",
        "                     n_embd=args.n_embd, n_embd2=args.n_embd2,\n",
        "                     ablate_cross_attention=args.ablate_cross_attention,\n",
        "                     n_ctx_head=args.n_head,)\n",
        "model = Transformer(config)\n",
        "model.to(args.device)\n",
        "print(f\"Model #params: {sum(p.numel() for p in model.parameters())}\")\n",
        "if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n",
        "    print(\"resuming from existing model in the workdir\")\n",
        "    model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n",
        "if args.sample_only:\n",
        "    # save_samples(num=50)\n",
        "    print('This functionality is temporarily commented out')\n",
        "    sys.exit()\n",
        "\n",
        "# init optimizer and batch loader\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
        "scheduler = StepLR(optimizer, step_size=10000, gamma=args.lr_decay)\n",
        "batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "collapsed": true,
        "id": "MvHVOusgH_z8",
        "outputId": "da30108a-2bf7-4977-ec16-838c1cfe91f0"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=args.wandb_project,\n",
        "    entity=args.wandb_entity,\n",
        "    name=args.wandb_run_name,\n",
        "    config=args\n",
        ")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"n_layer\": config.n_layer,\n",
        "    \"n_head\": config.n_head,\n",
        "    \"n_embd\": config.n_embd,\n",
        "    \"learning_rate\": args.learning_rate,\n",
        "    \"weight_decay\": args.weight_decay,\n",
        "    \"batch_size\": args.batch_size,\n",
        "    \"ablate_cross_attention\": args.ablate_cross_attention,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_EKkV_W1tZu"
      },
      "source": [
        "## Sample from the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RplUgs12B8xB"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, dataset, batch_size=50, max_batches=None):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
        "    losses = []\n",
        "    for i, batch in enumerate(loader):\n",
        "        batch = [t.to(args.device) for t in batch]\n",
        "        X, C, Y = batch\n",
        "        logits, loss = model(X, C, Y)\n",
        "        losses.append(loss.item())\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "    mean_loss = torch.tensor(losses).mean().item()\n",
        "    model.train() # reset model back to training mode\n",
        "    return mean_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, idx, context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    steps = max(0, max_new_tokens-idx.size(1))\n",
        "    for i in range(steps):\n",
        "        # if the sequence context is growing too long we must crop it at block_size\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "        # forward the model to get the logits for the index in the sequence\n",
        "        logits, _ = model(idx_cond, context)\n",
        "        # pluck the logits at the final step and scale by desired temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop the logits to only the top k options\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        # apply softmax to convert logits to (normalized) probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # either sample from the distribution or take the most likely element\n",
        "        if do_sample:\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "        # append sampled index to the running sequence and continue\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def save_samples(model, dataset, num=2, model_device='cpu', warmup_steps=100, do_sample=False):\n",
        "    \"\"\" samples from the model and plots the decoded strokes \"\"\"\n",
        "    model_device = list(model.parameters())[0].device # hacky\n",
        "\n",
        "    stroke_seq, context = [], []\n",
        "    for i in range(num):\n",
        "      x, c, y = dataset[i]\n",
        "      stroke_seq.append(x) ; context.append(c)\n",
        "\n",
        "    X_init = torch.stack(stroke_seq).to(model_device)[:,:warmup_steps]\n",
        "    context = torch.stack(context).long().to(model_device)\n",
        "    top_k = None\n",
        "    steps = dataset.get_stroke_seq_length() - 1  # -1 because we already start with the first token\n",
        "\n",
        "    X_samp = generate(model, X_init, context, steps, top_k=top_k, do_sample=do_sample).to('cpu')\n",
        "\n",
        "    for i in range(X_samp.size(0)):\n",
        "        # get the i'th row of sampled integers, as python list\n",
        "        row = X_samp[i].detach().cpu().numpy()\n",
        "        offset_samp = dataset.decode_stroke(row)\n",
        "        point_samp = offsets_to_strokes(offset_samp)\n",
        "        decoded_ascii = dataset.decode_text(context[i])\n",
        "\n",
        "        # Plot the stroke\n",
        "        fig, ax = plot_strokes(point_samp, f'Sample {i+1}: \"{decoded_ascii}\"') #plt.axis('off')\n",
        "        tag = 'sample' if do_sample else 'topk'\n",
        "        fig.savefig(f\"{dataset.name}_{tag}_{i+1}.png\")\n",
        "        wandb.log({f\"{dataset.name}_{tag}_{i+1}\": wandb.Image(f\"{dataset.name}_{tag}_{i+1}.png\")})\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved {dataset.name}_{tag}_{i+1}.png\")\n",
        "\n",
        "    print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV1ntAoG1vqh"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7otFYXQhr9lz",
        "outputId": "f733eece-46c6-412c-fc59-73a076dbb098"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "best_loss = None\n",
        "step = 0\n",
        "while True:\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # get the next batch, ship to device, and unpack it to input and target\n",
        "    batch = batch_loader.next()\n",
        "    batch = [t.to(args.device) for t in batch]\n",
        "    X, C, Y = batch\n",
        "\n",
        "    # feed into the model\n",
        "    logits, loss = model(X, C, Y)\n",
        "\n",
        "    # calculate the gradient, update the weights\n",
        "    model.zero_grad(set_to_none=True) ; loss.backward()\n",
        "    optimizer.step() ; scheduler.step()\n",
        "    wandb.log({\"train_loss_step\": loss.item(), \"step\": step})\n",
        "\n",
        "    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n",
        "    if args.device.startswith('cuda'):\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    # logging\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms | lr {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "    # evaluate the model\n",
        "    if step > 0 and step % 1000 == 0:\n",
        "        train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
        "        test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n",
        "        wandb.log({\"train_loss\": train_loss, \"test_loss\": test_loss, \"step\": step })\n",
        "        print(f\"step {step} train loss: {train_loss:.4f} test loss: {test_loss:.4f}\")\n",
        "        # save the model to disk if it has improved\n",
        "        if best_loss is None or test_loss < best_loss:\n",
        "            out_path = os.path.join(args.work_dir, \"model.pt\")\n",
        "            print(f\"Test loss {test_loss:.4f} is the best so far, saving model to {out_path}\")\n",
        "            torch.save(model.state_dict(), out_path)\n",
        "            #wandb.save(out_path)\n",
        "            best_loss = test_loss\n",
        "\n",
        "    # sample from the model\n",
        "    if step > 0 and step % 1000 == 0:\n",
        "        save_samples(model, test_dataset, num=6, do_sample=True)\n",
        "        save_samples(model, test_dataset, num=6, do_sample=False)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=True)\n",
        "        save_samples(model, train_dataset, num=3, do_sample=False)\n",
        "\n",
        "    step += 1\n",
        "    # termination conditions\n",
        "    if args.max_steps >= 0 and step >= args.max_steps:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q83i143gIwwJ"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtO814TOsWU_"
      },
      "outputs": [],
      "source": [
        "plt.figure(dpi=200)\n",
        "img = mpimg.imread('test_sample_1.png')\n",
        "plt.imshow(img) ; plt.axis('off') ; plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoV4m0tGoedt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
