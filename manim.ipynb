{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AedYKpegSH-I"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM2L1Vgq8x932yGWY7GRYvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwimpee/cursivetransformer/blob/main/manim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWQy-xDwPsn"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install libcairo2-dev ffmpeg \\\n",
        "    texlive texlive-latex-extra texlive-fonts-extra \\\n",
        "    texlive-latex-recommended texlive-science \\\n",
        "    tipa libpango1.0-dev\n",
        "!pip install manim\n",
        "!pip install IPython==8.21.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf cursivetransformer && git clone https://github.com/zwimpee/cursivetransformer.git && cd cursivetransformer && pip install -r requirements.txt\n",
        "import sys\n",
        "sys.path.append('/content/cursivetransformer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wi_25wjQsl5",
        "outputId": "4d1c88cf-7ed2-41e2-ea00-6c24bce38dab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cursivetransformer'...\n",
            "remote: Enumerating objects: 2259, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 2259 (delta 365), reused 357 (delta 331), pack-reused 1859 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2259/2259), 30.05 MiB | 16.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1271/1271), done.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.4.0+cu121)\n",
            "Collecting wandb (from -r requirements.txt (line 2))\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (4.3.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from manim import *\n",
        "import numpy as np\n",
        "from cursivetransformer.model import get_all_args, get_checkpoint\n",
        "from cursivetransformer.data import create_datasets, load_and_parse_data, generate_word_combos, offsets_to_strokes\n",
        "from cursivetransformer.sample import plot_strokes, save_samples, generate_n_words, generate"
      ],
      "metadata": {
        "id": "HUTGL76ZJxnp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlrcUVFIb_mZ",
        "outputId": "8398c639-38a2-4bcf-81ff-555ddb3b3654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_all_args(False)\n",
        "args.sample_only = True\n",
        "args.load_from_run_id = '6le6tujz'\n",
        "args.dataset_name = 'bigbank'\n",
        "torch.manual_seed(args.seed)  # system inits\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "train_dataset, test_dataset = create_datasets(args)  # init datasets\n",
        "args.block_size = train_dataset.get_stroke_seq_length()\n",
        "args.context_block_size = train_dataset.get_text_seq_length()\n",
        "args.vocab_size = train_dataset.get_vocab_size()\n",
        "args.context_vocab_size = train_dataset.get_char_vocab_size()\n",
        "print(f\"Dataset determined that: {args.vocab_size=}, {args.block_size=}\")\n",
        "model, _, _, _, _ = get_checkpoint(args)\n",
        "\n",
        "# save_samples(model, test_dataset, num=6, do_sample=True, log_wandb=False)\n",
        "# save_samples(model, test_dataset, num=6, do_sample=False, log_wandb=False)\n",
        "# print(len(combo['points']))\n",
        "# _ = plot_strokes(combo['points'], title=combo['metadata']['asciiSequence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xgpv11udZqD",
        "outputId": "a189005e-bd65-4391-b05b-5d2fbf339116"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load dataset file from /content/cursivetransformer/data/bigbank.json.zip\n",
            "Succeeded in loading the bigbank dataset; contains 1900 items.\n",
            "For a dataset of 1805 examples we can generate 440811596555 combinations of 4 examples.\n",
            "Generating 497000 4-word examples.\n",
            "For a dataset of 95 examples we can generate 3183545 combinations of 4 examples.\n",
            "Generating 3000 4-word examples.\n",
            "Number of examples in the train dataset: 497000\n",
            "Number of examples in the test dataset: 3000\n",
            "Max token sequence length: 1000\n",
            "Number of unique characters in the ascii vocabulary: 71\n",
            "Ascii vocabulary:\n",
            "\t\" enaitoshrdx.vpukbgfcymzw1lqj804I92637OTAS5N)EHR\"'(BCQLMWYU,ZF!DXV?KPGJ\"\n",
            "Split up the dataset into 497000 training examples and 3000 test examples\n",
            "Dataset determined that: args.vocab_size=455, args.block_size=1000\n",
            "Number of Transformer parameters: 368064\n",
            "Model #params: 397184\n",
            "Finding latest checkpoint for W&B run id 6le6tujz\n",
            "  model:best_checkpoint:v70\n",
            "  model:best_checkpoint:v71\n",
            "  model:best_checkpoint:v72\n",
            "  model:best_checkpoint:v73\n",
            "  model:best_checkpoint:v74\n",
            "  model:best_checkpoint:v75\n",
            "  model:best_checkpoint:v76\n",
            "  model:best_checkpoint:v77\n",
            "  model:best_checkpoint:v78\n",
            "  model:best_checkpoint:v79\n",
            "  model:best_checkpoint:v80\n",
            "  model:best_checkpoint:v81\n",
            "  model:best_checkpoint:v82\n",
            "  model:best_checkpoint:v83\n",
            "  wandb-history:run-6le6tujz-history:v1\n",
            "Selected:  model:best_checkpoint:v83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Animations"
      ],
      "metadata": {
        "id": "GYz8xIVQSF3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Strokes"
      ],
      "metadata": {
        "id": "6rrngUP8SLMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_json = load_and_parse_data(args.dataset_name)\n",
        "combo_json = generate_word_combos(raw_json)\n",
        "combo = combo_json[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AObmmcVNvluJ",
        "outputId": "7fc91c49-f29b-4127-afbd-cc2d7d73db51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a dataset of 1900 examples we can generate 1141362300 combinations of 3 examples.\n",
            "Generating 10000 3-word examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%manim -qm -v WARNING PlotStrokesAnimation\n",
        "\n",
        "class PlotStrokesAnimation(Scene):\n",
        "    def construct(self):\n",
        "        # Title\n",
        "        title = Text(\"Plotting Handwritten Example\", font_size=36)\n",
        "        title.to_edge(UP)\n",
        "        self.play(Write(title))\n",
        "        self.wait(1)\n",
        "\n",
        "        # Use the sample data from combo\n",
        "        points = combo['points']\n",
        "        ascii_sequence = combo['metadata']['asciiSequence']\n",
        "\n",
        "        # Convert stroke data into Manim mobjects\n",
        "        strokes = self.create_strokes(points)\n",
        "\n",
        "        # Display the strokes\n",
        "        self.play(Create(strokes, run_time=5))\n",
        "        self.wait(2)\n",
        "\n",
        "        # Add label for the ASCII sequence\n",
        "        label = Text(f\"ASCII Sequence: {ascii_sequence}\", font_size=24)\n",
        "        label.next_to(strokes, DOWN)\n",
        "        self.play(Write(label))\n",
        "        self.wait(3)\n",
        "\n",
        "    def create_strokes(self, points):\n",
        "        # Initialize variables\n",
        "        stroke_paths = VGroup()\n",
        "        path_points = []  # List to hold points for the current path\n",
        "\n",
        "        # Iterate through the points\n",
        "        for i in range(len(points)):\n",
        "            x, y, p = points[i]\n",
        "            # Invert the y-coordinate\n",
        "            y = -y\n",
        "            point = np.array([x, y, 0])\n",
        "\n",
        "            if p == 1:\n",
        "                # Pen down, collect points\n",
        "                path_points.append(point)\n",
        "            else:\n",
        "                # Pen up, create a path if there are collected points\n",
        "                if path_points:\n",
        "                    current_path = VMobject()\n",
        "                    current_path.set_points_as_corners(path_points)\n",
        "                    stroke_paths.add(current_path)\n",
        "                    path_points = []  # Reset for the next path\n",
        "\n",
        "        # Add the last path if any points remain\n",
        "        if path_points:\n",
        "            current_path = VMobject()\n",
        "            current_path.set_points_as_corners(path_points)\n",
        "            stroke_paths.add(current_path)\n",
        "\n",
        "        # Style the strokes\n",
        "        stroke_paths.set_stroke(color=BLUE, width=2)\n",
        "        stroke_paths.move_to(ORIGIN)\n",
        "\n",
        "        # Scale strokes to fit the scene\n",
        "        stroke_paths.scale(3)\n",
        "\n",
        "        return stroke_paths"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/media/jupyter/PlotStrokesAnimation@2024-09-15@17-36-10.mp4": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nw4PR5FuSK_8",
        "outputId": "e6963ca3-a2c0-41fc-eb12-e55f9892e432"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m18.1\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.18.1</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"media/jupyter/PlotStrokesAnimation@2024-09-15@17-36-10.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Model Outputs"
      ],
      "metadata": {
        "id": "wookM_sZaCbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_device = next(model.parameters()).device\n",
        "warmup_steps = 100\n",
        "do_sample = False\n",
        "num = 1\n",
        "stroke_seq, context = [], []\n",
        "for i in range(num):\n",
        "  x, c, y = test_dataset[i]\n",
        "  stroke_seq.append(x) ; context.append(c)\n",
        "\n",
        "X_init = torch.stack(stroke_seq).to(model_device)[:,:warmup_steps]\n",
        "context = torch.stack(context).long().to(model_device)\n",
        "top_k = None\n",
        "steps = test_dataset.get_stroke_seq_length() - 1  # -1 because we already start with the first token\n",
        "\n",
        "X_samp = generate(model, X_init, context, steps, top_k=top_k, do_sample=do_sample).to('cpu')\n",
        "\n",
        "examples = []\n",
        "for i in range(X_samp.size(0)):\n",
        "    # get the i'th row of sampled integers, as python list\n",
        "    row = X_samp[i].detach().cpu().numpy()\n",
        "    offset_samp = test_dataset.decode_stroke(row)\n",
        "    points = offsets_to_strokes(offset_samp)\n",
        "    ascii_sequence = test_dataset.decode_text(context[i])\n",
        "    examples.append((points, ascii_sequence))"
      ],
      "metadata": {
        "id": "v8ZQq6TFvkHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%manim -qm -v WARNING PlotModelOutputAnimation\n",
        "\n",
        "class PlotModelOutputAnimation(PlotStrokesAnimation):\n",
        "    def construct(self):\n",
        "        # Title\n",
        "        title = Text(\"Plotting Model Handwritten Example\", font_size=36)\n",
        "        title.to_edge(UP)\n",
        "        self.play(Write(title))\n",
        "        self.wait(1)\n",
        "\n",
        "        # Use the sample data from combo\n",
        "        # points = combo['points']\n",
        "        # ascii_sequence = combo['metadata']['asciiSequence']\n",
        "\n",
        "        for points, ascii_sequence in examples:\n",
        "            # Convert stroke data into Manim mobjects\n",
        "            strokes = self.create_strokes(points)\n",
        "\n",
        "            # Display the strokes\n",
        "            self.play(Create(strokes, run_time=5))\n",
        "            self.wait(2)\n",
        "\n",
        "            # Add label for the ASCII sequence\n",
        "            label = Text(f\"ASCII Sequence: {ascii_sequence}\", font_size=24)\n",
        "            label.next_to(strokes, DOWN)\n",
        "            self.play(Write(label))\n",
        "            self.wait(3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/media/jupyter/PlotModelOutputAnimation@2024-09-15@17-37-04.mp4": {
              "data": "",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Tf8VL5dqmH1P",
        "outputId": "2554e2db-6418-4ca5-c631-98b52c31da37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m18.1\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.18.1</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"media/jupyter/PlotModelOutputAnimation@2024-09-15@17-37-04.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stroke Tokenization"
      ],
      "metadata": {
        "id": "AedYKpegSH-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%manim -qm -v WARNING StrokeTokenizationAnimation\n",
        "\n",
        "class StrokeTokenizationAnimation(Scene):\n",
        "    def construct(self):\n",
        "        # Title\n",
        "        title = Text(\"Cursive Transformer Tokenization Scheme\", font_size=36)\n",
        "        title.to_edge(UP)\n",
        "        self.play(Write(title))\n",
        "        self.wait(1)\n",
        "\n",
        "        # Step 1: Raw Stroke Data Visualization\n",
        "        self.raw_stroke_data()\n",
        "\n",
        "        # Step 2: Compute Offsets\n",
        "        self.compute_offsets()\n",
        "\n",
        "        # Step 3: Convert to Polar Coordinates\n",
        "        self.convert_to_polar()\n",
        "\n",
        "        # Step 4: Binning and Token Assignment\n",
        "        self.binning_and_token_assignment()\n",
        "\n",
        "        # Step 5: Final Token Sequence\n",
        "        self.final_token_sequence()\n",
        "\n",
        "    def raw_stroke_data(self):\n",
        "        # Raw stroke data points (x, y, pen_state)\n",
        "        self.raw_points = [\n",
        "            [-5, 2, 0], [-4, 2.5, 0], [-3, 2, 0], [-2, 1.5, 0],\n",
        "            [-1, 1, 0], [0, 0.5, 0], [1, 0, 0], [2, -0.5, 0]\n",
        "        ]\n",
        "        self.pen_states = [1]*len(self.raw_points)  # Pen down for all points\n",
        "\n",
        "        # Create dots for each point (use full 3D points)\n",
        "        dots = VGroup(*[\n",
        "            Dot(point=point, color=BLUE)\n",
        "            for point in self.raw_points\n",
        "        ])\n",
        "\n",
        "        # Create lines between points\n",
        "        lines = VGroup(*[\n",
        "            Line(self.raw_points[i], self.raw_points[i+1], color=BLUE)\n",
        "            for i in range(len(self.raw_points)-1)\n",
        "        ])\n",
        "\n",
        "        # Group dots and lines\n",
        "        stroke = VGroup(lines, dots).shift(LEFT*3)\n",
        "        stroke_label = Text(\"Raw Stroke Data (x, y, p)\", font_size=24)\n",
        "        stroke_label.next_to(stroke, DOWN)\n",
        "\n",
        "        self.play(Create(stroke), Write(stroke_label))\n",
        "        self.wait(2)\n",
        "\n",
        "        # Store for later use\n",
        "        self.stroke = stroke\n",
        "        self.stroke_label = stroke_label\n",
        "\n",
        "    def compute_offsets(self):\n",
        "        # Transition\n",
        "        self.play(\n",
        "            self.stroke.animate.shift(UP*2 + LEFT*2),\n",
        "            self.stroke_label.animate.shift(UP*2 + LEFT*2),\n",
        "        )\n",
        "\n",
        "        # Compute offsets (Δx, Δy, p)\n",
        "        self.offsets = []\n",
        "        for i in range(1, len(self.raw_points)):\n",
        "            dx = self.raw_points[i][0] - self.raw_points[i-1][0]\n",
        "            dy = self.raw_points[i][1] - self.raw_points[i-1][1]\n",
        "            p = self.pen_states[i]\n",
        "            self.offsets.append([dx, dy, p])\n",
        "\n",
        "        # Display offsets as arrows\n",
        "        offset_arrows = VGroup(*[\n",
        "            Arrow(\n",
        "                start=self.raw_points[i-1],\n",
        "                end=self.raw_points[i],\n",
        "                buff=0,\n",
        "                color=GREEN\n",
        "            )\n",
        "            for i in range(1, len(self.raw_points))\n",
        "        ])\n",
        "\n",
        "        offsets_label = Text(\"Offsets (Δx, Δy, p)\", font_size=24)\n",
        "        offsets_label.next_to(offset_arrows, DOWN)\n",
        "\n",
        "        self.play(Create(offset_arrows), Write(offsets_label))\n",
        "        self.wait(2)\n",
        "\n",
        "        # Store for later use\n",
        "        self.offset_arrows = offset_arrows\n",
        "        self.offsets_label = offsets_label\n",
        "\n",
        "    def convert_to_polar(self):\n",
        "        # Transition\n",
        "        self.play(\n",
        "            self.offset_arrows.animate.shift(RIGHT*4),\n",
        "            self.offsets_label.animate.shift(RIGHT*4),\n",
        "        )\n",
        "\n",
        "        # Convert to polar coordinates (r, θ, p)\n",
        "        self.polar_coords = []\n",
        "        for offset in self.offsets:\n",
        "            dx, dy, p = offset\n",
        "            r = np.hypot(dx, dy)\n",
        "            theta = np.arctan2(dy, dx)\n",
        "            self.polar_coords.append([r, theta, p])\n",
        "\n",
        "        # Display polar vectors\n",
        "        origin_points = [self.raw_points[i-1] for i in range(1, len(self.raw_points))]\n",
        "        polar_arrows = VGroup(*[\n",
        "            Arrow(\n",
        "                start=origin,\n",
        "                end=origin + np.array([\n",
        "                    coord[0]*np.cos(coord[1]),\n",
        "                    coord[0]*np.sin(coord[1]),\n",
        "                    0\n",
        "                ]),\n",
        "                buff=0,\n",
        "                color=YELLOW\n",
        "            )\n",
        "            for origin, coord in zip(origin_points, self.polar_coords)\n",
        "        ])\n",
        "\n",
        "        polar_label = Text(\"Polar Coordinates (r, θ, p)\", font_size=24)\n",
        "        polar_label.next_to(polar_arrows, DOWN)\n",
        "\n",
        "        self.play(Create(polar_arrows), Write(polar_label))\n",
        "        self.wait(2)\n",
        "\n",
        "        # Store for later use\n",
        "        self.polar_arrows = polar_arrows\n",
        "        self.polar_label = polar_label\n",
        "\n",
        "    def binning_and_token_assignment(self):\n",
        "        # Transition\n",
        "        self.play(\n",
        "            self.offsets_label.animate.shift(UP*3 + LEFT*4),\n",
        "            self.polar_label.animate.shift(UP*3 + RIGHT*2),\n",
        "        )\n",
        "        self.play(\n",
        "            FadeOut(self.stroke),\n",
        "            FadeOut(self.stroke_label),\n",
        "            FadeOut(self.offset_arrows),\n",
        "            FadeOut(self.polar_arrows),\n",
        "        )\n",
        "\n",
        "        # Bins as per StrokeDataset\n",
        "        theta_bins = np.linspace(-np.pi, np.pi, 151)\n",
        "        r_bins_pen_down = np.concatenate([\n",
        "            np.array([0]),\n",
        "            np.linspace(0.0001, 0.050, 50),\n",
        "            np.geomspace(0.051, 2.25, 101)[:-1]\n",
        "        ])\n",
        "        r_bins_pen_up = r_bins_pen_down + max(r_bins_pen_down) + 1\n",
        "        r_bins = np.concatenate([r_bins_pen_down, r_bins_pen_up])\n",
        "\n",
        "        # Cumulative sizes\n",
        "        feature_sizes = [len(r_bins), len(theta_bins)]\n",
        "        cumulative_sizes = np.cumsum([0] + feature_sizes)\n",
        "\n",
        "        # Token IDs and assignments\n",
        "        token_assignments = []\n",
        "        for coord in self.polar_coords:\n",
        "            r, theta, p = coord\n",
        "            # Binning theta\n",
        "            theta_idx = np.digitize(theta, theta_bins) - 1\n",
        "            theta_token = theta_idx + cumulative_sizes[1]\n",
        "\n",
        "            # Binning r\n",
        "            if p == 1:  # Pen down\n",
        "                r_idx = np.digitize(r, r_bins_pen_down) - 1\n",
        "            else:  # Pen up\n",
        "                r_idx = np.digitize(r, r_bins_pen_up) - 1 + len(r_bins_pen_down)\n",
        "            r_token = r_idx + cumulative_sizes[0]\n",
        "\n",
        "            token_assignments.append((theta_token, r_token))\n",
        "\n",
        "        # Store for later use\n",
        "        self.token_assignments = token_assignments\n",
        "\n",
        "        # Display token assignments\n",
        "        tokens_text = VGroup()\n",
        "        for i, (theta_token, r_token) in enumerate(self.token_assignments):\n",
        "            text = Text(\n",
        "                f\"Token {2*i+1}: θ ID {theta_token}\\n\"\n",
        "                f\"Token {2*i+2}: r ID {r_token}\",\n",
        "                font_size=20, line_spacing=1\n",
        "            ).shift(LEFT*3 + DOWN*(i - len(self.token_assignments)/2))\n",
        "            tokens_text.add(text)\n",
        "\n",
        "        tokens_label = Text(\"Binning and Token Assignment\", font_size=24)\n",
        "        tokens_label.next_to(tokens_text, UP)\n",
        "\n",
        "        self.play(Write(tokens_label), Write(tokens_text))\n",
        "        self.wait(3)\n",
        "\n",
        "        # Store for later use\n",
        "        self.tokens_text = tokens_text\n",
        "\n",
        "    def final_token_sequence(self):\n",
        "        # Transition\n",
        "        self.play(\n",
        "            FadeOut(self.offsets_label),\n",
        "            FadeOut(self.polar_label),\n",
        "            FadeOut(self.tokens_text),\n",
        "        )\n",
        "\n",
        "        # Create final token sequence\n",
        "        final_tokens = []\n",
        "        for theta_token, r_token in self.token_assignments:\n",
        "            final_tokens.append(theta_token)\n",
        "            final_tokens.append(r_token)\n",
        "\n",
        "        tokens_str = \", \".join(map(str, final_tokens))\n",
        "        final_text = Text(\n",
        "            f\"Final Token Sequence:\\n[{tokens_str}]\",\n",
        "            font_size=24, line_spacing=1.5\n",
        "        )\n",
        "\n",
        "        self.play(Write(final_text))\n",
        "        self.wait(3)\n",
        "\n",
        "        # Conclusion\n",
        "        conclusion = Text(\n",
        "            \"This token sequence is input to the Transformer model.\",\n",
        "            font_size=24\n",
        "        ).next_to(final_text, DOWN, buff=1)\n",
        "        self.play(Write(conclusion))\n",
        "        self.wait(3)"
      ],
      "metadata": {
        "id": "DCe0gcV8G77p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}